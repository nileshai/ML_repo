{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Translated Text and Annotations - Japanese</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>私の名前は [Marquez 00:00:00].です。どんな御用でしょうか？もしもし？こん...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>それで、私はここに座って、おそらく&lt;TIME&gt;30分&lt;/TIME&gt;間確認メールを待っていました。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>私があなたたちからそのメールを受け取るまで、彼らは私に何も乗ることができませんが、私はここに...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ええと、女性は私を連れて行きました、彼女は私を連れて行き、彼女と一緒にそれを持っていきました...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[crosstalk00:02:51] &lt;SPL&gt;Cはチャーリー、Jはジェーン、38、Kはカ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Translated Text and Annotations - Japanese\n",
       "0  私の名前は [Marquez 00:00:00].です。どんな御用でしょうか？もしもし？こん...\n",
       "1   それで、私はここに座って、おそらく<TIME>30分</TIME>間確認メールを待っていました。\n",
       "2  私があなたたちからそのメールを受け取るまで、彼らは私に何も乗ることができませんが、私はここに...\n",
       "3  ええと、女性は私を連れて行きました、彼女は私を連れて行き、彼女と一緒にそれを持っていきました...\n",
       "4  [crosstalk00:02:51] <SPL>Cはチャーリー、Jはジェーン、38、Kはカ..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel('final_FA_data.xls')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for index, row in df.iterrows():\\n    temp = clean(str(row[\"Translated Text and Annotations - Japanese\"]))\\n\\n    print(temp)'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean(row1):\n",
    "    modified_string = re.sub(r\"\\<[^<>]*\\>\", \"\", row1)    #remove tags\n",
    "    result = re.sub(r\"\\[[^<>]*\\]\", \"\", modified_string)  #remove callbacks\n",
    "\n",
    "    return(result)\n",
    "\n",
    "\"\"\"for index, row in df.iterrows():\n",
    "    temp = clean(str(row[\"Translated Text and Annotations - Japanese\"]))\n",
    "\n",
    "    print(temp)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xls to tsv for <> data labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"transcript_tsv.txt\", \"w\")\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    _ = str(row[\"Translated Text and Annotations - Japanese\"])\n",
    "    flag = 0\n",
    "    q = \"\"\n",
    "    p =''\n",
    "    temp = clean(_)\n",
    "    for i in range(len(_)): \n",
    "        tag=''\n",
    "        entity=''\n",
    "        if _[i]=='<' and _[i+1] != \"/\":\n",
    "            flag = 1\n",
    "            i=i+1\n",
    "            while _[i] != \">\":\n",
    "                tag = tag + _[i]\n",
    "                i=i+1\n",
    "            i=i+1\n",
    "            while _[i] != \"<\":\n",
    "                entity = entity + _[i]\n",
    "                i=i+1\n",
    "            #p = p + entity + \"\\n\"\n",
    "            if q == \"\":\n",
    "                q = q + entity + \" - \" + tag\n",
    "            else:\n",
    "                q = q + \"\\t\" + entity + \" - \" + tag\n",
    "    if flag:\n",
    "        p = temp + \"\\t\" + q\n",
    "        f.write(p + \"\\n\")\n",
    "        #if tag!=\"\" and entity != \"\":\n",
    "        #    print(tag,entity)\n",
    "        # 大阪ガスお客様センター - ORG,小池 - PERSON\n",
    "    else:\n",
    "        p = temp\n",
    "        f.write(p + \"\\n\")\n",
    "        \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test set - removing labels and formatting into tsv for spacy testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Translated Text and Annotations - Japanese</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>私の名前は [Marquez 00:00:00].です。どんな御用でしょうか？もしもし？こん...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>それで、私はここに座って、おそらく30分間確認メールを待っていました。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>私があなたたちからそのメールを受け取るまで、彼らは私に何も乗ることができませんが、私はここに...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ええと、女性は私を連れて行きました、彼女は私を連れて行き、彼女と一緒にそれを持っていきました...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[crosstalk00:02:51] Cはチャーリー、Jはジェーン、38、Kはカイト。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Translated Text and Annotations - Japanese\n",
       "0  私の名前は [Marquez 00:00:00].です。どんな御用でしょうか？もしもし？こん...\n",
       "1                それで、私はここに座って、おそらく30分間確認メールを待っていました。\n",
       "2  私があなたたちからそのメールを受け取るまで、彼らは私に何も乗ることができませんが、私はここに...\n",
       "3  ええと、女性は私を連れて行きました、彼女は私を連れて行き、彼女と一緒にそれを持っていきました...\n",
       "4       [crosstalk00:02:51] Cはチャーリー、Jはジェーン、38、Kはカイト。"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel('/home/kritika/Downloads/Uniphore/Test_set/FA_test_set.xls')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean(row1):\n",
    "    modified_string = re.sub(r\"\\<[^<>]*\\>\", \"\", row1)    #remove tags\n",
    "    result = re.sub(r\"\\[[^<>]*\\]\", \"\", modified_string)  #remove callbacks\n",
    "\n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"FA_test_tsv.txt\", \"w\")\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    _ = str(row[\"Translated Text and Annotations - Japanese\"])\n",
    "    flag = 0\n",
    "    q = \"\"\n",
    "    p =''\n",
    "    temp = clean(_)\n",
    "    for i in range(len(_)): \n",
    "        tag=''\n",
    "        entity=''\n",
    "        if _[i]=='<' and _[i+1] != \"/\":\n",
    "            flag = 1\n",
    "            i=i+1\n",
    "            while _[i] != \">\":\n",
    "                tag = tag + _[i]\n",
    "                i=i+1\n",
    "            i=i+1\n",
    "            while _[i] != \"<\":\n",
    "                entity = entity + _[i]\n",
    "                i=i+1\n",
    "            #p = p + entity + \"\\n\"\n",
    "            if q == \"\":\n",
    "                q = q + entity + \" - \" + tag\n",
    "            else:\n",
    "                q = q + \"\\t\" + entity + \" - \" + tag\n",
    "    if flag:\n",
    "        p = temp + \"\\t\" + q\n",
    "        f.write(p + \"\\n\")\n",
    "        #if tag!=\"\" and entity != \"\":\n",
    "        #    print(tag,entity)\n",
    "        # 大阪ガスお客様センター - ORG,小池 - PERSON\n",
    "    else:\n",
    "        p = temp\n",
    "        f.write(p + \"\\n\")\n",
    "        \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train set - removing labels and formatting into tsv for spacy training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Translated Text and Annotations - Japanese</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>チェックインの場合、ほとんどの空港では、ターミナルに到着し、出発の少なくとも二時間前にチェッ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>時間通りの出発を確実にするために、出発の30分前にゲートにいることを確認してください。出発前...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>そして、ここにもう一つのヒントがあります。チケットカウンターを迂回して、飛行機に最初に搭乗し...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>もしそうなら、&lt;ORG&gt;全日空航空&lt;/ORG&gt;ワールドマスターカードはあなたのためです。40...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;ORG&gt;全日空航空&lt;/ORG&gt;のマスターカードに申し込むと、承認されると40,000ボーナ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Translated Text and Annotations - Japanese\n",
       "0  チェックインの場合、ほとんどの空港では、ターミナルに到着し、出発の少なくとも二時間前にチェッ...\n",
       "1  時間通りの出発を確実にするために、出発の30分前にゲートにいることを確認してください。出発前...\n",
       "2  そして、ここにもう一つのヒントがあります。チケットカウンターを迂回して、飛行機に最初に搭乗し...\n",
       "3  もしそうなら、<ORG>全日空航空</ORG>ワールドマスターカードはあなたのためです。40...\n",
       "4  <ORG>全日空航空</ORG>のマスターカードに申し込むと、承認されると40,000ボーナ..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel('/home/kritika/Downloads/Uniphore/Training_set/FA_train_set_spacy_3entities.xls')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6936"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean(row1):\n",
    "    modified_string = re.sub(r\"\\<[^<>]*\\>\", \"\", row1)    #remove tags\n",
    "    result = re.sub(r\"\\[[^<>]*\\]\", \"\", modified_string)  #remove callbacks\n",
    "\n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"FA_train_spacy3e_tsv.txt\", \"w\")\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    _ = str(row[\"Translated Text and Annotations - Japanese\"])\n",
    "    flag = 0\n",
    "    q = \"\"\n",
    "    p =''\n",
    "    temp = clean(_)\n",
    "    for i in range(len(_)): \n",
    "        tag=''\n",
    "        entity=''\n",
    "        if _[i]=='<' and _[i+1] != \"/\":\n",
    "            flag = 1\n",
    "            i=i+1\n",
    "            while _[i] != \">\":\n",
    "                tag = tag + _[i]\n",
    "                i=i+1\n",
    "            i=i+1\n",
    "            while _[i] != \"<\":\n",
    "                entity = entity + _[i]\n",
    "                i=i+1\n",
    "            #p = p + entity + \"\\n\"\n",
    "            if q == \"\":\n",
    "                q = q + entity + \" - \" + tag\n",
    "            else:\n",
    "                q = q + \"\\t\" + entity + \" - \" + tag\n",
    "    if flag:\n",
    "        p = temp + \"\\t\" + q\n",
    "        f.write(p + \"\\n\")\n",
    "        #if tag!=\"\" and entity != \"\":\n",
    "        #    print(tag,entity)\n",
    "        # 大阪ガスお客様センター - ORG,小池 - PERSON\n",
    "    else:\n",
    "        p = temp\n",
    "        f.write(p + \"\\n\")\n",
    "        \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tsv to Spacy format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from sudachipy import tokenizer, dictionary\n",
    "\n",
    "SENTENCE_SEP = '\\t'\n",
    "ENTITY_SEP = '-'\n",
    "\n",
    "\n",
    "def tokenize(sentence, mode):\n",
    "    \"\"\" To perform tokenization on the train sentences and the values \"\"\"\n",
    "    tokenizer_obj = dictionary.Dictionary().create()\n",
    "    if mode == \"A\":\n",
    "        split_mode = tokenizer.Tokenizer.SplitMode.A\n",
    "    elif mode == \"B\":\n",
    "        split_mode = tokenizer.Tokenizer.SplitMode.B\n",
    "    elif mode == \"C\":\n",
    "        split_mode = tokenizer.Tokenizer.SplitMode.C\n",
    "    else:\n",
    "        raise ValueError(\"Invalid Mode: only A, B or C is allowed\")\n",
    "\n",
    "    return \"\".join([m.surface() for m in tokenizer_obj.tokenize(sentence, split_mode)])\n",
    "\n",
    "\n",
    "def format_train_data(train_data_file, output_file, language):\n",
    "    \"\"\" formats the given train data into spacy's required format \"\"\"\n",
    "    if train_data_file.endswith('.txt'):\n",
    "        with open(train_data_file, 'r', encoding=\"utf8\", newline=\"\") as f:\n",
    "            line_list = f.readlines()\n",
    "        train_data_list = []\n",
    "        \n",
    "        for line in line_list:\n",
    "            line_split_list = line.split(SENTENCE_SEP)\n",
    "            train_sentence = line_split_list.pop(0).strip()\n",
    "            train_sentence = tokenize(train_sentence, \"B\")\n",
    "            entities = {\"entities\": []}   \n",
    "            nex = 0\n",
    "            \n",
    "            for entity in line_split_list:\n",
    "                value = entity.split(ENTITY_SEP)[0].strip()\n",
    "                value = tokenize(value, \"B\")\n",
    "                entity_name = entity.split(ENTITY_SEP)[1].strip()    \n",
    "                search = train_sentence.find(value, nex)\n",
    "\n",
    "                if search!= -1:\n",
    "                    entity_tuple = (search, search + len(value), entity_name)\n",
    "                    entities[\"entities\"].append(entity_tuple)\n",
    "                    nex = search + len(value)\n",
    "\n",
    "            train_data_list.append((train_sentence, entities))\n",
    "    train_data_json = {\"train_data\": train_data_list}\n",
    "    with open(output_file, \"w\", encoding='utf8') as json_file:\n",
    "        json.dump(train_data_json, json_file, ensure_ascii=False)\n",
    "\n",
    "\n",
    "train_file = r'/home/kritika/Downloads/Uniphore/Training_set/FA_train_spacy3e_tsv.txt'\n",
    "output_file = r'../FA_train_spacy3e_format.json'\n",
    "format_train_data(train_file, output_file, \"japanese\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try for one sent which contains multiple occurences of person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_data': [('こんにちは、私の名前はテリー・フォスターです。オンラインでチケットを購入しました。そして一つは、私の名前が間違ったものに印刷されているということです。テリー・フォスター・フェイ・フォスターと書いてあり、テリー・フェイ・フォスターだけのはずです。', {'entities': [(11, 20, 'PERSON'), (75, 94, 'PERSON'), (101, 114, 'PERSON')]})]}\n"
     ]
    }
   ],
   "source": [
    "train_data_file = r'/home/kritika/Downloads/Uniphore/Training_set/try.txt'\n",
    "\n",
    "if train_data_file.endswith('.txt'):\n",
    "    with open(train_data_file, 'r', encoding=\"utf8\", newline=\"\") as f:\n",
    "        line_list = f.readlines()\n",
    "    train_data_list = []\n",
    "    for line in line_list:\n",
    "        line_split_list = line.split(SENTENCE_SEP)\n",
    "        train_sentence = line_split_list.pop(0).strip()\n",
    "        train_sentence = tokenize(train_sentence, \"B\")\n",
    "        entities = {\"entities\": []}   \n",
    "        nex = 0\n",
    "        for entity in line_split_list:\n",
    "            value = entity.split(ENTITY_SEP)[0].strip()\n",
    "            value = tokenize(value, \"B\")\n",
    "            entity_name = entity.split(ENTITY_SEP)[1].strip()    \n",
    "            search = train_sentence.find(value, nex)\n",
    "            \n",
    "            if search!= -1:\n",
    "                entity_tuple = (search, search + len(value), entity_name)\n",
    "                entities[\"entities\"].append(entity_tuple)\n",
    "                nex = search + len(value)\n",
    "                #print(\"next:\", nex)\n",
    "                \n",
    "        train_data_list.append((train_sentence, entities))\n",
    "        #print(temp)\n",
    "train_data_json = {\"train_data\": train_data_list}\n",
    "print(train_data_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
