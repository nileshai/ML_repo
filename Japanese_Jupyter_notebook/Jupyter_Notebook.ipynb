{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean(row1):\n",
    "    modified_string = re.sub(r\"\\<[^<>]*\\>\", \"\", row1)    #remove tags\n",
    "    result = re.sub(r\"\\[[^<>]*\\]\", \"\", modified_string)  #remove callbacks\n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from sudachipy import tokenizer, dictionary\n",
    "\n",
    "SENTENCE_SEP = '\\t'\n",
    "ENTITY_SEP = '-'\n",
    "\n",
    "\n",
    "def tokenize(sentence, mode):\n",
    "    \"\"\" To perform tokenization on the train sentences and the values \"\"\"\n",
    "    tokenizer_obj = dictionary.Dictionary().create()\n",
    "    if mode == \"A\":\n",
    "        split_mode = tokenizer.Tokenizer.SplitMode.A\n",
    "    elif mode == \"B\":\n",
    "        split_mode = tokenizer.Tokenizer.SplitMode.B\n",
    "    elif mode == \"C\":\n",
    "        split_mode = tokenizer.Tokenizer.SplitMode.C\n",
    "    else:\n",
    "        raise ValueError(\"Invalid Mode: only A, B or C is allowed\")\n",
    "\n",
    "    return \"\".join([m.surface() for m in tokenizer_obj.tokenize(sentence, split_mode)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f = open(\"scr3_new.txt\", \"w\")\n",
    "\n",
    "\n",
    "#_ = str(row[\"Labeled Cleaned Transcription\"])\n",
    "_ = 'はい、かしこまりました<PERSON>渋谷</PERSON>様ですね.はい、 [filler/] お客様情報確認ができました.'\n",
    "flag = 0\n",
    "q = \"\"\n",
    "p =''\n",
    "temp = clean(_)\n",
    "for i in range(len(_)): \n",
    "    tag=''\n",
    "    entity=''\n",
    "    if _[i]=='<' and _[i+1] != \"/\":\n",
    "        flag = 1\n",
    "        i=i+1\n",
    "        while _[i] != \">\":\n",
    "            tag = tag + _[i]\n",
    "            i=i+1\n",
    "        i=i+1\n",
    "        while _[i] != \"<\":\n",
    "            entity = entity + _[i]\n",
    "            i=i+1\n",
    "        #p = p + entity + \"\\n\"\n",
    "        if q == \"\":\n",
    "            q = q + entity + \" - \" + tag\n",
    "        else:\n",
    "            q = q + \"\\t\" + entity + \" - \" + tag\n",
    "if flag:\n",
    "    p = temp + \"\\t\" + q\n",
    "    #f.write(p + \"\\n\")\n",
    "    #if tag!=\"\" and entity != \"\":\n",
    "    #    print(tag,entity)\n",
    "    # 大阪ガスお客様センター - ORG,小池 - PERSON\n",
    "else:\n",
    "    p = temp\n",
    "    #f.write(p + \"\\n\")\n",
    "print(p)\n",
    "#f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import random # random function for to remove bais in Traning Data\n",
    "\n",
    "# for batch parsing \n",
    "from spacy.util import minibatch, compounding\n",
    "import json\n",
    "# For evaluateing the model from testing set\n",
    "from spacy.gold import GoldParse\n",
    "from spacy.scorer import Scorer\n",
    "import pickle\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "nlp = spacy.load('ja_core_news_lg')\n",
    "\n",
    "with open('new_data_sudachy.json', mode='r', encoding='utf8') as train_file:\n",
    "    train_json = json.load(train_file)\n",
    "train_data = train_json[\"train_data\"]\n",
    "\n",
    "with open('krypton_eval_sudachy_new.json', mode='r', encoding='utf8') as train_file:\n",
    "    train_json = json.load(train_file)\n",
    "test_data = train_json[\"train_data\"]\n",
    "\n",
    "def spacy_retrain(train_data_file, new_model_location):\n",
    "    with open(train_data_file, mode='r', encoding='utf8') as train_file:\n",
    "        train_json = json.load(train_file)\n",
    "    train_data = train_json[\"train_data\"]\n",
    "\n",
    "    # For reproducing same results during multiple run\n",
    "    s = 42\n",
    "    np.random.seed(s)\n",
    "    spacy.util.fix_random_seed(s)\n",
    "    \n",
    "    #optimizer = Adam(\n",
    "    #learning_rate=0.001\n",
    "    #)\n",
    "    optimizer = nlp.entity.create_optimizer()\n",
    "    \n",
    "    ner = nlp.get_pipe('ner')\n",
    "    for _, annotations in train_data:\n",
    "        for ent in annotations.get(\"entities\"):\n",
    "            ner.add_label(ent[2])\n",
    "    \n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "    print(f'[OtherPipes] = {other_pipes} will be disabled')\n",
    "\n",
    "    model = nlp\n",
    "    X = []\n",
    "    y = []\n",
    "    for text, annotations in train_data:\n",
    "        X.append(text)\n",
    "        y.append(annotations)\n",
    "\n",
    "    n_iter = 5\n",
    "    loss = []\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train ner\n",
    "        if model is None:\n",
    "            optimizer = nlp.begin_training()\n",
    "        else:\n",
    "            optimizer = nlp.resume_training()\n",
    "        for i in range(n_iter):\n",
    "            losses = {}\n",
    "            nlp.update(X, y, sgd=optimizer, drop=0.25, losses=losses)\n",
    "            # nlp.entity.update(d, g)\n",
    "            print(\"Losses\", losses)\n",
    "            loss.append(losses['ner'])\n",
    "            if losses['ner'] <= min(loss):\n",
    "                flag = i\n",
    "                nlp.to_disk(new_model_location)\n",
    "    return loss, flag\n",
    "      \n",
    "\n",
    "train_file = r'full_filler_scr.json'\n",
    "new_model_loc = r'./full_fuller_try'\n",
    "loss,f = spacy_retrain(train_file, new_model_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadNERModel(modelName = \"modelTrained\"):\n",
    "    nlp = spacy.load(modelName)\n",
    "    return nlp\n",
    "\n",
    "def evaluate(ner_model, examples):\n",
    "    scorer = Scorer()\n",
    "     \n",
    "    #loading tags for each input and Evaluating them\n",
    "    for input_, annotations in examples:\n",
    "        tags = []\n",
    "        # loading text\n",
    "        doc_gold_text = ner_model.make_doc(input_)\n",
    "        \n",
    "        #loading all tags for that text\n",
    "        for ent in annotations.get('entities'):\n",
    "            tags.append(ent)\n",
    "            \n",
    "        # Evaluating the tags    \n",
    "        gold = GoldParse(doc_gold_text, entities=tags)\n",
    "        pred_value = ner_model(input_)\n",
    "        \n",
    "        scorer.score(pred_value, gold)\n",
    "        \n",
    "        \n",
    "    return scorer.scores\n",
    "\n",
    "model = loadNERModel('./full_scr1_filler/')\n",
    "print(evaluate(model, test_data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BERT",
   "language": "python",
   "name": "bert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
