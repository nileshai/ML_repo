{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "# import bert\n",
    "# from bert import BertModelLayer\n",
    "import functools\n",
    "\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import sentencepiece as spm\n",
    "\n",
    "import tf_sentencepiece as tfs\n",
    "import tensorflow_text as tftext\n",
    "import sys\n",
    "sys.path.extend([\"/space/users/jgeorge/git/tensorflow_repos/models\"])\n",
    "sys.path.extend([\"/space/users/jgeorge/git/tensorflow_repos/\"])\n",
    "sys.path.extend([\"/space/users/jgeorge/git/tensorflow_repos/sentencepiece/compiled_proto\"])\n",
    "# sys.path.extend([\"/space/users/jgeorge/git/tensorflow_repos/sentencepiece\"])\n",
    "\n",
    "\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import csv\n",
    "import collections\n",
    "import pandas as pd\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from absl import logging\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# pylint: disable=g-import-not-at-top,redefined-outer-name,reimported\n",
    "# from official.modeling import model_training_utils\n",
    "\n",
    "# from official.nlp.modeling.models import bert_classifier, bert_pretrainer\n",
    "# # from official.nlp.modeling.models.bert_classifier import BertClassifier\n",
    "# # from official.nlp.modeling.models.bert_pretrainer import BertPretrainer\n",
    "# # from official.nlp import bert_modeling as modeling\n",
    "# # from official.nlp import bert_models\n",
    "# from official.nlp import optimization\n",
    "# from official.nlp.bert import common_flags\n",
    "# from official.nlp.bert import input_pipeline\n",
    "# from official.nlp.bert import model_saving_utils\n",
    "# from official.utils.misc import distribution_utils\n",
    "# from official.utils.misc import keras_utils\n",
    "# from official.nlp.bert import tokenization\n",
    "\n",
    "# from official.nlp.albert import configs as albert_configs\n",
    "# from official.nlp.bert import run_classifier as run_classifier_bert\n",
    "\n",
    "from tensorflow.python.keras.utils import tf_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from bert import tokenization as bert_tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import sentencepiece_model_pb2\n",
    "import sentencepiece_pb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/space/users/jgeorge/git/tensorflow_experiments\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:4', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:5', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:6', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:7', device_type='GPU')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.experimental.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:4', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "##restricting no of gpus\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "device_to_use = gpus[4]\n",
    "tf.config.experimental.set_memory_growth(device_to_use,True)\n",
    "tf.config.experimental.set_visible_devices(device_to_use, 'GPU')\n",
    "print(tf.config.experimental.get_visible_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##setting direcory for downloading tfhub modules\n",
    "os.environ[\"TFHUB_CACHE_DIR\"] = '/space/engineering/tfhub_modules'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_piece_folder=\"/var/extra/users/jgeorge/tf2.0/sentence_piece\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_piece_model_path=os.path.join(sentence_piece_folder,\"multilingual.proto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file=\"/space/engineering/pretrained_models/bert/multi_cased_L-12_H-768_A-12/vocab.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dish_small_spm_file='/var/extra/users/jgeorge/git/tensorflow_repos/sentencepiece/build/models/spm_dish.model'\n",
    "dish_small_spm_vocab='/var/extra/users/jgeorge/git/tensorflow_repos/sentencepiece/build/models/spm_dish.vocab'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentence_piece_processor = spm.SentencePieceProcessor(model_file=dish_small_spm_file)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sentence_piece_trainer = spm.SentencePieceTrainer.Train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_full_tokenizer = bert_tokenization.FullTokenizer(vocab_file,do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['talk', 'to', 'an', 'agent']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_full_tokenizer.tokenize(\"talk to an agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['talk', 'to', 'aa', '##gent']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_full_tokenizer.tokenize(\"talk to aagent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'talk to aagent'>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant(\"talk to aagent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['talk', 'to', 'aa', '##gent']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_full_tokenizer.tokenize(\"talk to aagent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[31311, 10114, 10151, 18980]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_full_tokenizer.convert_tokens_to_ids(bert_full_tokenizer.tokenize(\"talk to an agent\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = []\n",
    "with open(vocab_file,'r') as inp_f:\n",
    "    for word in inp_f:\n",
    "        vocab.append(word.strip())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[PAD]',\n",
       " '[unused1]',\n",
       " '[unused2]',\n",
       " '[unused3]',\n",
       " '[unused4]',\n",
       " '[unused5]',\n",
       " '[unused6]',\n",
       " '[unused7]',\n",
       " '[unused8]',\n",
       " '[unused9]',\n",
       " '[unused10]',\n",
       " '[unused11]',\n",
       " '[unused12]',\n",
       " '[unused13]',\n",
       " '[unused14]',\n",
       " '[unused15]',\n",
       " '[unused16]',\n",
       " '[unused17]',\n",
       " '[unused18]',\n",
       " '[unused19]',\n",
       " '[unused20]',\n",
       " '[unused21]',\n",
       " '[unused22]',\n",
       " '[unused23]',\n",
       " '[unused24]',\n",
       " '[unused25]',\n",
       " '[unused26]',\n",
       " '[unused27]',\n",
       " '[unused28]',\n",
       " '[unused29]',\n",
       " '[unused30]',\n",
       " '[unused31]',\n",
       " '[unused32]',\n",
       " '[unused33]',\n",
       " '[unused34]',\n",
       " '[unused35]',\n",
       " '[unused36]',\n",
       " '[unused37]',\n",
       " '[unused38]',\n",
       " '[unused39]',\n",
       " '[unused40]',\n",
       " '[unused41]',\n",
       " '[unused42]',\n",
       " '[unused43]',\n",
       " '[unused44]',\n",
       " '[unused45]',\n",
       " '[unused46]',\n",
       " '[unused47]',\n",
       " '[unused48]',\n",
       " '[unused49]',\n",
       " '[unused50]',\n",
       " '[unused51]',\n",
       " '[unused52]',\n",
       " '[unused53]',\n",
       " '[unused54]',\n",
       " '[unused55]',\n",
       " '[unused56]',\n",
       " '[unused57]',\n",
       " '[unused58]',\n",
       " '[unused59]',\n",
       " '[unused60]',\n",
       " '[unused61]',\n",
       " '[unused62]',\n",
       " '[unused63]',\n",
       " '[unused64]',\n",
       " '[unused65]',\n",
       " '[unused66]',\n",
       " '[unused67]',\n",
       " '[unused68]',\n",
       " '[unused69]',\n",
       " '[unused70]',\n",
       " '[unused71]',\n",
       " '[unused72]',\n",
       " '[unused73]',\n",
       " '[unused74]',\n",
       " '[unused75]',\n",
       " '[unused76]',\n",
       " '[unused77]',\n",
       " '[unused78]',\n",
       " '[unused79]',\n",
       " '[unused80]',\n",
       " '[unused81]',\n",
       " '[unused82]',\n",
       " '[unused83]',\n",
       " '[unused84]',\n",
       " '[unused85]',\n",
       " '[unused86]',\n",
       " '[unused87]',\n",
       " '[unused88]',\n",
       " '[unused89]',\n",
       " '[unused90]',\n",
       " '[unused91]',\n",
       " '[unused92]',\n",
       " '[unused93]',\n",
       " '[unused94]',\n",
       " '[unused95]',\n",
       " '[unused96]',\n",
       " '[unused97]',\n",
       " '[unused98]',\n",
       " '[unused99]',\n",
       " '[UNK]',\n",
       " '[CLS]',\n",
       " '[SEP]',\n",
       " '[MASK]',\n",
       " '<S>',\n",
       " '<T>',\n",
       " '!',\n",
       " '\"',\n",
       " '#',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " '+',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " ':',\n",
       " ';',\n",
       " '<',\n",
       " '=',\n",
       " '>',\n",
       " '?',\n",
       " '@',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'X',\n",
       " 'Y',\n",
       " 'Z',\n",
       " '[',\n",
       " '\\\\',\n",
       " ']',\n",
       " '^',\n",
       " '_',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z',\n",
       " '{',\n",
       " '|',\n",
       " '}',\n",
       " '~',\n",
       " '¡',\n",
       " '¢',\n",
       " '£',\n",
       " '¥',\n",
       " '¦',\n",
       " '§',\n",
       " '¨',\n",
       " '©',\n",
       " 'ª',\n",
       " '«',\n",
       " '¬',\n",
       " '®',\n",
       " '°',\n",
       " '±',\n",
       " '²',\n",
       " '³',\n",
       " 'µ',\n",
       " '¶',\n",
       " '·',\n",
       " '¹',\n",
       " 'º',\n",
       " '»',\n",
       " '¼',\n",
       " '½',\n",
       " '¾',\n",
       " '¿',\n",
       " 'À',\n",
       " 'Á',\n",
       " 'Â',\n",
       " 'Ã',\n",
       " 'Ä',\n",
       " 'Å',\n",
       " 'Æ',\n",
       " 'Ç',\n",
       " 'È',\n",
       " 'É',\n",
       " 'Ê',\n",
       " 'Ë',\n",
       " 'Ì',\n",
       " 'Í',\n",
       " 'Î',\n",
       " 'Ð',\n",
       " 'Ñ',\n",
       " 'Ò',\n",
       " 'Ó',\n",
       " 'Ô',\n",
       " 'Õ',\n",
       " 'Ö',\n",
       " '×',\n",
       " 'Ø',\n",
       " 'Ú',\n",
       " 'Ü',\n",
       " 'Ý',\n",
       " 'Þ',\n",
       " 'ß',\n",
       " 'à',\n",
       " 'á',\n",
       " 'â',\n",
       " 'ã',\n",
       " 'ä',\n",
       " 'å',\n",
       " 'æ',\n",
       " 'ç',\n",
       " 'è',\n",
       " 'é',\n",
       " 'ê',\n",
       " 'ë',\n",
       " 'ì',\n",
       " 'í',\n",
       " 'î',\n",
       " 'ï',\n",
       " 'ð',\n",
       " 'ñ',\n",
       " 'ò',\n",
       " 'ó',\n",
       " 'ô',\n",
       " 'õ',\n",
       " 'ö',\n",
       " '÷',\n",
       " 'ø',\n",
       " 'ù',\n",
       " 'ú',\n",
       " 'û',\n",
       " 'ü',\n",
       " 'ý',\n",
       " 'þ',\n",
       " 'ÿ',\n",
       " 'Ā',\n",
       " 'ā',\n",
       " 'Ă',\n",
       " 'ă',\n",
       " 'Ą',\n",
       " 'ą',\n",
       " 'Ć',\n",
       " 'ć',\n",
       " 'Č',\n",
       " 'č',\n",
       " 'Ď',\n",
       " 'ď',\n",
       " 'Đ',\n",
       " 'đ',\n",
       " 'Ē',\n",
       " 'ē',\n",
       " 'Ĕ',\n",
       " 'ĕ',\n",
       " 'Ė',\n",
       " 'ė',\n",
       " 'ę',\n",
       " 'ě',\n",
       " 'Ğ',\n",
       " 'ğ',\n",
       " 'ġ',\n",
       " 'Ģ',\n",
       " 'ģ',\n",
       " 'Ħ',\n",
       " 'ħ',\n",
       " 'ĩ',\n",
       " 'Ī',\n",
       " 'ī',\n",
       " 'Į',\n",
       " 'į',\n",
       " 'İ',\n",
       " 'ı',\n",
       " 'Ķ',\n",
       " 'ķ',\n",
       " 'ĺ',\n",
       " 'Ļ',\n",
       " 'ļ',\n",
       " 'Ľ',\n",
       " 'ľ',\n",
       " 'Ł',\n",
       " 'ł',\n",
       " 'ń',\n",
       " 'Ņ',\n",
       " 'ņ',\n",
       " 'ň',\n",
       " 'ŉ',\n",
       " 'ŋ',\n",
       " 'Ō',\n",
       " 'ō',\n",
       " 'ŏ',\n",
       " 'Ő',\n",
       " 'ő',\n",
       " 'Œ',\n",
       " 'œ',\n",
       " 'ŕ',\n",
       " 'Ř',\n",
       " 'ř',\n",
       " 'Ś',\n",
       " 'ś',\n",
       " 'Ş',\n",
       " 'ş',\n",
       " 'Š',\n",
       " 'š',\n",
       " 'Ţ',\n",
       " 'ţ',\n",
       " 'Ť',\n",
       " 'ť',\n",
       " 'ũ',\n",
       " 'Ū',\n",
       " 'ū',\n",
       " 'ŭ',\n",
       " 'ů',\n",
       " 'Ű',\n",
       " 'ű',\n",
       " 'ų',\n",
       " 'ŵ',\n",
       " 'ŷ',\n",
       " 'Ź',\n",
       " 'ź',\n",
       " 'Ż',\n",
       " 'ż',\n",
       " 'Ž',\n",
       " 'ž',\n",
       " 'Ə',\n",
       " 'ƒ',\n",
       " 'ơ',\n",
       " 'Ư',\n",
       " 'ư',\n",
       " 'ǎ',\n",
       " 'ǐ',\n",
       " 'ǔ',\n",
       " 'ǫ',\n",
       " 'ǹ',\n",
       " 'Ș',\n",
       " 'ș',\n",
       " 'Ț',\n",
       " 'ț',\n",
       " 'ɐ',\n",
       " 'ɑ',\n",
       " 'ɔ',\n",
       " 'ɕ',\n",
       " 'ə',\n",
       " 'ɛ',\n",
       " 'ɡ',\n",
       " 'ɣ',\n",
       " 'ɨ',\n",
       " 'ɪ',\n",
       " 'ɲ',\n",
       " 'ɾ',\n",
       " 'ʁ',\n",
       " 'ʃ',\n",
       " 'ʊ',\n",
       " 'ʎ',\n",
       " 'ʒ',\n",
       " 'ʔ',\n",
       " 'ʙ',\n",
       " 'ʰ',\n",
       " 'ʲ',\n",
       " 'ʳ',\n",
       " 'ʷ',\n",
       " 'ʸ',\n",
       " 'ʻ',\n",
       " 'ʼ',\n",
       " 'ʾ',\n",
       " 'ʿ',\n",
       " 'ˈ',\n",
       " 'ː',\n",
       " 'ˡ',\n",
       " 'ˢ',\n",
       " '̀',\n",
       " '́',\n",
       " '̃',\n",
       " '̄',\n",
       " '̍',\n",
       " '̥',\n",
       " '̧',\n",
       " '̲',\n",
       " '͡',\n",
       " '΄',\n",
       " 'Ά',\n",
       " 'Έ',\n",
       " 'Ή',\n",
       " 'Ί',\n",
       " 'Ό',\n",
       " 'Ύ',\n",
       " 'Ώ',\n",
       " 'ΐ',\n",
       " 'Α',\n",
       " 'Β',\n",
       " 'Γ',\n",
       " 'Δ',\n",
       " 'Ε',\n",
       " 'Ζ',\n",
       " 'Η',\n",
       " 'Θ',\n",
       " 'Ι',\n",
       " 'Κ',\n",
       " 'Λ',\n",
       " 'Μ',\n",
       " 'Ν',\n",
       " 'Ξ',\n",
       " 'Ο',\n",
       " 'Π',\n",
       " 'Ρ',\n",
       " 'Σ',\n",
       " 'Τ',\n",
       " 'Υ',\n",
       " 'Φ',\n",
       " 'Χ',\n",
       " 'Ψ',\n",
       " 'Ω',\n",
       " 'ά',\n",
       " 'έ',\n",
       " 'ή',\n",
       " 'ί',\n",
       " 'α',\n",
       " 'β',\n",
       " 'γ',\n",
       " 'δ',\n",
       " 'ε',\n",
       " 'ζ',\n",
       " 'η',\n",
       " 'θ',\n",
       " 'ι',\n",
       " 'κ',\n",
       " 'λ',\n",
       " 'μ',\n",
       " 'ν',\n",
       " 'ξ',\n",
       " 'ο',\n",
       " 'π',\n",
       " 'ρ',\n",
       " 'ς',\n",
       " 'σ',\n",
       " 'τ',\n",
       " 'υ',\n",
       " 'φ',\n",
       " 'χ',\n",
       " 'ψ',\n",
       " 'ω',\n",
       " 'ϊ',\n",
       " 'ϋ',\n",
       " 'ό',\n",
       " 'ύ',\n",
       " 'ώ',\n",
       " 'Ё',\n",
       " 'Ђ',\n",
       " 'Ѓ',\n",
       " 'Є',\n",
       " 'Ѕ',\n",
       " 'І',\n",
       " 'Ї',\n",
       " 'Ј',\n",
       " 'Љ',\n",
       " 'Њ',\n",
       " 'Ћ',\n",
       " 'Ќ',\n",
       " 'Ў',\n",
       " 'Џ',\n",
       " 'А',\n",
       " 'Б',\n",
       " 'В',\n",
       " 'Г',\n",
       " 'Д',\n",
       " 'Е',\n",
       " 'Ж',\n",
       " 'З',\n",
       " 'И',\n",
       " 'Й',\n",
       " 'К',\n",
       " 'Л',\n",
       " 'М',\n",
       " 'Н',\n",
       " 'О',\n",
       " 'П',\n",
       " 'Р',\n",
       " 'С',\n",
       " 'Т',\n",
       " 'У',\n",
       " 'Ф',\n",
       " 'Х',\n",
       " 'Ц',\n",
       " 'Ч',\n",
       " 'Ш',\n",
       " 'Щ',\n",
       " 'Ъ',\n",
       " 'Ы',\n",
       " 'Ь',\n",
       " 'Э',\n",
       " 'Ю',\n",
       " 'Я',\n",
       " 'а',\n",
       " 'б',\n",
       " 'в',\n",
       " 'г',\n",
       " 'д',\n",
       " 'е',\n",
       " 'ж',\n",
       " 'з',\n",
       " 'и',\n",
       " 'й',\n",
       " 'к',\n",
       " 'л',\n",
       " 'м',\n",
       " 'н',\n",
       " 'о',\n",
       " 'п',\n",
       " 'р',\n",
       " 'с',\n",
       " 'т',\n",
       " 'у',\n",
       " 'ф',\n",
       " 'х',\n",
       " 'ц',\n",
       " 'ч',\n",
       " 'ш',\n",
       " 'щ',\n",
       " 'ъ',\n",
       " 'ы',\n",
       " 'ь',\n",
       " 'э',\n",
       " 'ю',\n",
       " 'я',\n",
       " 'ѐ',\n",
       " 'ё',\n",
       " 'ђ',\n",
       " 'ѓ',\n",
       " 'є',\n",
       " 'ѕ',\n",
       " 'і',\n",
       " 'ї',\n",
       " 'ј',\n",
       " 'љ',\n",
       " 'њ',\n",
       " 'ћ',\n",
       " 'ќ',\n",
       " 'ѝ',\n",
       " 'ў',\n",
       " 'џ',\n",
       " 'ѣ',\n",
       " 'Ґ',\n",
       " 'ґ',\n",
       " 'Ғ',\n",
       " 'ғ',\n",
       " 'Җ',\n",
       " 'җ',\n",
       " 'Ҙ',\n",
       " 'ҙ',\n",
       " 'Қ',\n",
       " 'қ',\n",
       " 'Ҡ',\n",
       " 'ҡ',\n",
       " 'ң',\n",
       " 'ҫ',\n",
       " 'Ү',\n",
       " 'ү',\n",
       " 'Ұ',\n",
       " 'ұ',\n",
       " 'Ҳ',\n",
       " 'ҳ',\n",
       " 'Ҷ',\n",
       " 'ҷ',\n",
       " 'Һ',\n",
       " 'һ',\n",
       " 'Ӏ',\n",
       " 'ӑ',\n",
       " 'ӗ',\n",
       " 'Ә',\n",
       " 'ә',\n",
       " 'ӣ',\n",
       " 'Ө',\n",
       " 'ө',\n",
       " 'Ӯ',\n",
       " 'ӯ',\n",
       " 'ӳ',\n",
       " 'Ա',\n",
       " 'Բ',\n",
       " 'Գ',\n",
       " 'Դ',\n",
       " 'Ե',\n",
       " 'Զ',\n",
       " 'Է',\n",
       " 'Ը',\n",
       " 'Թ',\n",
       " 'Ժ',\n",
       " 'Ի',\n",
       " 'Լ',\n",
       " 'Խ',\n",
       " 'Ծ',\n",
       " 'Կ',\n",
       " 'Հ',\n",
       " 'Ձ',\n",
       " 'Ղ',\n",
       " 'Ճ',\n",
       " 'Մ',\n",
       " 'Յ',\n",
       " 'Ն',\n",
       " 'Շ',\n",
       " 'Ո',\n",
       " 'Չ',\n",
       " 'Պ',\n",
       " 'Ջ',\n",
       " 'Ռ',\n",
       " 'Ս',\n",
       " 'Վ',\n",
       " 'Տ',\n",
       " 'Ր',\n",
       " 'Ց',\n",
       " 'Ւ',\n",
       " 'Փ',\n",
       " 'Ք',\n",
       " 'Օ',\n",
       " 'Ֆ',\n",
       " '՚',\n",
       " '՛',\n",
       " '՜',\n",
       " '՝',\n",
       " '՞',\n",
       " 'ա',\n",
       " 'բ',\n",
       " 'գ',\n",
       " 'դ',\n",
       " 'ե',\n",
       " 'զ',\n",
       " 'է',\n",
       " 'ը',\n",
       " 'թ',\n",
       " 'ժ',\n",
       " 'ի',\n",
       " 'լ',\n",
       " 'խ',\n",
       " 'ծ',\n",
       " 'կ',\n",
       " 'հ',\n",
       " 'ձ',\n",
       " 'ղ',\n",
       " 'ճ',\n",
       " 'մ',\n",
       " 'յ',\n",
       " 'ն',\n",
       " 'շ',\n",
       " 'ո',\n",
       " 'չ',\n",
       " 'պ',\n",
       " 'ջ',\n",
       " 'ռ',\n",
       " 'ս',\n",
       " 'վ',\n",
       " 'տ',\n",
       " 'ր',\n",
       " 'ց',\n",
       " 'ւ',\n",
       " 'փ',\n",
       " 'ք',\n",
       " 'օ',\n",
       " 'ֆ',\n",
       " 'և',\n",
       " '։',\n",
       " '֊',\n",
       " 'ְ',\n",
       " 'ֱ',\n",
       " 'ֲ',\n",
       " 'ִ',\n",
       " 'ֵ',\n",
       " 'ֶ',\n",
       " 'ַ',\n",
       " 'ָ',\n",
       " 'ֹ',\n",
       " 'ּ',\n",
       " '־',\n",
       " 'ׁ',\n",
       " 'ׂ',\n",
       " '׃',\n",
       " 'א',\n",
       " 'ב',\n",
       " 'ג',\n",
       " 'ד',\n",
       " 'ה',\n",
       " 'ו',\n",
       " 'ז',\n",
       " 'ח',\n",
       " 'ט',\n",
       " 'י',\n",
       " 'ך',\n",
       " 'כ',\n",
       " 'ל',\n",
       " 'ם',\n",
       " 'מ',\n",
       " 'ן',\n",
       " 'נ',\n",
       " 'ס',\n",
       " 'ע',\n",
       " 'ף',\n",
       " 'פ',\n",
       " 'ץ',\n",
       " 'צ',\n",
       " 'ק',\n",
       " 'ר',\n",
       " 'ש',\n",
       " 'ת',\n",
       " '׳',\n",
       " '״',\n",
       " '،',\n",
       " '؍',\n",
       " 'ؒ',\n",
       " '؛',\n",
       " '؟',\n",
       " 'ء',\n",
       " 'آ',\n",
       " 'أ',\n",
       " 'ؤ',\n",
       " 'إ',\n",
       " 'ئ',\n",
       " 'ا',\n",
       " 'ب',\n",
       " 'ة',\n",
       " 'ت',\n",
       " 'ث',\n",
       " 'ج',\n",
       " 'ح',\n",
       " 'خ',\n",
       " 'د',\n",
       " 'ذ',\n",
       " 'ر',\n",
       " 'ز',\n",
       " 'س',\n",
       " 'ش',\n",
       " 'ص',\n",
       " 'ض',\n",
       " 'ط',\n",
       " 'ظ',\n",
       " 'ع',\n",
       " 'غ',\n",
       " 'ـ',\n",
       " 'ف',\n",
       " 'ق',\n",
       " 'ك',\n",
       " 'ل',\n",
       " 'م',\n",
       " 'ن',\n",
       " 'ه',\n",
       " 'و',\n",
       " 'ى',\n",
       " 'ي',\n",
       " 'ً',\n",
       " 'ٌ',\n",
       " 'ٍ',\n",
       " 'َ',\n",
       " 'ُ',\n",
       " 'ِ',\n",
       " 'ّ',\n",
       " 'ْ',\n",
       " 'ٔ',\n",
       " 'ٛ',\n",
       " '٠',\n",
       " '١',\n",
       " '٢',\n",
       " '٣',\n",
       " '٤',\n",
       " '٥',\n",
       " '٩',\n",
       " '٪',\n",
       " '٫',\n",
       " '٬',\n",
       " '٭',\n",
       " 'ٰ',\n",
       " 'ٹ',\n",
       " 'پ',\n",
       " 'چ',\n",
       " 'ڈ',\n",
       " 'ڑ',\n",
       " 'ژ',\n",
       " 'ڤ',\n",
       " 'ک',\n",
       " 'ڭ',\n",
       " 'گ',\n",
       " 'ں',\n",
       " 'ھ',\n",
       " 'ۀ',\n",
       " 'ہ',\n",
       " 'ۂ',\n",
       " 'ۃ',\n",
       " 'ۆ',\n",
       " 'ۇ',\n",
       " 'ی',\n",
       " 'ے',\n",
       " 'ۓ',\n",
       " '۔',\n",
       " '۰',\n",
       " '۱',\n",
       " '۲',\n",
       " '۳',\n",
       " '۴',\n",
       " '۵',\n",
       " '۶',\n",
       " '۷',\n",
       " '۸',\n",
       " '۹',\n",
       " 'ँ',\n",
       " 'ं',\n",
       " 'ः',\n",
       " 'अ',\n",
       " 'आ',\n",
       " 'इ',\n",
       " 'ई',\n",
       " 'उ',\n",
       " 'ऊ',\n",
       " 'ऋ',\n",
       " 'ऍ',\n",
       " 'ऎ',\n",
       " 'ए',\n",
       " 'ऐ',\n",
       " 'ऑ',\n",
       " 'ओ',\n",
       " 'औ',\n",
       " 'क',\n",
       " 'ख',\n",
       " 'ग',\n",
       " 'घ',\n",
       " 'ङ',\n",
       " 'च',\n",
       " 'छ',\n",
       " 'ज',\n",
       " 'झ',\n",
       " 'ञ',\n",
       " 'ट',\n",
       " 'ठ',\n",
       " 'ड',\n",
       " 'ढ',\n",
       " 'ण',\n",
       " 'त',\n",
       " 'थ',\n",
       " 'द',\n",
       " 'ध',\n",
       " 'न',\n",
       " 'प',\n",
       " 'फ',\n",
       " 'ब',\n",
       " 'भ',\n",
       " 'म',\n",
       " 'य',\n",
       " 'र',\n",
       " 'ऱ',\n",
       " 'ल',\n",
       " 'ळ',\n",
       " 'व',\n",
       " 'श',\n",
       " 'ष',\n",
       " 'स',\n",
       " 'ह',\n",
       " '़',\n",
       " 'ऽ',\n",
       " 'ा',\n",
       " 'ि',\n",
       " 'ी',\n",
       " 'ु',\n",
       " 'ू',\n",
       " 'ृ',\n",
       " 'ॄ',\n",
       " 'ॅ',\n",
       " 'ॆ',\n",
       " 'े',\n",
       " 'ै',\n",
       " 'ॉ',\n",
       " 'ो',\n",
       " 'ौ',\n",
       " '्',\n",
       " 'ॐ',\n",
       " '॓',\n",
       " 'ॠ',\n",
       " '।',\n",
       " '॥',\n",
       " '०',\n",
       " '१',\n",
       " '२',\n",
       " '३',\n",
       " '४',\n",
       " '५',\n",
       " '६',\n",
       " '७',\n",
       " '८',\n",
       " '९',\n",
       " '॰',\n",
       " 'ॲ',\n",
       " 'ঁ',\n",
       " 'ং',\n",
       " 'ঃ',\n",
       " 'অ',\n",
       " 'আ',\n",
       " 'ই',\n",
       " 'ঈ',\n",
       " 'উ',\n",
       " 'ঊ',\n",
       " 'ঋ',\n",
       " 'এ',\n",
       " 'ঐ',\n",
       " 'ও',\n",
       " 'ঔ',\n",
       " 'ক',\n",
       " 'খ',\n",
       " 'গ',\n",
       " 'ঘ',\n",
       " 'ঙ',\n",
       " 'চ',\n",
       " 'ছ',\n",
       " 'জ',\n",
       " 'ঝ',\n",
       " 'ঞ',\n",
       " 'ট',\n",
       " 'ঠ',\n",
       " 'ড',\n",
       " 'ঢ',\n",
       " 'ণ',\n",
       " 'ত',\n",
       " 'থ',\n",
       " 'দ',\n",
       " 'ধ',\n",
       " 'ন',\n",
       " 'প',\n",
       " 'ফ',\n",
       " 'ব',\n",
       " 'ভ',\n",
       " 'ম',\n",
       " 'য',\n",
       " 'র',\n",
       " 'ল',\n",
       " 'শ',\n",
       " 'ষ',\n",
       " 'স',\n",
       " 'হ',\n",
       " '়',\n",
       " 'া',\n",
       " 'ি',\n",
       " 'ী',\n",
       " 'ু',\n",
       " 'ূ',\n",
       " 'ৃ',\n",
       " 'ে',\n",
       " 'ৈ',\n",
       " 'ো',\n",
       " 'ৌ',\n",
       " '্',\n",
       " 'ৎ',\n",
       " '০',\n",
       " '১',\n",
       " '২',\n",
       " '৩',\n",
       " '৪',\n",
       " '৫',\n",
       " '৬',\n",
       " ...]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "start  0\n",
      "substr is  talk\n",
      "in vocab  talk\n",
      "cur_substr end talk\n",
      "start after update  4 end after update  4\n",
      "2\n",
      "start  0\n",
      "substr is  to\n",
      "in vocab  to\n",
      "cur_substr end to\n",
      "start after update  2 end after update  2\n",
      "6\n",
      "start  0\n",
      "substr is  aagent\n",
      "substr is  aagen\n",
      "substr is  aage\n",
      "substr is  aag\n",
      "substr is  aa\n",
      "in vocab  aa\n",
      "cur_substr end aa\n",
      "start after update  2 end after update  2\n",
      "start  2\n",
      "substr is  ##gent\n",
      "in vocab  ##gent\n",
      "cur_substr end ##gent\n",
      "start after update  6 end after update  6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['talk', 'to', 'aa', '##gent']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_custom(text):\n",
    "    output_tokens = []\n",
    "    unk_token=\"UNK\"\n",
    "    for token in text.split():\n",
    "        sub_tokens = []\n",
    "        chars = list(token)\n",
    "        is_bad = False\n",
    "        start = 0\n",
    "        print(len(chars))\n",
    "        while start < len(chars):\n",
    "            print(\"start \",start)\n",
    "            end = len(chars)\n",
    "            cur_substr=None\n",
    "            while start < end:\n",
    "                substr = \"\".join(chars[start:end])\n",
    "                if start>0:\n",
    "                    substr = \"##\"+substr\n",
    "                print(\"substr is \",substr)\n",
    "                if substr in vocab:\n",
    "                    print(\"in vocab \",substr)\n",
    "                    cur_substr = substr\n",
    "                    break\n",
    "                end-=1\n",
    "            if cur_substr is None:\n",
    "                is_bad=True\n",
    "                break\n",
    "            print(\"cur_substr end\", cur_substr)\n",
    "            sub_tokens.append(cur_substr)\n",
    "            start = end\n",
    "            print(\"start after update \",start, \"end after update \",end)\n",
    "        if is_bad:\n",
    "            output_tokens.append(unk_token)\n",
    "        else:\n",
    "            output_tokens.extend(sub_tokens)\n",
    "    return output_tokens\n",
    "                        \n",
    "tokenize_custom(\"talk to aagent\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([b''], dtype=object)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = tf.Variable([''],shape=(None,))\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=string, numpy=array([b'', b'abc', b'def'], dtype=object)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.concat([s,['abc', 'def']],axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=() dtype=bool, numpy=False>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_bad = tf.Variable(False)\n",
    "is_bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "missing_index=-1\n",
    "vocab_table = tf.lookup.StaticHashTable(\n",
    "            tf.lookup.TextFileInitializer(vocab_file,\n",
    "                                          key_dtype=tf.string,key_index=tf.lookup.TextFileIndex.WHOLE_LINE,\n",
    "                                     value_dtype=tf.int64,value_index=tf.lookup.TextFileIndex.LINE_NUMBER),default_value=missing_index)\n",
    "def is_vocab_missing(vocab_lookup):\n",
    "    return vocab_lookup==missing_index\n",
    "\n",
    "def inner_cond(start,end,word,chars,vocab_lookup):\n",
    "    cond = tf.less(start,end) and is_vocab_missing(vocab_lookup)\n",
    "    tf.print(f'vocab lookup inside inner_cond  start {start} end {end} condition {vocab_lookup} is_vocab_missing {is_vocab_missing(vocab_lookup)} & entire cond {cond}')\n",
    "    return cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chars_len(chars):\n",
    "    chars_len = tf.shape(chars)[0]\n",
    "    # when it is ragged tensor\n",
    "#     chars_len = chars.nrows()\n",
    "    return chars_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def inner_while_body(start,end,word,vocab_lookup):\n",
    "def inner_while_body(start,end,word,chars,vocab_lookup):\n",
    "    tf.print(f'inner_while_body start {start} end {end} word {word} chars {chars}\\n')\n",
    "#     chars = tf.strings.bytes_split(word)\n",
    "    substring = tf.strings.join(tf.slice(chars,begin=start,size=end-start))\n",
    "#     tf.strings.reduce_join()\n",
    "    if start>0:\n",
    "        substring = tf.strings.join([\"##\",substring])\n",
    "    # assigning it rather than creating a new variable so that this is accessible to other\n",
    "    # function referencing this variable\n",
    "    vocab_lookup.assign(vocab_table.lookup(tf.constant(substring)))\n",
    "    tf.print(f'substring {substring} vocab lookup {vocab_lookup}')\n",
    "    if vocab_lookup==missing_index:\n",
    "#         print(f'missing {substring}')\n",
    "        tf.print(f'missing {substring}')\n",
    "        # subtracting only if missing\n",
    "        end.assign_sub([1])\n",
    "    else:\n",
    "        tf.print(f'present {substring}')\n",
    "    \n",
    "    return (start,end,substring,chars,vocab_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outer_cond(start,end,word,chars,is_bad,sub_tokens):\n",
    "    tf.print(f'outer_cond start {start} end {end}  chars {chars} is_bad {is_bad} sub_tokens {sub_tokens}')\n",
    "    return (start < tf.shape(chars)[0]) and not is_bad\n",
    "\n",
    "def outer_while_loop(start,end,word,chars,is_bad,sub_tokens):\n",
    "    tf.print(f'outer_while_loop start {start} end {end}  chars {chars} is_bad {is_bad} sub_tokens {sub_tokens}')\n",
    "    # vocab_lookup = vocab_table.lookup(tf.constant(substring))\n",
    "    chars_len = get_chars_len(chars)\n",
    "    end = tf.Variable([chars_len])\n",
    "    substring = tf.strings.join(tf.slice(chars,begin=start,size=end-start))\n",
    "    vocab_table = tf.Variable(missing_index,shape=(),name='vocab_lookup',dtype=tf.int64)\n",
    "#     vocab_lookup.assign(vocab_table.lookup(tf.constant(substring)))\n",
    "\n",
    "    (start_1,end_1,substring_1,chars_1,vocab_lookup) = tf.while_loop(inner_cond,inner_while_body,loop_vars=[start,end,substring,chars,vocab_lookup])\n",
    "    tf.print(f'****end of inner while start {start_1} end {end_1} substring {substring_1} chars {chars_1} vocab_lookup {vocab_lookup} \\n\\n')\n",
    "    if not is_vocab_missing(vocab_lookup):\n",
    "        sub_tokens_concat = tf.concat([sub_tokens,[substring_1]],axis=0)\n",
    "        tf.print(\"sub tokens concat \",sub_tokens_concat, 'shape ',tf.shape(sub_tokens_concat))\n",
    "#         sub_tokens.append(substring)\n",
    "        tf.print('subtokens ',sub_tokens, ' sub tokens shape ',sub_tokens)\n",
    "        sub_tokens.assign(sub_tokens_concat)\n",
    "        tf.print(f'subtokens appended ')\n",
    "    else:\n",
    "        # if vocab missing even after the inner loop then this word can't be formed using\n",
    "        # word existing in the dictionary\n",
    "        is_bad.assign(True)\n",
    "    tf.print(f'start before assigning end {start} & end val {end}')\n",
    "    start.assign(end_1)\n",
    "    tf.print(f'current subtokens {sub_tokens}')\n",
    "    return (start,end,word,chars,is_bad,sub_tokens)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tf.concat([sub_tokens1,'agent'],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_word_tokenize(word):\n",
    "    word = tf.squeeze(word)\n",
    "    chars = tf.strings.bytes_split(word)#.to_tensor()\n",
    "    tf.print(f'per_word_tokenize word  {word} shape {tf.shape(word)} chars {chars}')\n",
    "#     chars_len = tf.shape(chars)[0]\n",
    "    chars_len = get_chars_len(chars)\n",
    "#     start = tf.zeros((1,),dtype=tf.int32)\n",
    "    start = tf.Variable([0],dtype=tf.int32)\n",
    "    end = tf.Variable([chars_len])\n",
    "\n",
    "    missing_index=-1\n",
    "    \n",
    "    is_bad = tf.Variable(False)\n",
    "    sub_tokens = tf.Variable([],shape=(None,),dtype=tf.string)\n",
    "    \n",
    "    (start,end,word,chars,is_bad,sub_tokens_1) =  tf.while_loop(outer_cond,outer_while_loop,loop_vars=[start,end,word,chars,is_bad,sub_tokens])\n",
    "    sub_tokens.assign(sub_tokens_1)\n",
    "#     sub_tokens_out = tf.expand_dims(sub_tokens,axis=0)\n",
    "    return sub_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "per_word_tokenize word  b'aagent' shape [] chars [b'a' b'a' b'g' b'e' b'n' b't']\n",
      "outer_cond start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([6], dtype=int32)>  chars [b'a' b'a' b'g' b'e' b'n' b't'] is_bad <tf.Variable 'Variable:0' shape=() dtype=bool, numpy=False> sub_tokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([], dtype=object)>\n",
      "outer_while_loop start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([6], dtype=int32)>  chars [b'a' b'a' b'g' b'e' b'n' b't'] is_bad <tf.Variable 'Variable:0' shape=() dtype=bool, numpy=False> sub_tokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([], dtype=object)>\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'vocab_lookup' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-60b7f9418f57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msub_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mper_word_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'aagent'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-21-9789ba30d1a8>\u001b[0m in \u001b[0;36mper_word_tokenize\u001b[0;34m(word)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0msub_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mchars\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mis_bad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msub_tokens_1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhile_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouter_cond\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mouter_while_loop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mchars\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mis_bad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msub_tokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0msub_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_tokens_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m#     sub_tokens_out = tf.expand_dims(sub_tokens,axis=0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m                   \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'in a future version'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m                   if date is None else ('after %s' % date), instructions)\n\u001b[0;32m--> 574\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     doc = _add_deprecated_arg_value_notice_to_docstring(\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop_v2\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, maximum_iterations, name)\u001b[0m\n\u001b[1;32m   2489\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2490\u001b[0m       \u001b[0mmaximum_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaximum_iterations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2491\u001b[0;31m       return_same_structure=True)\n\u001b[0m\u001b[1;32m   2492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\u001b[0m\n\u001b[1;32m   2725\u001b[0m                                               list(loop_vars))\n\u001b[1;32m   2726\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0mcond\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2727\u001b[0;31m         \u001b[0mloop_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2728\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtry_to_pack\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_basetuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2729\u001b[0m           \u001b[0mpacked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-415fb790989d>\u001b[0m in \u001b[0;36mouter_while_loop\u001b[0;34m(start, end, word, chars, is_bad, sub_tokens)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#     vocab_lookup.assign(vocab_table.lookup(tf.constant(substring)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0mstart_1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mend_1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msubstring_1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mchars_1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocab_lookup\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhile_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minner_cond\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minner_while_body\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msubstring\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mchars\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocab_lookup\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'****end of inner while start {start_1} end {end_1} substring {substring_1} chars {chars_1} vocab_lookup {vocab_lookup} \\n\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_vocab_missing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_lookup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'vocab_lookup' referenced before assignment"
     ]
    }
   ],
   "source": [
    "sub_tokens = per_word_tokenize('aagent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([b'aa', b'##gent'], dtype=object)>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'aa ##gent'>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.reduce_join(sub_tokens,separator=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joined_per_word_tokenized(word):\n",
    "    sub_tokens = per_word_tokenize(word)\n",
    "    return tf.strings.reduce_join(sub_tokens,separator=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "per_word_tokenize word  b'aagent' shape [] chars [b'a' b'a' b'g' b'e' b'n' b't']\n",
      "outer_cond start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([6], dtype=int32)>  chars [b'a' b'a' b'g' b'e' b'n' b't'] is_bad <tf.Variable 'Variable:0' shape=() dtype=bool, numpy=False> sub_tokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([], dtype=object)>\n",
      "outer_while_loop start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([6], dtype=int32)>  chars [b'a' b'a' b'g' b'e' b'n' b't'] is_bad <tf.Variable 'Variable:0' shape=() dtype=bool, numpy=False> sub_tokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([], dtype=object)>\n",
      "vocab lookup inside inner_cond  start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([6], dtype=int32)> condition <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=-1> is_vocab_missing True & entire cond True\n",
      "inner_while_body start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([6], dtype=int32)> word b'aagent' chars [b'a' b'a' b'g' b'e' b'n' b't']\n",
      "\n",
      "substring b'aagent' vocab lookup <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=-1>\n",
      "missing b'aagent'\n",
      "vocab lookup inside inner_cond  start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([5], dtype=int32)> condition <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=-1> is_vocab_missing True & entire cond True\n",
      "inner_while_body start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([5], dtype=int32)> word b'aagent' chars [b'a' b'a' b'g' b'e' b'n' b't']\n",
      "\n",
      "substring b'aagen' vocab lookup <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=-1>\n",
      "missing b'aagen'\n",
      "vocab lookup inside inner_cond  start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([4], dtype=int32)> condition <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=-1> is_vocab_missing True & entire cond True\n",
      "inner_while_body start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([4], dtype=int32)> word b'aagen' chars [b'a' b'a' b'g' b'e' b'n' b't']\n",
      "\n",
      "substring b'aage' vocab lookup <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=-1>\n",
      "missing b'aage'\n",
      "vocab lookup inside inner_cond  start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([3], dtype=int32)> condition <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=-1> is_vocab_missing True & entire cond True\n",
      "inner_while_body start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([3], dtype=int32)> word b'aage' chars [b'a' b'a' b'g' b'e' b'n' b't']\n",
      "\n",
      "substring b'aag' vocab lookup <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=-1>\n",
      "missing b'aag'\n",
      "vocab lookup inside inner_cond  start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)> condition <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=-1> is_vocab_missing True & entire cond True\n",
      "inner_while_body start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)> word b'aag' chars [b'a' b'a' b'g' b'e' b'n' b't']\n",
      "\n",
      "substring b'aa' vocab lookup <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=28335>\n",
      "present b'aa'\n",
      "vocab lookup inside inner_cond  start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)> condition <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=28335> is_vocab_missing False & entire cond False\n",
      "****end of inner while start [0] end [2] substring b'aa' chars [b'a' b'a' b'g' b'e' b'n' b't'] vocab_lookup 28335 \n",
      "\n",
      "\n",
      "sub tokens concat  [\"aa\"] shape  [1]\n",
      "subtokens  []  sub tokens shape  []\n",
      "subtokens appended \n",
      "start before assigning end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> & end val <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)>\n",
      "current subtokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([b'aa'], dtype=object)>\n",
      "outer_cond start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)>  chars [b'a' b'a' b'g' b'e' b'n' b't'] is_bad <tf.Variable 'Variable:0' shape=() dtype=bool, numpy=False> sub_tokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([b'aa'], dtype=object)>\n",
      "outer_while_loop start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)>  chars [b'a' b'a' b'g' b'e' b'n' b't'] is_bad <tf.Variable 'Variable:0' shape=() dtype=bool, numpy=False> sub_tokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([b'aa'], dtype=object)>\n",
      "vocab lookup inside inner_cond  start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([6], dtype=int32)> condition <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=-1> is_vocab_missing True & entire cond True\n",
      "inner_while_body start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([6], dtype=int32)> word b'gent' chars [b'a' b'a' b'g' b'e' b'n' b't']\n",
      "\n",
      "substring b'##gent' vocab lookup <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=22500>\n",
      "present b'##gent'\n",
      "vocab lookup inside inner_cond  start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([6], dtype=int32)> condition <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=22500> is_vocab_missing False & entire cond False\n",
      "****end of inner while start [2] end [6] substring b'##gent' chars [b'a' b'a' b'g' b'e' b'n' b't'] vocab_lookup 22500 \n",
      "\n",
      "\n",
      "sub tokens concat  [\"aa\" \"##gent\"] shape  [2]\n",
      "subtokens  [\"aa\"]  sub tokens shape  [\"aa\"]\n",
      "subtokens appended \n",
      "start before assigning end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)> & end val <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([6], dtype=int32)>\n",
      "current subtokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([b'aa', b'##gent'], dtype=object)>\n",
      "outer_cond start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([6], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([6], dtype=int32)>  chars [b'a' b'a' b'g' b'e' b'n' b't'] is_bad <tf.Variable 'Variable:0' shape=() dtype=bool, numpy=False> sub_tokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([b'aa', b'##gent'], dtype=object)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([b'aa', b'##gent'], dtype=object)>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "per_word_tokenize('aagent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'g', b'e', b'n', b't'], [b'a', b'a', b'g', b'e', b'n', b't']]>"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = tf.constant('agent aagent')\n",
    "slist = tf.strings.split(s1)\n",
    "slist2 = tf.strings.bytes_split(slist)\n",
    "slist2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=string, numpy=array([b'talk to an agent', b' talk to an aagent'], dtype=object)>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.split('talk to an agent, talk to an aagent',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentence(sentence,max_sequence_length=10):\n",
    "    words = tf.strings.split(sentence)\n",
    "    tf.print(f'words in process sentence {words}')\n",
    "#     list_words = tf.expand_dims(words,axis=1)\n",
    "#     tf.print(f'words in process sentence {list_words} shape {tf.shape(list_words)}')\n",
    "    subword_tokenized_list = tf.map_fn(joined_per_word_tokenized,words,parallel_iterations=None,infer_shape=False)\n",
    "    sentence_tokenized = tf.strings.reduce_join(subword_tokenized_list,separator=' ')\n",
    "    processed_tokens = tf.strings.split(sentence_tokenized)\n",
    "    tf.print(f'processed tokens {processed_tokens}')\n",
    "    sequence_length = tf.shape(processed_tokens)[-1]\n",
    "    trimmed_max_length = max_sequence_length-2\n",
    "    values_trimmed = tf.cond(tf.greater(sequence_length,trimmed_max_length), \n",
    "                                    lambda : tf.slice(processed_tokens,begin=[0],size=[trimmed_max_length],name='trimmed_out'),lambda : processed_tokens)\n",
    "    concat = tf.concat([['[CLS]'],values_trimmed,['[SEP]']],axis=0,name='concat_out')\n",
    "    actual_token_length = tf.shape(concat)[-1]\n",
    "    token_ids = vocab_table.lookup(concat)\n",
    "    padded = tf.pad(token_ids,paddings=[[0,max_sequence_length-actual_token_length]],name='input_ids')\n",
    "    return padded\n",
    "#     return sentence_tokenized\n",
    "#     return subword_tokenized_list\n",
    "#     return processed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words in process sentence [b'talk' b'to' b'an' b'aagent']\n",
      "per_word_tokenize word  b'talk' shape [] chars [b't' b'a' b'l' b'k']\n",
      "outer_cond start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([4], dtype=int32)>  chars [b't' b'a' b'l' b'k'] is_bad <tf.Variable 'Variable:0' shape=() dtype=bool, numpy=False> sub_tokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([], dtype=object)>\n",
      "outer_while_loop start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([4], dtype=int32)>  chars [b't' b'a' b'l' b'k'] is_bad <tf.Variable 'Variable:0' shape=() dtype=bool, numpy=False> sub_tokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([], dtype=object)>\n",
      "vocab lookup inside inner_cond  start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([4], dtype=int32)> condition <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=-1> is_vocab_missing True & entire cond True\n",
      "inner_while_body start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([4], dtype=int32)> word b'talk' chars [b't' b'a' b'l' b'k']\n",
      "\n",
      "substring b'talk' vocab lookup <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=31311>\n",
      "present b'talk'\n",
      "vocab lookup inside inner_cond  start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([4], dtype=int32)> condition <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=31311> is_vocab_missing False & entire cond False\n",
      "****end of inner while start [0] end [4] substring b'talk' chars [b't' b'a' b'l' b'k'] vocab_lookup 31311 \n",
      "\n",
      "\n",
      "sub tokens concat  [\"talk\"] shape  [1]\n",
      "subtokens  []  sub tokens shape  []\n",
      "subtokens appended \n",
      "start before assigning end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> & end val <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([4], dtype=int32)>\n",
      "current subtokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([b'talk'], dtype=object)>\n",
      "outer_cond start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([4], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([4], dtype=int32)>  chars [b't' b'a' b'l' b'k'] is_bad <tf.Variable 'Variable:0' shape=() dtype=bool, numpy=False> sub_tokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([b'talk'], dtype=object)>\n",
      "per_word_tokenize word  b'to' shape [] chars [b't' b'o']\n",
      "outer_cond start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)>  chars [b't' b'o'] is_bad <tf.Variable 'Variable:0' shape=() dtype=bool, numpy=False> sub_tokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([], dtype=object)>\n",
      "outer_while_loop start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)>  chars [b't' b'o'] is_bad <tf.Variable 'Variable:0' shape=() dtype=bool, numpy=False> sub_tokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([], dtype=object)>\n",
      "vocab lookup inside inner_cond  start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)> condition <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=-1> is_vocab_missing True & entire cond True\n",
      "inner_while_body start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)> word b'to' chars [b't' b'o']\n",
      "\n",
      "substring b'to' vocab lookup <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=10114>\n",
      "present b'to'\n",
      "vocab lookup inside inner_cond  start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)> condition <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=10114> is_vocab_missing False & entire cond False\n",
      "****end of inner while start [0] end [2] substring b'to' chars [b't' b'o'] vocab_lookup 10114 \n",
      "\n",
      "\n",
      "sub tokens concat  [\"to\"] shape  [1]\n",
      "subtokens  []  sub tokens shape  []\n",
      "subtokens appended \n",
      "start before assigning end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> & end val <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)>\n",
      "current subtokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([b'to'], dtype=object)>\n",
      "outer_cond start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)>  chars [b't' b'o'] is_bad <tf.Variable 'Variable:0' shape=() dtype=bool, numpy=False> sub_tokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([b'to'], dtype=object)>\n",
      "per_word_tokenize word  b'an' shape [] chars [b'a' b'n']\n",
      "outer_cond start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)>  chars [b'a' b'n'] is_bad <tf.Variable 'Variable:0' shape=() dtype=bool, numpy=False> sub_tokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([], dtype=object)>\n",
      "outer_while_loop start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)>  chars [b'a' b'n'] is_bad <tf.Variable 'Variable:0' shape=() dtype=bool, numpy=False> sub_tokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([], dtype=object)>\n",
      "vocab lookup inside inner_cond  start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)> condition <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=-1> is_vocab_missing True & entire cond True\n",
      "inner_while_body start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)> word b'an' chars [b'a' b'n']\n",
      "\n",
      "substring b'an' vocab lookup <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=10151>\n",
      "present b'an'\n",
      "vocab lookup inside inner_cond  start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)> condition <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=10151> is_vocab_missing False & entire cond False\n",
      "****end of inner while start [0] end [2] substring b'an' chars [b'a' b'n'] vocab_lookup 10151 \n",
      "\n",
      "\n",
      "sub tokens concat  [\"an\"] shape  [1]\n",
      "subtokens  []  sub tokens shape  []\n",
      "subtokens appended \n",
      "start before assigning end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> & end val <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)>\n",
      "current subtokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([b'an'], dtype=object)>\n",
      "outer_cond start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)>  chars [b'a' b'n'] is_bad <tf.Variable 'Variable:0' shape=() dtype=bool, numpy=False> sub_tokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([b'an'], dtype=object)>\n",
      "per_word_tokenize word  b'aagent' shape [] chars [b'a' b'a' b'g' b'e' b'n' b't']\n",
      "outer_cond start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([6], dtype=int32)>  chars [b'a' b'a' b'g' b'e' b'n' b't'] is_bad <tf.Variable 'Variable:0' shape=() dtype=bool, numpy=False> sub_tokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([], dtype=object)>\n",
      "outer_while_loop start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([6], dtype=int32)>  chars [b'a' b'a' b'g' b'e' b'n' b't'] is_bad <tf.Variable 'Variable:0' shape=() dtype=bool, numpy=False> sub_tokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([], dtype=object)>\n",
      "vocab lookup inside inner_cond  start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([6], dtype=int32)> condition <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=-1> is_vocab_missing True & entire cond True\n",
      "inner_while_body start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([6], dtype=int32)> word b'aagent' chars [b'a' b'a' b'g' b'e' b'n' b't']\n",
      "\n",
      "substring b'aagent' vocab lookup <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=-1>\n",
      "missing b'aagent'\n",
      "vocab lookup inside inner_cond  start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([5], dtype=int32)> condition <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=-1> is_vocab_missing True & entire cond True\n",
      "inner_while_body start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([5], dtype=int32)> word b'aagent' chars [b'a' b'a' b'g' b'e' b'n' b't']\n",
      "\n",
      "substring b'aagen' vocab lookup <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=-1>\n",
      "missing b'aagen'\n",
      "vocab lookup inside inner_cond  start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([4], dtype=int32)> condition <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=-1> is_vocab_missing True & entire cond True\n",
      "inner_while_body start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([4], dtype=int32)> word b'aagen' chars [b'a' b'a' b'g' b'e' b'n' b't']\n",
      "\n",
      "substring b'aage' vocab lookup <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=-1>\n",
      "missing b'aage'\n",
      "vocab lookup inside inner_cond  start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([3], dtype=int32)> condition <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=-1> is_vocab_missing True & entire cond True\n",
      "inner_while_body start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([3], dtype=int32)> word b'aage' chars [b'a' b'a' b'g' b'e' b'n' b't']\n",
      "\n",
      "substring b'aag' vocab lookup <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=-1>\n",
      "missing b'aag'\n",
      "vocab lookup inside inner_cond  start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)> condition <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=-1> is_vocab_missing True & entire cond True\n",
      "inner_while_body start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)> word b'aag' chars [b'a' b'a' b'g' b'e' b'n' b't']\n",
      "\n",
      "substring b'aa' vocab lookup <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=28335>\n",
      "present b'aa'\n",
      "vocab lookup inside inner_cond  start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)> condition <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=28335> is_vocab_missing False & entire cond False\n",
      "****end of inner while start [0] end [2] substring b'aa' chars [b'a' b'a' b'g' b'e' b'n' b't'] vocab_lookup 28335 \n",
      "\n",
      "\n",
      "sub tokens concat  [\"aa\"] shape  [1]\n",
      "subtokens  []  sub tokens shape  []\n",
      "subtokens appended \n",
      "start before assigning end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> & end val <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)>\n",
      "current subtokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([b'aa'], dtype=object)>\n",
      "outer_cond start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)>  chars [b'a' b'a' b'g' b'e' b'n' b't'] is_bad <tf.Variable 'Variable:0' shape=() dtype=bool, numpy=False> sub_tokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([b'aa'], dtype=object)>\n",
      "outer_while_loop start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)>  chars [b'a' b'a' b'g' b'e' b'n' b't'] is_bad <tf.Variable 'Variable:0' shape=() dtype=bool, numpy=False> sub_tokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([b'aa'], dtype=object)>\n",
      "vocab lookup inside inner_cond  start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([6], dtype=int32)> condition <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=-1> is_vocab_missing True & entire cond True\n",
      "inner_while_body start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([6], dtype=int32)> word b'gent' chars [b'a' b'a' b'g' b'e' b'n' b't']\n",
      "\n",
      "substring b'##gent' vocab lookup <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=22500>\n",
      "present b'##gent'\n",
      "vocab lookup inside inner_cond  start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([6], dtype=int32)> condition <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=22500> is_vocab_missing False & entire cond False\n",
      "****end of inner while start [2] end [6] substring b'##gent' chars [b'a' b'a' b'g' b'e' b'n' b't'] vocab_lookup 22500 \n",
      "\n",
      "\n",
      "sub tokens concat  [\"aa\" \"##gent\"] shape  [2]\n",
      "subtokens  [\"aa\"]  sub tokens shape  [\"aa\"]\n",
      "subtokens appended \n",
      "start before assigning end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)> & end val <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([6], dtype=int32)>\n",
      "current subtokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([b'aa', b'##gent'], dtype=object)>\n",
      "outer_cond start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([6], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([6], dtype=int32)>  chars [b'a' b'a' b'g' b'e' b'n' b't'] is_bad <tf.Variable 'Variable:0' shape=() dtype=bool, numpy=False> sub_tokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([b'aa', b'##gent'], dtype=object)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=int64, numpy=\n",
       "array([  101, 31311, 10114, 10151, 28335, 22500,   102,     0,     0,\n",
       "           0])>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_sentence('talk to an aagent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DType' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-454a86dbc544>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DType' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "tf.strings.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentences(sentences):\n",
    "    sentences_tokenized = tf.map_fn(process_sentence,sentences,dtype=tf.int64)\n",
    "    return sentences_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([b'talk to an agent' b' talk to an aagent'], shape=(2,), dtype=string)\n",
      "words in process sentence [b'talk' b'to' b'an' b'agent']\n",
      "per_word_tokenize word  b'talk' shape [] chars [b't' b'a' b'l' b'k']\n",
      "outer_cond start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([4], dtype=int32)>  chars [b't' b'a' b'l' b'k'] is_bad <tf.Variable 'Variable:0' shape=() dtype=bool, numpy=False> sub_tokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([], dtype=object)>\n",
      "outer_while_loop start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([4], dtype=int32)>  chars [b't' b'a' b'l' b'k'] is_bad <tf.Variable 'Variable:0' shape=() dtype=bool, numpy=False> sub_tokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([], dtype=object)>\n",
      "vocab lookup inside inner_cond  start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([4], dtype=int32)> condition <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=-1> is_vocab_missing True & entire cond True\n",
      "inner_while_body start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([4], dtype=int32)> word b'talk' chars [b't' b'a' b'l' b'k']\n",
      "\n",
      "substring b'talk' vocab lookup <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=31311>\n",
      "present b'talk'\n",
      "vocab lookup inside inner_cond  start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([4], dtype=int32)> condition <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=31311> is_vocab_missing False & entire cond False\n",
      "****end of inner while start [0] end [4] substring b'talk' chars [b't' b'a' b'l' b'k'] vocab_lookup 31311 \n",
      "\n",
      "\n",
      "sub tokens concat  [\"talk\"] shape  [1]\n",
      "subtokens  []  sub tokens shape  []\n",
      "subtokens appended \n",
      "start before assigning end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> & end val <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([4], dtype=int32)>\n",
      "current subtokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([b'talk'], dtype=object)>\n",
      "outer_cond start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([4], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([4], dtype=int32)>  chars [b't' b'a' b'l' b'k'] is_bad <tf.Variable 'Variable:0' shape=() dtype=bool, numpy=False> sub_tokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([b'talk'], dtype=object)>\n",
      "per_word_tokenize word  b'to' shape [] chars [b't' b'o']\n",
      "outer_cond start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)>  chars [b't' b'o'] is_bad <tf.Variable 'Variable:0' shape=() dtype=bool, numpy=False> sub_tokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([], dtype=object)>\n",
      "outer_while_loop start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)>  chars [b't' b'o'] is_bad <tf.Variable 'Variable:0' shape=() dtype=bool, numpy=False> sub_tokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([], dtype=object)>\n",
      "vocab lookup inside inner_cond  start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)> condition <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=-1> is_vocab_missing True & entire cond True\n",
      "inner_while_body start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)> word b'to' chars [b't' b'o']\n",
      "\n",
      "substring b'to' vocab lookup <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=10114>\n",
      "present b'to'\n",
      "vocab lookup inside inner_cond  start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)> condition <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=10114> is_vocab_missing False & entire cond False\n",
      "****end of inner while start [0] end [2] substring b'to' chars [b't' b'o'] vocab_lookup 10114 \n",
      "\n",
      "\n",
      "sub tokens concat  [\"to\"] shape  [1]\n",
      "subtokens  []  sub tokens shape  []\n",
      "subtokens appended \n",
      "start before assigning end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> & end val <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)>\n",
      "current subtokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([b'to'], dtype=object)>\n",
      "outer_cond start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)>  chars [b't' b'o'] is_bad <tf.Variable 'Variable:0' shape=() dtype=bool, numpy=False> sub_tokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([b'to'], dtype=object)>\n",
      "per_word_tokenize word  b'an' shape [] chars [b'a' b'n']\n",
      "outer_cond start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)>  chars [b'a' b'n'] is_bad <tf.Variable 'Variable:0' shape=() dtype=bool, numpy=False> sub_tokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([], dtype=object)>\n",
      "outer_while_loop start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)>  chars [b'a' b'n'] is_bad <tf.Variable 'Variable:0' shape=() dtype=bool, numpy=False> sub_tokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([], dtype=object)>\n",
      "vocab lookup inside inner_cond  start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)> condition <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=-1> is_vocab_missing True & entire cond True\n",
      "inner_while_body start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)> word b'an' chars [b'a' b'n']\n",
      "\n",
      "substring b'an' vocab lookup <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=10151>\n",
      "present b'an'\n",
      "vocab lookup inside inner_cond  start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)> condition <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=10151> is_vocab_missing False & entire cond False\n",
      "****end of inner while start [0] end [2] substring b'an' chars [b'a' b'n'] vocab_lookup 10151 \n",
      "\n",
      "\n",
      "sub tokens concat  [\"an\"] shape  [1]\n",
      "subtokens  []  sub tokens shape  []\n",
      "subtokens appended \n",
      "start before assigning end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> & end val <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)>\n",
      "current subtokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([b'an'], dtype=object)>\n",
      "outer_cond start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)>  chars [b'a' b'n'] is_bad <tf.Variable 'Variable:0' shape=() dtype=bool, numpy=False> sub_tokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([b'an'], dtype=object)>\n",
      "per_word_tokenize word  b'agent' shape [] chars [b'a' b'g' b'e' b'n' b't']\n",
      "outer_cond start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([5], dtype=int32)>  chars [b'a' b'g' b'e' b'n' b't'] is_bad <tf.Variable 'Variable:0' shape=() dtype=bool, numpy=False> sub_tokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([], dtype=object)>\n",
      "outer_while_loop start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([5], dtype=int32)>  chars [b'a' b'g' b'e' b'n' b't'] is_bad <tf.Variable 'Variable:0' shape=() dtype=bool, numpy=False> sub_tokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([], dtype=object)>\n",
      "vocab lookup inside inner_cond  start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([5], dtype=int32)> condition <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=-1> is_vocab_missing True & entire cond True\n",
      "inner_while_body start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([5], dtype=int32)> word b'agent' chars [b'a' b'g' b'e' b'n' b't']\n",
      "\n",
      "substring b'agent' vocab lookup <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=18980>\n",
      "present b'agent'\n",
      "vocab lookup inside inner_cond  start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([5], dtype=int32)> condition <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=18980> is_vocab_missing False & entire cond False\n",
      "****end of inner while start [0] end [5] substring b'agent' chars [b'a' b'g' b'e' b'n' b't'] vocab_lookup 18980 \n",
      "\n",
      "\n",
      "sub tokens concat  [\"agent\"] shape  [1]\n",
      "subtokens  []  sub tokens shape  []\n",
      "subtokens appended \n",
      "start before assigning end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> & end val <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([5], dtype=int32)>\n",
      "current subtokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([b'agent'], dtype=object)>\n",
      "outer_cond start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([5], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([5], dtype=int32)>  chars [b'a' b'g' b'e' b'n' b't'] is_bad <tf.Variable 'Variable:0' shape=() dtype=bool, numpy=False> sub_tokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([b'agent'], dtype=object)>\n",
      "processed tokens [b'talk' b'to' b'an' b'agent']\n",
      "words in process sentence [b'talk' b'to' b'an' b'aagent']\n",
      "per_word_tokenize word  b'talk' shape [] chars [b't' b'a' b'l' b'k']\n",
      "outer_cond start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([4], dtype=int32)>  chars [b't' b'a' b'l' b'k'] is_bad <tf.Variable 'Variable:0' shape=() dtype=bool, numpy=False> sub_tokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([], dtype=object)>\n",
      "outer_while_loop start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([4], dtype=int32)>  chars [b't' b'a' b'l' b'k'] is_bad <tf.Variable 'Variable:0' shape=() dtype=bool, numpy=False> sub_tokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([], dtype=object)>\n",
      "vocab lookup inside inner_cond  start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([4], dtype=int32)> condition <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=-1> is_vocab_missing True & entire cond True\n",
      "inner_while_body start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([4], dtype=int32)> word b'talk' chars [b't' b'a' b'l' b'k']\n",
      "\n",
      "substring b'talk' vocab lookup <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=31311>\n",
      "present b'talk'\n",
      "vocab lookup inside inner_cond  start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([4], dtype=int32)> condition <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=31311> is_vocab_missing False & entire cond False\n",
      "****end of inner while start [0] end [4] substring b'talk' chars [b't' b'a' b'l' b'k'] vocab_lookup 31311 \n",
      "\n",
      "\n",
      "sub tokens concat  [\"talk\"] shape  [1]\n",
      "subtokens  []  sub tokens shape  []\n",
      "subtokens appended \n",
      "start before assigning end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> & end val <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([4], dtype=int32)>\n",
      "current subtokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([b'talk'], dtype=object)>\n",
      "outer_cond start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([4], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([4], dtype=int32)>  chars [b't' b'a' b'l' b'k'] is_bad <tf.Variable 'Variable:0' shape=() dtype=bool, numpy=False> sub_tokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([b'talk'], dtype=object)>\n",
      "per_word_tokenize word  b'to' shape [] chars [b't' b'o']\n",
      "outer_cond start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)>  chars [b't' b'o'] is_bad <tf.Variable 'Variable:0' shape=() dtype=bool, numpy=False> sub_tokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([], dtype=object)>\n",
      "outer_while_loop start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)>  chars [b't' b'o'] is_bad <tf.Variable 'Variable:0' shape=() dtype=bool, numpy=False> sub_tokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([], dtype=object)>\n",
      "vocab lookup inside inner_cond  start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)> condition <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=-1> is_vocab_missing True & entire cond True\n",
      "inner_while_body start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)> word b'to' chars [b't' b'o']\n",
      "\n",
      "substring b'to' vocab lookup <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=10114>\n",
      "present b'to'\n",
      "vocab lookup inside inner_cond  start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)> condition <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=10114> is_vocab_missing False & entire cond False\n",
      "****end of inner while start [0] end [2] substring b'to' chars [b't' b'o'] vocab_lookup 10114 \n",
      "\n",
      "\n",
      "sub tokens concat  [\"to\"] shape  [1]\n",
      "subtokens  []  sub tokens shape  []\n",
      "subtokens appended \n",
      "start before assigning end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> & end val <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)>\n",
      "current subtokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([b'to'], dtype=object)>\n",
      "outer_cond start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)>  chars [b't' b'o'] is_bad <tf.Variable 'Variable:0' shape=() dtype=bool, numpy=False> sub_tokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([b'to'], dtype=object)>\n",
      "per_word_tokenize word  b'an' shape [] chars [b'a' b'n']\n",
      "outer_cond start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)>  chars [b'a' b'n'] is_bad <tf.Variable 'Variable:0' shape=() dtype=bool, numpy=False> sub_tokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([], dtype=object)>\n",
      "outer_while_loop start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)>  chars [b'a' b'n'] is_bad <tf.Variable 'Variable:0' shape=() dtype=bool, numpy=False> sub_tokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([], dtype=object)>\n",
      "vocab lookup inside inner_cond  start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)> condition <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=-1> is_vocab_missing True & entire cond True\n",
      "inner_while_body start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)> word b'an' chars [b'a' b'n']\n",
      "\n",
      "substring b'an' vocab lookup <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=10151>\n",
      "present b'an'\n",
      "vocab lookup inside inner_cond  start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)> condition <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=10151> is_vocab_missing False & entire cond False\n",
      "****end of inner while start [0] end [2] substring b'an' chars [b'a' b'n'] vocab_lookup 10151 \n",
      "\n",
      "\n",
      "sub tokens concat  [\"an\"] shape  [1]\n",
      "subtokens  []  sub tokens shape  []\n",
      "subtokens appended \n",
      "start before assigning end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> & end val <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)>\n",
      "current subtokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([b'an'], dtype=object)>\n",
      "outer_cond start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)>  chars [b'a' b'n'] is_bad <tf.Variable 'Variable:0' shape=() dtype=bool, numpy=False> sub_tokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([b'an'], dtype=object)>\n",
      "per_word_tokenize word  b'aagent' shape [] chars [b'a' b'a' b'g' b'e' b'n' b't']\n",
      "outer_cond start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([6], dtype=int32)>  chars [b'a' b'a' b'g' b'e' b'n' b't'] is_bad <tf.Variable 'Variable:0' shape=() dtype=bool, numpy=False> sub_tokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([], dtype=object)>\n",
      "outer_while_loop start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([6], dtype=int32)>  chars [b'a' b'a' b'g' b'e' b'n' b't'] is_bad <tf.Variable 'Variable:0' shape=() dtype=bool, numpy=False> sub_tokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([], dtype=object)>\n",
      "vocab lookup inside inner_cond  start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([6], dtype=int32)> condition <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=-1> is_vocab_missing True & entire cond True\n",
      "inner_while_body start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([6], dtype=int32)> word b'aagent' chars [b'a' b'a' b'g' b'e' b'n' b't']\n",
      "\n",
      "substring b'aagent' vocab lookup <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=-1>\n",
      "missing b'aagent'\n",
      "vocab lookup inside inner_cond  start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([5], dtype=int32)> condition <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=-1> is_vocab_missing True & entire cond True\n",
      "inner_while_body start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([5], dtype=int32)> word b'aagent' chars [b'a' b'a' b'g' b'e' b'n' b't']\n",
      "\n",
      "substring b'aagen' vocab lookup <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=-1>\n",
      "missing b'aagen'\n",
      "vocab lookup inside inner_cond  start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([4], dtype=int32)> condition <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=-1> is_vocab_missing True & entire cond True\n",
      "inner_while_body start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([4], dtype=int32)> word b'aagen' chars [b'a' b'a' b'g' b'e' b'n' b't']\n",
      "\n",
      "substring b'aage' vocab lookup <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=-1>\n",
      "missing b'aage'\n",
      "vocab lookup inside inner_cond  start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([3], dtype=int32)> condition <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=-1> is_vocab_missing True & entire cond True\n",
      "inner_while_body start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([3], dtype=int32)> word b'aage' chars [b'a' b'a' b'g' b'e' b'n' b't']\n",
      "\n",
      "substring b'aag' vocab lookup <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=-1>\n",
      "missing b'aag'\n",
      "vocab lookup inside inner_cond  start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)> condition <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=-1> is_vocab_missing True & entire cond True\n",
      "inner_while_body start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)> word b'aag' chars [b'a' b'a' b'g' b'e' b'n' b't']\n",
      "\n",
      "substring b'aa' vocab lookup <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=28335>\n",
      "present b'aa'\n",
      "vocab lookup inside inner_cond  start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)> condition <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=28335> is_vocab_missing False & entire cond False\n",
      "****end of inner while start [0] end [2] substring b'aa' chars [b'a' b'a' b'g' b'e' b'n' b't'] vocab_lookup 28335 \n",
      "\n",
      "\n",
      "sub tokens concat  [\"aa\"] shape  [1]\n",
      "subtokens  []  sub tokens shape  []\n",
      "subtokens appended \n",
      "start before assigning end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> & end val <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)>\n",
      "current subtokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([b'aa'], dtype=object)>\n",
      "outer_cond start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)>  chars [b'a' b'a' b'g' b'e' b'n' b't'] is_bad <tf.Variable 'Variable:0' shape=() dtype=bool, numpy=False> sub_tokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([b'aa'], dtype=object)>\n",
      "outer_while_loop start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)>  chars [b'a' b'a' b'g' b'e' b'n' b't'] is_bad <tf.Variable 'Variable:0' shape=() dtype=bool, numpy=False> sub_tokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([b'aa'], dtype=object)>\n",
      "vocab lookup inside inner_cond  start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([6], dtype=int32)> condition <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=-1> is_vocab_missing True & entire cond True\n",
      "inner_while_body start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([6], dtype=int32)> word b'gent' chars [b'a' b'a' b'g' b'e' b'n' b't']\n",
      "\n",
      "substring b'##gent' vocab lookup <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=22500>\n",
      "present b'##gent'\n",
      "vocab lookup inside inner_cond  start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([6], dtype=int32)> condition <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=22500> is_vocab_missing False & entire cond False\n",
      "****end of inner while start [2] end [6] substring b'##gent' chars [b'a' b'a' b'g' b'e' b'n' b't'] vocab_lookup 22500 \n",
      "\n",
      "\n",
      "sub tokens concat  [\"aa\" \"##gent\"] shape  [2]\n",
      "subtokens  [\"aa\"]  sub tokens shape  [\"aa\"]\n",
      "subtokens appended \n",
      "start before assigning end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)> & end val <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([6], dtype=int32)>\n",
      "current subtokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([b'aa', b'##gent'], dtype=object)>\n",
      "outer_cond start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([6], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([6], dtype=int32)>  chars [b'a' b'a' b'g' b'e' b'n' b't'] is_bad <tf.Variable 'Variable:0' shape=() dtype=bool, numpy=False> sub_tokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([b'aa', b'##gent'], dtype=object)>\n",
      "processed tokens [b'talk' b'to' b'an' b'aa' b'##gent']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 10), dtype=int64, numpy=\n",
       "array([[  101, 31311, 10114, 10151, 18980,   102,     0,     0,     0,\n",
       "            0],\n",
       "       [  101, 31311, 10114, 10151, 28335, 22500,   102,     0,     0,\n",
       "            0]])>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_list = tf.strings.split('talk to an agent, talk to an aagent',sep=',')\n",
    "print(sentence_list)\n",
    "process_sentences(sentence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.layers.Layer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertTokenizationCustom(tf.keras.layers.Layer):\n",
    "    def __init__(self,vocab_path):\n",
    "        super(BertTokenizationCustom, self).__init__(trainable=False,dtype=tf.int32)\n",
    "        \n",
    "        self.missing_index=-1\n",
    "        self.vocab_path = vocab_path\n",
    "        self.vocab_table = tf.lookup.StaticHashTable(\n",
    "            tf.lookup.TextFileInitializer(vocab_file,\n",
    "                                          key_dtype=tf.string,key_index=tf.lookup.TextFileIndex.WHOLE_LINE,\n",
    "                                     value_dtype=tf.int64,value_index=tf.lookup.TextFileIndex.LINE_NUMBER),default_value=missing_index)\n",
    "    def build(self,input_shape):\n",
    "        #self.model_proto = tf.io.gfile.GFile(self.model_path, 'rb').read()\n",
    "        ## pad_id is usually 0 \n",
    "#         [self.CLS_ID,self.SEP_ID,self.PAD_ID]  = tfs.piece_to_id(['[CLS]','[SEP]','[PAD]'],model_file=self.model_path)\n",
    "        self.built=True\n",
    "        \n",
    "#     @tf.function    \n",
    "    def call(self, input_text):\n",
    "        ##tensorflow sentence piece works while exporting to graph while, tf_text sentencepiece doesn't\n",
    "        encoded_text = self.process_sentences(input_text)\n",
    "        return encoded_text\n",
    "    \n",
    "    def is_vocab_missing(self,vocab_lookup):\n",
    "        return vocab_lookup==self.missing_index\n",
    "    \n",
    "    def get_chars_len(self,chars):\n",
    "        chars_len = tf.shape(chars)[0]\n",
    "        # when it is ragged tensor\n",
    "    #     chars_len = chars.nrows()\n",
    "        return chars_len\n",
    "\n",
    "    def inner_cond(self,start,end,word,chars,vocab_lookup):\n",
    "        cond = tf.less(start,end) and is_vocab_missing(vocab_lookup)\n",
    "#         tf.print(f'vocab lookup inside inner_cond  start {start} end {end} condition {vocab_lookup} is_vocab_missing {is_vocab_missing(vocab_lookup)} & entire cond {cond}')\n",
    "        return cond\n",
    "    \n",
    "    # def inner_while_body(start,end,word,vocab_lookup):\n",
    "    def inner_while_body(self,start,end,word,chars,vocab_lookup):\n",
    "#         tf.print(f'inner_while_body start {start} end {end} word {word} chars {chars}\\n')\n",
    "    #     chars = tf.strings.bytes_split(word)\n",
    "        substring = tf.strings.join(tf.slice(chars,begin=start,size=end-start))\n",
    "    #     tf.strings.reduce_join()\n",
    "        if start>0:\n",
    "            substring = tf.strings.join([\"##\",substring])\n",
    "        # assigning it rather than creating a new variable so that this is accessible to other\n",
    "        # function referencing this variable\n",
    "        vocab_lookup.assign(self.vocab_table.lookup(tf.constant(substring)))\n",
    "#         tf.print(f'substring {substring} vocab lookup {vocab_lookup}')\n",
    "        if vocab_lookup==missing_index:\n",
    "    #         print(f'missing {substring}')\n",
    "#             tf.print(f'missing {substring}')\n",
    "            # subtracting only if missing\n",
    "            end.assign_sub([1])\n",
    "#         else:\n",
    "#             tf.print(f'present {substring}')\n",
    "\n",
    "        return (start,end,substring,chars,vocab_lookup)\n",
    "    \n",
    "    def outer_cond(self,start,end,word,chars,is_bad,sub_tokens):\n",
    "#         tf.print(f'outer_cond start {start} end {end}  chars {chars} is_bad {is_bad} sub_tokens {sub_tokens}')\n",
    "        return (start < tf.shape(chars)[0]) and not is_bad\n",
    "\n",
    "    def outer_while_loop(self,start,end,word,chars,is_bad,sub_tokens):\n",
    "#         tf.print(f'outer_while_loop start {start} end {end}  chars {chars} is_bad {is_bad} sub_tokens {sub_tokens}')\n",
    "        # vocab_lookup = vocab_table.lookup(tf.constant(substring))\n",
    "        chars_len = get_chars_len(chars)\n",
    "        end = tf.Variable([chars_len])\n",
    "        substring = tf.strings.join(tf.slice(chars,begin=start,size=end-start))\n",
    "        vocab_lookup = tf.Variable(missing_index,shape=(),name='vocab_lookup',dtype=tf.int64)\n",
    "    #     vocab_lookup.assign(vocab_table.lookup(tf.constant(substring)))\n",
    "\n",
    "        (start_1,end_1,substring_1,chars_1,vocab_lookup) = tf.while_loop(inner_cond,inner_while_body,loop_vars=[start,end,substring,chars,vocab_lookup])\n",
    "#         tf.print(f'****end of inner while start {start_1} end {end_1} substring {substring_1} chars {chars_1} vocab_lookup {vocab_lookup} \\n\\n')\n",
    "        if not is_vocab_missing(vocab_lookup):\n",
    "            sub_tokens_concat = tf.concat([sub_tokens,[substring_1]],axis=0)\n",
    "#             tf.print(\"sub tokens concat \",sub_tokens_concat, 'shape ',tf.shape(sub_tokens_concat))\n",
    "    #         sub_tokens.append(substring)\n",
    "#             tf.print('subtokens ',sub_tokens, ' sub tokens shape ',sub_tokens)\n",
    "            sub_tokens.assign(sub_tokens_concat)\n",
    "#             tf.print(f'subtokens appended ')\n",
    "        else:\n",
    "            # if vocab missing even after the inner loop then this word can't be formed using\n",
    "            # word existing in the dictionary\n",
    "            is_bad.assign(True)\n",
    "#         tf.print(f'start before assigning end {start} & end val {end}')\n",
    "        start.assign(end_1)\n",
    "#         tf.print(f'current subtokens {sub_tokens}')\n",
    "        return (start,end,word,chars,is_bad,sub_tokens)\n",
    "    \n",
    "    def per_word_tokenize(self,word):\n",
    "        word = tf.squeeze(word)\n",
    "        chars = tf.strings.bytes_split(word)#.to_tensor()\n",
    "#         tf.print(f'per_word_tokenize word  {word} shape {tf.shape(word)} chars {chars}')\n",
    "    #     chars_len = tf.shape(chars)[0]\n",
    "        chars_len = get_chars_len(chars)\n",
    "    #     start = tf.zeros((1,),dtype=tf.int32)\n",
    "        start = tf.Variable([0],dtype=tf.int32)\n",
    "        end = tf.Variable([chars_len])\n",
    "\n",
    "        missing_index=-1\n",
    "\n",
    "        is_bad = tf.Variable(False)\n",
    "        sub_tokens = tf.Variable([],shape=(None,),dtype=tf.string)\n",
    "\n",
    "        (start,end,word,chars,is_bad,sub_tokens_1) =  tf.while_loop(outer_cond,outer_while_loop,loop_vars=[start,end,word,chars,is_bad,sub_tokens])\n",
    "        sub_tokens.assign(sub_tokens_1)\n",
    "    #     sub_tokens_out = tf.expand_dims(sub_tokens,axis=0)\n",
    "        return sub_tokens\n",
    "    def joined_per_word_tokenized(self,word):\n",
    "        sub_tokens = per_word_tokenize(word)\n",
    "        return tf.strings.reduce_join(sub_tokens,separator=' ')\n",
    "    \n",
    "    def process_sentence(self,sentence,max_sequence_length=10):\n",
    "        words = tf.strings.split(sentence)\n",
    "#         tf.print(f'words in process sentence {words}')\n",
    "    #     list_words = tf.expand_dims(words,axis=1)\n",
    "    #     tf.print(f'words in process sentence {list_words} shape {tf.shape(list_words)}')\n",
    "        subword_tokenized_list = tf.map_fn(joined_per_word_tokenized,words,parallel_iterations=None,infer_shape=False)\n",
    "        sentence_tokenized = tf.strings.reduce_join(subword_tokenized_list,separator=' ')\n",
    "        processed_tokens = tf.strings.split(sentence_tokenized)\n",
    "#         tf.print(f'processed tokens {processed_tokens}')\n",
    "        sequence_length = tf.shape(processed_tokens)[-1]\n",
    "        trimmed_max_length = max_sequence_length-2\n",
    "        values_trimmed = tf.cond(tf.greater(sequence_length,trimmed_max_length), \n",
    "                                        lambda : tf.slice(processed_tokens,begin=[0],size=[trimmed_max_length],name='trimmed_out'),lambda : processed_tokens)\n",
    "        concat = tf.concat([['[CLS]'],values_trimmed,['[SEP]']],axis=0,name='concat_out')\n",
    "        actual_token_length = tf.shape(concat)[-1]\n",
    "        token_ids = self.vocab_table.lookup(concat)\n",
    "        padded = tf.pad(token_ids,paddings=[[0,max_sequence_length-actual_token_length]],name='input_ids')\n",
    "        return padded\n",
    "    \n",
    "    def process_sentences(self,sentences):\n",
    "        sentences_tokenized = tf.map_fn(process_sentence,sentences)\n",
    "        return sentences_tokenized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab lookup inside condition <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=-1> True & entire cond True\n",
      "vocab lookup <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=-1>\n",
      "missing b'aagent'\n",
      "vocab lookup inside condition <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=-1> True & entire cond True\n",
      "vocab lookup <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=-1>\n",
      "missing b'aagen'\n",
      "vocab lookup inside condition <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=-1> True & entire cond True\n",
      "vocab lookup <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=-1>\n",
      "missing b'aage'\n",
      "vocab lookup inside condition <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=-1> True & entire cond True\n",
      "vocab lookup <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=-1>\n",
      "missing b'aag'\n",
      "vocab lookup inside condition <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=-1> True & entire cond True\n",
      "vocab lookup <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=28335>\n",
      "present b'aa'\n",
      "vocab lookup inside condition <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=28335> False & entire cond False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>,\n",
       " <tf.Tensor: shape=(), dtype=int32, numpy=1>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'aa'>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=28335>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.while_loop(inner_cond,inner_while_body,loop_vars=[start,end,word,vocab_lookup])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/space/engineering/pretrained_models/bert/multi_cased_L-12_H-768_A-12/vocab.txt'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    <ipython-input-79-fe51f0aa31a9>:20 call  *\n        encoded_text = self.process_sentences(input_text)\n    <ipython-input-82-4978118e6274>:133 process_sentences  *\n        sentences_tokenized = tf.map_fn(process_sentence,sentences)\n    /opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py:574 new_func  **\n        return func(*args, **kwargs)\n    /opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/ops/map_fn.py:425 map_fn_v2\n        name=name)\n    /opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/ops/map_fn.py:270 map_fn\n        maximum_iterations=n)\n    /opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py:2688 while_loop\n        back_prop=back_prop)\n    /opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/ops/while_v2.py:196 while_loop\n        add_control_dependencies=add_control_dependencies)\n    /opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py:981 func_graph_from_py_func\n        func_outputs = python_func(*func_args, **func_kwargs)\n    /opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/ops/while_v2.py:174 wrapped_body\n        outputs = body(*_pack_sequence_as(orig_loop_vars, args))\n    /opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/ops/map_fn.py:259 compute\n        packed_fn_values = fn(packed_values)\n    <ipython-input-73-ff25ddb990a6>:6 process_sentence\n        subword_tokenized_list = tf.map_fn(joined_per_word_tokenized,words,parallel_iterations=None,infer_shape=False)\n    /opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py:574 new_func\n        return func(*args, **kwargs)\n    /opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/ops/map_fn.py:425 map_fn_v2\n        name=name)\n    /opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/ops/map_fn.py:270 map_fn\n        maximum_iterations=n)\n    /opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py:2688 while_loop\n        back_prop=back_prop)\n    /opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/ops/while_v2.py:196 while_loop\n        add_control_dependencies=add_control_dependencies)\n    /opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py:981 func_graph_from_py_func\n        func_outputs = python_func(*func_args, **func_kwargs)\n    /opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/ops/while_v2.py:174 wrapped_body\n        outputs = body(*_pack_sequence_as(orig_loop_vars, args))\n    /opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/ops/map_fn.py:259 compute\n        packed_fn_values = fn(packed_values)\n    <ipython-input-32-c61d2f63105a>:2 joined_per_word_tokenized\n        sub_tokens = per_word_tokenize(word)\n    <ipython-input-29-9789ba30d1a8>:9 per_word_tokenize\n        end = tf.Variable([chars_len])\n    /opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/ops/variables.py:261 __call__\n        return cls._variable_v2_call(*args, **kwargs)\n    /opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/ops/variables.py:255 _variable_v2_call\n        shape=shape)\n    /opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/ops/variables.py:236 <lambda>\n        previous_getter = lambda **kws: default_variable_creator_v2(None, **kws)\n    /opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py:2647 default_variable_creator_v2\n        shape=shape)\n    /opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/ops/variables.py:263 __call__\n        return super(VariableMetaclass, cls).__call__(*args, **kwargs)\n    /opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1434 __init__\n        distribute_strategy=distribute_strategy)\n    /opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1568 _init_from_args\n        name=\"initial_value\", dtype=dtype)\n    /opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:1341 convert_to_tensor\n        ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n    /opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:1455 _autopacking_conversion_function\n        return _autopacking_helper(v, dtype, name or \"packed\")\n    /opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:1361 _autopacking_helper\n        return gen_array_ops.pack(list_or_tuple, name=name)\n    /opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py:6333 pack\n        values, axis=axis, name=name, ctx=_ctx)\n    /opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py:6375 pack_eager_fallback\n        ctx=ctx, name=name)\n    /opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/eager/execute.py:75 quick_execute\n        raise e\n    /opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/eager/execute.py:60 quick_execute\n        inputs, attrs, num_outputs)\n\n    TypeError: An op outside of the function building code is being passed\n    a \"Graph\" tensor. It is possible to have Graph tensors\n    leak out of the function building context by including a\n    tf.init_scope in your function building code.\n    For example, the following function will fail:\n      @tf.function\n      def has_init_scope():\n        my_constant = tf.constant(1.)\n        with tf.init_scope():\n          added = my_constant * 2\n    The graph tensor has name: strided_slice:0\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-03747bffae86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0minput_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'input_text'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtokenized_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertTokenizationCustom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel_lookup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'input_text'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenized_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    920\u001b[0m                     not base_layer_utils.is_in_eager_or_tf_function()):\n\u001b[1;32m    921\u001b[0m                   \u001b[0;32mwith\u001b[0m \u001b[0mauto_control_deps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAutomaticControlDependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0macd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 922\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    923\u001b[0m                     \u001b[0;31m# Wrap Tensors in `outputs` in `tf.identity` to avoid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m                     \u001b[0;31m# circular dependencies.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    <ipython-input-79-fe51f0aa31a9>:20 call  *\n        encoded_text = self.process_sentences(input_text)\n    <ipython-input-82-4978118e6274>:133 process_sentences  *\n        sentences_tokenized = tf.map_fn(process_sentence,sentences)\n    /opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py:574 new_func  **\n        return func(*args, **kwargs)\n    /opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/ops/map_fn.py:425 map_fn_v2\n        name=name)\n    /opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/ops/map_fn.py:270 map_fn\n        maximum_iterations=n)\n    /opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py:2688 while_loop\n        back_prop=back_prop)\n    /opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/ops/while_v2.py:196 while_loop\n        add_control_dependencies=add_control_dependencies)\n    /opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py:981 func_graph_from_py_func\n        func_outputs = python_func(*func_args, **func_kwargs)\n    /opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/ops/while_v2.py:174 wrapped_body\n        outputs = body(*_pack_sequence_as(orig_loop_vars, args))\n    /opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/ops/map_fn.py:259 compute\n        packed_fn_values = fn(packed_values)\n    <ipython-input-73-ff25ddb990a6>:6 process_sentence\n        subword_tokenized_list = tf.map_fn(joined_per_word_tokenized,words,parallel_iterations=None,infer_shape=False)\n    /opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py:574 new_func\n        return func(*args, **kwargs)\n    /opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/ops/map_fn.py:425 map_fn_v2\n        name=name)\n    /opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/ops/map_fn.py:270 map_fn\n        maximum_iterations=n)\n    /opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py:2688 while_loop\n        back_prop=back_prop)\n    /opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/ops/while_v2.py:196 while_loop\n        add_control_dependencies=add_control_dependencies)\n    /opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py:981 func_graph_from_py_func\n        func_outputs = python_func(*func_args, **func_kwargs)\n    /opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/ops/while_v2.py:174 wrapped_body\n        outputs = body(*_pack_sequence_as(orig_loop_vars, args))\n    /opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/ops/map_fn.py:259 compute\n        packed_fn_values = fn(packed_values)\n    <ipython-input-32-c61d2f63105a>:2 joined_per_word_tokenized\n        sub_tokens = per_word_tokenize(word)\n    <ipython-input-29-9789ba30d1a8>:9 per_word_tokenize\n        end = tf.Variable([chars_len])\n    /opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/ops/variables.py:261 __call__\n        return cls._variable_v2_call(*args, **kwargs)\n    /opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/ops/variables.py:255 _variable_v2_call\n        shape=shape)\n    /opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/ops/variables.py:236 <lambda>\n        previous_getter = lambda **kws: default_variable_creator_v2(None, **kws)\n    /opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py:2647 default_variable_creator_v2\n        shape=shape)\n    /opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/ops/variables.py:263 __call__\n        return super(VariableMetaclass, cls).__call__(*args, **kwargs)\n    /opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1434 __init__\n        distribute_strategy=distribute_strategy)\n    /opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1568 _init_from_args\n        name=\"initial_value\", dtype=dtype)\n    /opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:1341 convert_to_tensor\n        ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n    /opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:1455 _autopacking_conversion_function\n        return _autopacking_helper(v, dtype, name or \"packed\")\n    /opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:1361 _autopacking_helper\n        return gen_array_ops.pack(list_or_tuple, name=name)\n    /opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py:6333 pack\n        values, axis=axis, name=name, ctx=_ctx)\n    /opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py:6375 pack_eager_fallback\n        ctx=ctx, name=name)\n    /opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/eager/execute.py:75 quick_execute\n        raise e\n    /opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/eager/execute.py:60 quick_execute\n        inputs, attrs, num_outputs)\n\n    TypeError: An op outside of the function building code is being passed\n    a \"Graph\" tensor. It is possible to have Graph tensors\n    leak out of the function building context by including a\n    tf.init_scope in your function building code.\n    For example, the following function will fail:\n      @tf.function\n      def has_init_scope():\n        my_constant = tf.constant(1.)\n        with tf.init_scope():\n          added = my_constant * 2\n    The graph tensor has name: strided_slice:0\n"
     ]
    }
   ],
   "source": [
    "input_text = tf.keras.Input(shape=(),dtype=tf.string,name='input_text')\n",
    "tokenized_out = BertTokenizationCustom(vocab_path=vocab_file)(input_text)\n",
    "model_lookup = tf.keras.Model(inputs={'input_text':input_text},outputs=tokenized_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lookup_out = VocabLookup(vocab_path=vocab_path)(input_text)\n",
    "lookup_out2 = VocabLookup2()(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_lookup = tf.keras.Model(inputs={'input_text':input_text},outputs=lookup_out)\n",
    "model_lookup2 = tf.keras.Model(inputs={'input_text':input_text},outputs=lookup_out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'aagen'>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "substring = tf.strings.join(tf.slice(chars,begin=start,size=end-start))\n",
    "substring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=string, numpy=array([b'a', b'a', b'g', b'e', b'n'], dtype=object)>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor grgaph for Innermost while loop:\n",
    "# while start < end:\n",
    "#     substr = \"\".join(chars[start:end])\n",
    "#     if start>0:\n",
    "#         substr = \"##\"+substr\n",
    "#     if substr in vocab:\n",
    "#         cur_substr = substr\n",
    "#         break\n",
    "#     end-=1\n",
    "\n",
    "input_text= tf.strings.split('talk to an agent')\n",
    "CharList = tf.strings.bytes_split(aa)\n",
    "CharListLen = tf.strings.length(aa)\n",
    "i =tf.constant(0)\n",
    "end = CharListLen[i].numpy()\n",
    "input_word =input_text[i].numpy()\n",
    "#input_word = CharList[i].numpy()\n",
    "substr =tf.Variable('',tf.string)\n",
    "def innerWhileCond(start,end,word):\n",
    "    return tf.less(start, end)\n",
    "    \n",
    "def innerWhileBody(start,end,word):\n",
    "    preString = tf.Variable(\"##\",tf.string)\n",
    "    chars = tf.strings.bytes_split(word)\n",
    "    substr = tf.strings.join(chars[start:end])\n",
    "    def f1(preString,substr):\n",
    "        substr = tf.strings.join(preString,substr)\n",
    "        return substr\n",
    "    def f3(substr):\n",
    "        substr = substr\n",
    "        return substr\n",
    "    substr= tf.cond(tf.less(0, start), f1(preString,substr),f3(substr))\n",
    "    tf.subtract(end,1)\n",
    "#     def f2():\n",
    "#         curr_substr = substr\n",
    "#         return curr_substr\n",
    "#     def f3():\n",
    "#         return tf.subtract(end,1)\n",
    "#     tf.cond(tf.less(0, start), f2, f3)\n",
    "    return (start,end,substr)\n",
    "\n",
    "\n",
    "res = tf.while_loop(innerWhileCond,innerWhileBody,[i,end,input_word])\n",
    "print(res)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_init = tf.lookup.TextFileInitializer(self.vocab_path,tf.string,tf.lookup.TextFileIndex.WHOLE_LINE,\n",
    "                              tf.int64,tf.lookup.TextFileIndex.LINE_NUMBER)\n",
    "        self.table = tf.lookup.StaticHashTable(table_init,-1)\n",
    "        self.built=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullTokenizer(tf.keras.layers.Layer):\n",
    "    def __init__(self,vocab_path):\n",
    "        super(FullTokenizer, self).__init__(trainable=False,dtype=tf.int32)\n",
    "        self.vocab_path = vocab_path\n",
    "    def build(self,input_shape):\n",
    "        table_init = tf.lookup.TextFileInitializer(self.vocab_path,tf.string,tf.lookup.TextFileIndex.WHOLE_LINE,\n",
    "                              tf.int64,tf.lookup.TextFileIndex.LINE_NUMBER)\n",
    "        self.table = tf.lookup.StaticHashTable(table_init,-1)\n",
    "        self.built=True\n",
    "        \n",
    "    def call(self, input_text):\n",
    "        splitted_text = tf.strings.split(input_text)\n",
    "        word_ids = self.table.lookup(splitted_text)\n",
    "        return word_ids\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(VocabLookup, self).get_config()\n",
    "        config.update({'vocab_path': self.vocab_path})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=string, numpy=array([b'talk', b'to', b'an', b'agent'], dtype=object)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text=\"talk to an agent\"\n",
    "tf.strings.split(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "in converted code:\n\n    <ipython-input-41-211a43b9ec10>:12 call  *\n        splitted_text = tf.strings.split(input_text).to_tensor()\n\n    AttributeError: 'SparseTensor' object has no attribute 'to_tensor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-acdd2dded222>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0minput_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'input_text'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtokenizer_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFullTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmodel_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'input_text'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf1.15/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    852\u001b[0m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_layer_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmark_as_return\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 854\u001b[0;31m                   \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOperatorNotAllowedInGraphError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf1.15/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    235\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: in converted code:\n\n    <ipython-input-41-211a43b9ec10>:12 call  *\n        splitted_text = tf.strings.split(input_text).to_tensor()\n\n    AttributeError: 'SparseTensor' object has no attribute 'to_tensor'\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "input_text = tf.keras.Input(shape=(),dtype=tf.string,name='input_text')\n",
    "tokenizer_out = FullTokenizer(vocab_path=vocab_file)(input_text)\n",
    "model_tokenizer = tf.keras.Model(inputs={'input_text':input_text},outputs=tokenizer_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('predict from model1 ', model_tokenizer.predict(['hi testing lookup in tf randomtext','testing lookup']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lookup.save(model_dir_lookup)\n",
    "model_lookup_loaded = tf.keras.models.load_model(model_dir_lookup)\n",
    "print('loaded model config 1 - \\n',model_lookup_loaded.get_config(),'\\n')\n",
    "print('predict from loaded model1 ',model_lookup_loaded.predict(['hi testing lookup in tf randomtext']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/tensorflow/tensorflow/issues/38305\n",
    "class VocabLookup(tf.keras.layers.Layer):\n",
    "    def __init__(self,vocab_path):\n",
    "        super(VocabLookup, self).__init__(trainable=False,dtype=tf.int32)\n",
    "        self.vocab_path = vocab_path\n",
    "    def build(self,input_shape):\n",
    "        table_init = tf.lookup.TextFileInitializer(self.vocab_path,tf.string,tf.lookup.TextFileIndex.WHOLE_LINE,\n",
    "                              tf.int64,tf.lookup.TextFileIndex.LINE_NUMBER)\n",
    "        self.table = tf.lookup.StaticHashTable(table_init,-1)\n",
    "        self.built=True\n",
    "        \n",
    "    def call(self, input_text):\n",
    "        splitted_text = tf.strings.split(input_text).to_tensor()\n",
    "        word_ids = self.table.lookup(splitted_text)\n",
    "        return word_ids\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(VocabLookup, self).get_config()\n",
    "        config.update({'vocab_path': self.vocab_path})\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "spt = sentencepiece_pb2.SentencePieceText()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "170"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spt.ParseFromString(sentence_piece_processor.encode_as_serialized_proto('talk to an agent'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: \"talk to an agent\"\n",
      "pieces {\n",
      "  piece: \"\\342\\226\\201t\"\n",
      "  id: 3\n",
      "  surface: \"t\"\n",
      "  begin: 0\n",
      "  end: 1\n",
      "}\n",
      "pieces {\n",
      "  piece: \"a\"\n",
      "  id: 135\n",
      "  surface: \"a\"\n",
      "  begin: 1\n",
      "  end: 2\n",
      "}\n",
      "pieces {\n",
      "  piece: \"l\"\n",
      "  id: 140\n",
      "  surface: \"l\"\n",
      "  begin: 2\n",
      "  end: 3\n",
      "}\n",
      "pieces {\n",
      "  piece: \"k\"\n",
      "  id: 151\n",
      "  surface: \"k\"\n",
      "  begin: 3\n",
      "  end: 4\n",
      "}\n",
      "pieces {\n",
      "  piece: \"\\342\\226\\201to\"\n",
      "  id: 10\n",
      "  surface: \" to\"\n",
      "  begin: 4\n",
      "  end: 7\n",
      "}\n",
      "pieces {\n",
      "  piece: \"\\342\\226\\201an\"\n",
      "  id: 43\n",
      "  surface: \" an\"\n",
      "  begin: 7\n",
      "  end: 10\n",
      "}\n",
      "pieces {\n",
      "  piece: \"\\342\\226\\201a\"\n",
      "  id: 9\n",
      "  surface: \" a\"\n",
      "  begin: 10\n",
      "  end: 12\n",
      "}\n",
      "pieces {\n",
      "  piece: \"ge\"\n",
      "  id: 27\n",
      "  surface: \"ge\"\n",
      "  begin: 12\n",
      "  end: 14\n",
      "}\n",
      "pieces {\n",
      "  piece: \"nt\"\n",
      "  id: 95\n",
      "  surface: \"nt\"\n",
      "  begin: 14\n",
      "  end: 16\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(spt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentence_piece_processor.load_vocabulary(dish_small_spm_vocab,1)\n",
    "sentence_piece_processor.load_vocabulary(vocab_file,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_piece_processor.reset_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁t', 'a', 'l', 'k', '▁to', '▁an', '▁a', 'ge', 'nt']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sentence_piece_processor.encode_as_pieces(\"talk to an agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_piece_processor.tokenize(\"talk to an agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'talk to an agent'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_piece_processor.decode_ids([3, 4, 12, 13, 191, 3, 4, 14, 3, 85, 3, 12, 117, 19, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/space/engineering/pretrained_models/bert/multi_cased_L-12_H-768_A-12/vocab.txt'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "not a list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-6da33643466e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msentence_piece_processor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf1.15/lib/python3.7/site-packages/sentencepiece.py\u001b[0m in \u001b[0;36mSetVocabulary\u001b[0;34m(self, valid_vocab)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mSetVocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_sentencepiece\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentencePieceProcessor_SetVocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mResetVocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: not a list"
     ]
    }
   ],
   "source": [
    "sentence_piece_processor.set_vocabulary(vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Internal: /sentencepiece/src/sentencepiece_processor.cc(137) [model_] Model is not initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-d4b9f90dfbe8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msentence_piece_processor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m75\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf1.15/lib/python3.7/site-packages/sentencepiece.py\u001b[0m in \u001b[0;36mLoadVocabulary\u001b[0;34m(self, filename, threshold)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mLoadVocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_sentencepiece\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentencePieceProcessor_LoadVocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mSetEncoderVersion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_version\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Internal: /sentencepiece/src/sentencepiece_processor.cc(137) [model_] Model is not initialized."
     ]
    }
   ],
   "source": [
    "sentence_piece_processor.load_vocabulary(vocab_file,75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 - tensorflow 2.2 (tf2.2)",
   "language": "python",
   "name": "tf2.2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
