{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "# import bert\n",
    "# from bert import BertModelLayer\n",
    "\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import sentencepiece as spm\n",
    "\n",
    "import tf_sentencepiece as tfs\n",
    "import sys\n",
    "sys.path.extend([\"/var/extra/users/jgeorge/tf2.0/git/models/\"])\n",
    "\n",
    "\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import csv\n",
    "import collections\n",
    "import pandas as pd\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from absl import logging\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# pylint: disable=g-import-not-at-top,redefined-outer-name,reimported\n",
    "from official.modeling import model_training_utils\n",
    "\n",
    "from official.nlp.modeling.models import bert_classifier, bert_pretrainer\n",
    "# from official.nlp.modeling.models.bert_classifier import BertClassifier\n",
    "# from official.nlp.modeling.models.bert_pretrainer import BertPretrainer\n",
    "# from official.nlp import bert_modeling as modeling\n",
    "# from official.nlp import bert_models\n",
    "from official.nlp import optimization\n",
    "from official.nlp.bert import common_flags\n",
    "from official.nlp.bert import input_pipeline\n",
    "from official.nlp.bert import model_saving_utils\n",
    "from official.utils.misc import distribution_utils\n",
    "from official.utils.misc import keras_utils\n",
    "from official.nlp.bert import tokenization\n",
    "\n",
    "from official.nlp.albert import configs as albert_configs\n",
    "from official.nlp.bert import run_classifier as run_classifier_bert\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.engine import network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAGS = flags.FLAGS    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:4', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:5', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:6', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:7', device_type='GPU')]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.experimental.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "##restricting no of gpus\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "device_to_use = gpus[2]\n",
    "tf.config.experimental.set_memory_growth(device_to_use,True)\n",
    "tf.config.experimental.set_visible_devices(device_to_use, 'GPU')\n",
    "print(tf.config.experimental.get_visible_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TFHUB_CACHE_DIR\"] = '/space/engineering/tfhub_modules'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "dish_data_path='/var/extra/users/jgeorge/tf2.0/input/dish/data/jan17_2020/dishDataJan17.txt'\n",
    "df = pd.read_csv(dish_data_path,sep='\\t',header=None,names=['filename','text','granular_intent','ru_intent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>text</th>\n",
       "      <th>granular_intent</th>\n",
       "      <th>ru_intent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INT-sv1appis14-1504137880316-305603_4567</td>\n",
       "      <td>can you send my bill to my mail?</td>\n",
       "      <td>billing-preferences</td>\n",
       "      <td>billing-preferences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>INT-va1appis15-1504373018548-41332</td>\n",
       "      <td>My Wally receiver has lost Satellite signal in...</td>\n",
       "      <td>comp_part_signal_loss-issue</td>\n",
       "      <td>comp_part_signal_loss-issue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INT-sv1appis12-1503954587819-263368</td>\n",
       "      <td>I need a payment extension so i don't get my s...</td>\n",
       "      <td>payment_extension-request</td>\n",
       "      <td>payment_extension-request</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8ed8e9b1-10f5-438d-c409-e616c3ff9ede</td>\n",
       "      <td>how can i find my local channels. it seems i d...</td>\n",
       "      <td>channel_package-issue</td>\n",
       "      <td>channel_package-issue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>INT-sv1appis13-1504099917638-293735</td>\n",
       "      <td>Wanted to speak with someone about my bill</td>\n",
       "      <td>representative-request</td>\n",
       "      <td>representative-request</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   filename  \\\n",
       "0  INT-sv1appis14-1504137880316-305603_4567   \n",
       "1        INT-va1appis15-1504373018548-41332   \n",
       "2       INT-sv1appis12-1503954587819-263368   \n",
       "3      8ed8e9b1-10f5-438d-c409-e616c3ff9ede   \n",
       "4       INT-sv1appis13-1504099917638-293735   \n",
       "\n",
       "                                                text  \\\n",
       "0                   can you send my bill to my mail?   \n",
       "1  My Wally receiver has lost Satellite signal in...   \n",
       "2  I need a payment extension so i don't get my s...   \n",
       "3  how can i find my local channels. it seems i d...   \n",
       "4         Wanted to speak with someone about my bill   \n",
       "\n",
       "               granular_intent                    ru_intent  \n",
       "0          billing-preferences          billing-preferences  \n",
       "1  comp_part_signal_loss-issue  comp_part_signal_loss-issue  \n",
       "2    payment_extension-request    payment_extension-request  \n",
       "3        channel_package-issue        channel_package-issue  \n",
       "4       representative-request       representative-request  "
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train,df_val,y_train,y_val = train_test_split(df,df['granular_intent'],train_size=0.8,random_state=42,stratify=df['granular_intent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_exp_folder = '/space/users/jgeorge/projects/k/tensorflow2-question-answering/input/dish/data/jan17_2020/'\n",
    "main_exp_folder = '/var/extra/users/jgeorge/tf2.0/input/dish/data/jan17_2020/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = '/var/extra/users/jgeorge/tf2.0/input/dish/models/albert_en_large'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_csv_file='/var/extra/users/jgeorge/tf2.0/input/dish/data/jan17_2020/train.txt'\n",
    "eval_csv_file='/var/extra/users/jgeorge/tf2.0/input/dish/data/jan17_2020/val.txt'\n",
    "\n",
    "df_train.to_csv(train_csv_file,sep='\\t',header=False,index=False)\n",
    "df_val.to_csv(eval_csv_file,sep='\\t',header=False,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_file = '/var/extra/users/jgeorge/tf2.0/input/dish/data/jan17_2020/intentlist.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(intent_file,'w',encoding='utf-8') as out_f:\n",
    "    for intent in sorted(df_train['granular_intent'].unique()):\n",
    "        out_f.write(intent+'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_list = []\n",
    "with open(intent_file,'r') as inp_f:\n",
    "    for intent in inp_f:\n",
    "        intent_list.append(intent.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bytes_feature(value):\n",
    "  \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "  if isinstance(value, type(tf.constant(0))):\n",
    "    print(\"type constant \",type(tf.constant(0)))\n",
    "    value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
    "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _int64_feature_list(values):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path=os.path.join(main_exp_folder,'train.tfrecords')\n",
    "eval_data_path=os.path.join(main_exp_folder,'eval.tfrecords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "    def __init__(self,text,label):\n",
    "        self.text = text\n",
    "        self.label = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeatures(object):\n",
    "    def __init__(self,input_ids,input_mask,\n",
    "                segment_ids,label_id):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "spm_model_path = os.path.join(model_dir, \"assets\", \"30k-clean.model\")\n",
    "spm_model = tf.io.gfile.GFile(spm_model_path, 'rb').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[  48,   25, 7022,   96,  108]], dtype=int32)>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfs.encode(['this is jithin'],model_proto=spm_model)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_ids = tfs.encode(['why did my bill increase by $10 from last months bill?'],model_proto=spm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int64_list {\n",
       "  value: 483\n",
       "  value: 144\n",
       "  value: 51\n",
       "  value: 1071\n",
       "  value: 1839\n",
       "  value: 34\n",
       "  value: 14737\n",
       "  value: 37\n",
       "  value: 236\n",
       "  value: 818\n",
       "  value: 1071\n",
       "  value: 60\n",
       "}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_int64_feature_list(word_ids.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 12), dtype=int32, numpy=\n",
       "array([[  483,   144,    51,  1071,  1839,    34, 14737,    37,   236,\n",
       "          818,  1071,    60]], dtype=int32)>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_single_example(ex_index, example, label_list, max_seq_length,sentence_piece):\n",
    "    CLS_ID = 2\n",
    "    SEP_ID = 3\n",
    "    # -2 for [CLS], [SEP]\n",
    "    max_word_ids = max_seq_length # -2\n",
    "    input_ids = []\n",
    "    segment_ids = []\n",
    "    input_mask = []\n",
    "#     input_ids.append(CLS_ID)\n",
    "#     word_ids = sentence_piece.encode_as_ids(example.text)\n",
    "    #TODO can take values only when not in the graph mode i think\n",
    "    word_ids = tfs.encode([example.text],model_proto=sentence_piece).values[0]\n",
    "    \n",
    "#     print('word ids ',word_ids)\n",
    "    word_ids = word_ids[:max_word_ids]\n",
    "    input_ids.extend(word_ids)\n",
    "#     print(\"input text \",example.text)\n",
    "#     input_ids.append(SEP_ID)\n",
    "    segment_ids = [0]*len(input_ids)\n",
    "    input_mask = [1]*len(input_ids)\n",
    "    if len(input_ids)<max_seq_length:\n",
    "        diff = max_seq_length - len(input_ids)\n",
    "        input_ids.extend([0]*diff)\n",
    "        segment_ids.extend([0]*diff)\n",
    "        input_mask.extend([0]*diff)\n",
    "    assert(len(input_ids)==max_seq_length)\n",
    "    assert(len(segment_ids)==max_seq_length)\n",
    "    assert(len(input_mask)==max_seq_length)\n",
    "    label_map = {label:i for i,label in enumerate(label_list)}\n",
    "    label_id = label_map[example.label]\n",
    "    feature = InputFeatures(input_ids,input_mask,segment_ids,label_id)\n",
    "    return feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 128\n",
    "# max_seq_length = bert_config.max_position_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# albert_model_dir = '/var/extra/users/jgeorge/tf2.0/input/albert_base'\n",
    "albert_model_dir = '/var/extra/users/jgeorge/tf2.0/input/albert_en_large'\n",
    "# https://tfhub.dev/tensorflow/albert_en_large/1?tf-hub-format=compressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "albert_hub_url='https://tfhub.dev/tensorflow/albert_en_large/1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = albert_model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "albert_config_file='/var/extra/users/jgeorge/tf2.0/input/albert_base/assets/albert_config.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_config = modeling.AlbertConfig.from_json_file(FLAGS.bert_config_file)\n",
    "\n",
    "# bert_config = modeling.AlbertConfig.from_json_file(albert_config)\n",
    "\n",
    "albert_config = albert_configs.AlbertConfig.from_json_file(albert_config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vocab_size': 30000,\n",
       " 'hidden_size': 768,\n",
       " 'num_hidden_layers': 12,\n",
       " 'num_attention_heads': 12,\n",
       " 'hidden_act': 'gelu',\n",
       " 'intermediate_size': 3072,\n",
       " 'hidden_dropout_prob': 0,\n",
       " 'attention_probs_dropout_prob': 0,\n",
       " 'max_position_embeddings': 512,\n",
       " 'type_vocab_size': 2,\n",
       " 'initializer_range': 0.02,\n",
       " 'backward_compatible': True,\n",
       " 'embedding_size': 128,\n",
       " 'num_hidden_groups': 1,\n",
       " 'net_structure_type': 0,\n",
       " 'gap_size': 0,\n",
       " 'num_memory_blocks': 0,\n",
       " 'inner_group_num': 1,\n",
       " 'down_scale_factor': 1}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "albert_config.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_training_data(input_csv_file,output_file,model_dir,delimiter='\\t',max_seq_length=128):\n",
    "    spm_model_path = os.path.join(model_dir, \"assets\", \"30k-clean.model\")\n",
    "    spm_model = tf.io.gfile.GFile(spm_model_path, 'rb').read()\n",
    "#     sp = spm.SentencePieceProcessor()\n",
    "#     sp.load(spm_model)\n",
    "   \n",
    "    with open(input_csv_file,'r',encoding='utf-8') as csv_file, tf.io.TFRecordWriter(output_file) as tf_record_writer:\n",
    "        #tried using binary format for bytes_list, but csv_reader requires text format\n",
    "#     with open(input_csv_file,'rb') as csv_file, tf.io.TFRecordWriter(output_file) as tf_record_writer:\n",
    "        csv_reader = csv.reader(csv_file,delimiter=delimiter,quotechar='\"')\n",
    "        for i,cols in enumerate(csv_reader):\n",
    "            ##skipping filename & granular tag (granular tag & final tag are the same here)\n",
    "#             yield cols[1:-1]\n",
    "            features = collections.OrderedDict()\n",
    "            \n",
    "            filename = cols[0]\n",
    "            text = cols[1]\n",
    "            if(len(cols)<4):\n",
    "                print('filename ',filename)\n",
    "            intent = cols[2]\n",
    "            input_example = InputExample(text,intent)\n",
    "            feature = convert_single_example(ex_index=i,example=input_example,label_list=intent_list,\n",
    "                                             max_seq_length=max_seq_length,sentence_piece=spm_model)\n",
    "#             text_encoded = sp.encode_as_ids(text)\n",
    "#             features['filename'] = _bytes_feature(filename.encode())\n",
    "#             features['text'] = _int64_feature_list(text_encoded)\n",
    "#             features['intent']  = _bytes_feature(intent.encode())\n",
    "\n",
    "            features[\"input_ids\"] = _int64_feature_list(feature.input_ids)\n",
    "            features[\"input_mask\"] = _int64_feature_list(feature.input_mask)\n",
    "            features[\"segment_ids\"] = _int64_feature_list(feature.segment_ids)\n",
    "            features[\"label_id\"] = _int64_feature(feature.label_id)\n",
    "#             features[\"is_real_example\"]\n",
    "            tf_example = tf.train.Example(features=tf.train.Features(feature=features))\n",
    "            tf_record_writer.write(tf_example.SerializeToString())\n",
    "\n",
    "#     tf_record_writer.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_training_data(train_csv_file,train_data_path,model_dir,max_seq_length=max_seq_length)\n",
    "write_training_data(eval_csv_file,eval_data_path,model_dir,max_seq_length=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_ds = tf.data.TFRecordDataset(train_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _decode_data(record):\n",
    "#     features = {\n",
    "#         'filename':tf.io.FixedLenFeature([],tf.string),\n",
    "#         'text':tf.io.VarLenFeature(tf.int64),\n",
    "#         'intent':tf.io.FixedLenFeature([],tf.string)\n",
    "#     }\n",
    "    features = {\n",
    "        \"input_ids\":tf.io.FixedLenFeature([max_seq_length],tf.int64),\n",
    "        \"input_mask\":tf.io.FixedLenFeature([max_seq_length],tf.int64),\n",
    "        \"segment_ids\":tf.io.FixedLenFeature([max_seq_length],tf.int64),\n",
    "        \"label_id\":tf.io.FixedLenFeature([],tf.int64)\n",
    "    }\n",
    "    return tf.io.parse_single_example(record,features=features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = raw_ds.map(_decode_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': <tf.Tensor: shape=(128,), dtype=int64, numpy=\n",
      "array([  483,   144,    51,  1071,  1839,    34, 14737,    37,   236,\n",
      "         818,  1071,    60,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0])>, 'input_mask': <tf.Tensor: shape=(128,), dtype=int64, numpy=\n",
      "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])>, 'label_id': <tf.Tensor: shape=(), dtype=int64, numpy=103>, 'segment_ids': <tf.Tensor: shape=(128,), dtype=int64, numpy=\n",
      "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])>}\n",
      "{'input_ids': <tf.Tensor: shape=(128,), dtype=int64, numpy=\n",
      "array([  13,    1,  259,   20, 3547,   14, 1889, 2440, 3607,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0])>, 'input_mask': <tf.Tensor: shape=(128,), dtype=int64, numpy=\n",
      "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])>, 'label_id': <tf.Tensor: shape=(), dtype=int64, numpy=34>, 'segment_ids': <tf.Tensor: shape=(128,), dtype=int64, numpy=\n",
      "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])>}\n"
     ]
    }
   ],
   "source": [
    "for line in processed_data.take(2):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "albert_hub_url = \"https://tfhub.dev/google/albert_base/2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TFHUB_CACHE_DIR\"] = '/var/extra/engineering/tfhub_modules'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strategy = distribution_utils.get_distribution_strategy('mirrored',num_gpus=1)\n",
    "\n",
    "# strategy = tf.distribute.OneDeviceStrategy(\"device:GPU:2\")\n",
    "#since devices to use is set to 2 already, only 1 device is visible which is 0\n",
    "strategy = tf.distribute.OneDeviceStrategy(\"device:GPU:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_fn(num_classes, loss_factor=1.0):\n",
    "  \"\"\"Gets the classification loss function.\"\"\"\n",
    "\n",
    "  def classification_loss_fn(labels, logits):\n",
    "    \"\"\"Classification loss.\"\"\"\n",
    "    labels = tf.squeeze(labels)\n",
    "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "    one_hot_labels = tf.one_hot(\n",
    "        tf.cast(labels, dtype=tf.int32), depth=num_classes, dtype=tf.float32)\n",
    "    per_example_loss = -tf.reduce_sum(\n",
    "        tf.cast(one_hot_labels, dtype=tf.float32) * log_probs, axis=-1)\n",
    "    loss = tf.reduce_mean(per_example_loss)\n",
    "    loss *= loss_factor\n",
    "    return loss\n",
    "  return classification_loss_fn  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "albert_hub_url3='https://tfhub.dev/google/albert_base/3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://tfhub.dev/google/albert_base/2\n"
     ]
    }
   ],
   "source": [
    "print(albert_hub_url)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tags = set()\n",
    "tags.add('train')\n",
    "bert_module = hub.load(albert_hub_url3,tags=tags)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "bert_module.signatures['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "def get_model_part1(bert_config,float_type,num_labels,max_seq_length,final_layer_initializer=None,hub_module_url=None):\n",
    "#     if final_layer_initializer is not None:  \n",
    "#         initializer = final_layer_initializer\n",
    "#     else:\n",
    "#         initializer = tf.keras.initializers.TruncatedNormal(stddev=bert_config.initializer_range)\n",
    "\n",
    "    if not hub_module_url:\n",
    "        #TODO\n",
    "        print(\"create the model\")\n",
    "        return None\n",
    "#     input_word_ids = tf.keras.Input(shape=(max_seq_length),dtype=tf.int32,name='input_word_ids')\n",
    "    input_ids = tf.keras.Input(shape=(max_seq_length),dtype=tf.int32,name='input_ids')\n",
    "    input_mask = tf.keras.Input(shape=(max_seq_length),dtype=tf.int32,name='input_mask')\n",
    "#     input_type_ids = tf.keras.Input(shape=(max_seq_length),dtype=tf.int32,name='input_type_ids')\n",
    "    segment_ids = tf.keras.Input(shape=(max_seq_length),dtype=tf.int32,name='segment_ids')\n",
    "    return input_ids,input_mask,segment_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = set()\n",
    "tags.add('train')\n",
    "# bert_model = hub.KerasLayer(albert_hub_url,trainable=True,tags=tags,signature='tokens',output_key='pooled_output')\n",
    "bert_model = hub.KerasLayer(albert_hub_url,trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "albert_hub_loaded = hub.load(albert_hub_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_UserObject' object has no attribute 'tags'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-157-95d4b5d89206>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0malbert_hub_loaded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: '_UserObject' object has no attribute 'tags'"
     ]
    }
   ],
   "source": [
    "albert_hub_loaded.tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://tfhub.dev/google/albert_base/2'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "albert_hub_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_model_bert_old(bert_config,float_type,num_labels,max_seq_length,final_layer_initializer,hub_module_url,\n",
    "                   input_ids,input_mask,segment_ids):\n",
    "    tags = set()\n",
    "    tags.add('train')\n",
    "    print(hub_module_url)\n",
    "#     bert_module = hub.Module(hub_module_url,trainable=True)\n",
    "\n",
    "#     bert_module = hub.load(hub_module_url,tags=tags)\n",
    "#     bert_inputs = {'input_ids':input_ids,\n",
    "#                    'input_mask':input_mask,\n",
    "#                    'segment_ids':segment_ids}\n",
    "\n",
    "    # https://www.tensorflow.org/hub/common_issues\n",
    "#     bert_outputs = bert_module.signatures['tokens'](\n",
    "#       inputs=bert_inputs,\n",
    "#       signature=\"tokens\",\n",
    "#       as_dict=True)\n",
    "\n",
    "#     bert_outputs = bert_module.signatures['tokens'](bert_inputs) #bert_outputs\n",
    "#     bert_outputs = bert_module.signatures['tokens'](input_ids=input_ids,input_mask=input_mask,segment_ids=segment_ids)\n",
    "#     print(bert_outputs)\n",
    "    \n",
    "#     bert_model = hub.KerasLayer(albert_hub_url,trainable=True,tags=tags,signature='tokens',signature_outputs_as_dict=True)\n",
    "    \n",
    "    ### pooled_output will give the representation for [CLS]\n",
    "    ### sequence_output will give representations for all tokens\n",
    "#     bert_model = hub.KerasLayer(albert_hub_url,trainable=True,tags=tags,signature='tokens',output_key='pooled_output')\n",
    "    bert_model = hub.KerasLayer(albert_hub_url,trainable=True,tags=None)\n",
    "    print(' bert_model ',bert_model)\n",
    "#     pooled_output,_ = bert_model([input_word_ids,input_mask,input_type_ids]) \n",
    "#     pooled_output,_ = bert_model(input_ids=input_ids,input_mask=input_mask,segment_ids=segment_ids) \n",
    "#     pooled_output,_ = bert_model(inputs = [input_ids,input_mask,segment_ids]) \n",
    "#     pooled_output,temp = bert_model(inputs = {'input_ids':input_ids,'input_mask':input_mask,'segment_ids':segment_ids})\n",
    "    pooled_output = bert_model(inputs = {'input_ids':input_ids,'input_mask':input_mask,'segment_ids':segment_ids})\n",
    "#     output = tf.keras.layers.Dropout(rate = bert_config.hidden_dropout_prob)(pooled_output)\n",
    "#     pooled_output = bert_outputs['pooled_output']\n",
    "    output = pooled_output\n",
    "#     print('pooled_output ',pooled_output,' temp '+temp)\n",
    "    print('pooled_output ',pooled_output)\n",
    "#     output = bert_outputs['pooled_output']\n",
    "    return output,bert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_keras_compile_fit(model_dir,\n",
    "                          strategy,\n",
    "                          model_fn,\n",
    "                          train_input_fn,\n",
    "                          eval_input_fn,\n",
    "                          loss_fn,\n",
    "                          metric_fn,\n",
    "                          init_checkpoint,\n",
    "                          epochs,\n",
    "                          steps_per_epoch,\n",
    "                          steps_per_loop,\n",
    "                          eval_steps,\n",
    "                          custom_callbacks=None):\n",
    "  \"\"\"Runs BERT classifier model using Keras compile/fit API.\"\"\"\n",
    "\n",
    "  with strategy.scope():\n",
    "    training_dataset = train_input_fn()\n",
    "    evaluation_dataset = eval_input_fn() if eval_input_fn else None\n",
    "    bert_model, sub_model = model_fn()\n",
    "    optimizer = bert_model.optimizer\n",
    "\n",
    "    if init_checkpoint:\n",
    "      checkpoint = tf.train.Checkpoint(model=sub_model)\n",
    "      checkpoint.restore(init_checkpoint).assert_existing_objects_matched()\n",
    "\n",
    "    bert_model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss_fn,\n",
    "        metrics=[metric_fn()])\n",
    "#     ,experimental_steps_per_execution=steps_per_loop)\n",
    "\n",
    "    summary_dir = os.path.join(model_dir, 'summaries')\n",
    "    summary_callback = tf.keras.callbacks.TensorBoard(summary_dir)\n",
    "    checkpoint_path = os.path.join(model_dir, 'checkpoint')\n",
    "    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        checkpoint_path, save_weights_only=True)\n",
    "\n",
    "    if custom_callbacks is not None:\n",
    "      custom_callbacks += [summary_callback, checkpoint_callback]\n",
    "    else:\n",
    "      custom_callbacks = [summary_callback, checkpoint_callback]\n",
    "\n",
    "    bert_model.fit(\n",
    "        x=training_dataset,\n",
    "        validation_data=evaluation_dataset,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        epochs=epochs,\n",
    "        validation_steps=eval_steps,\n",
    "        callbacks=custom_callbacks)\n",
    "\n",
    "    return bert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_fn():\n",
    "    return tf.keras.metrics.SparseCategoricalAccuracy('test_accuracy',dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/var/extra/users/jgeorge/tf2.0/input/dish/models/albert_en_large'"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init_checkpoint=output_folder\n",
    "init_checkpoint=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = hub.KerasLayer(albert_hub_url,trainable=True,tags=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_model_bert(bert_config,float_type,num_labels,max_seq_length,final_layer_initializer,hub_module_url,\n",
    "                   input_ids,input_mask,segment_ids):\n",
    "    tags = set()\n",
    "    tags.add('train')\n",
    "\n",
    "    ### pooled_output will give the representation for [CLS]\n",
    "    ### sequence_output will give representations for all tokens\n",
    "#     bert_model = hub.KerasLayer(albert_hub_url,trainable=True,tags=tags,signature='tokens',output_key='pooled_output')\n",
    "    bert_model = hub.KerasLayer(albert_hub_url,trainable=True,tags=None)\n",
    "    print(' bert_model ',bert_model)\n",
    "#     pooled_output,_ = bert_model([input_word_ids,input_mask,input_type_ids]) \n",
    "#     pooled_output,_ = bert_model(input_ids=input_ids,input_mask=input_mask,segment_ids=segment_ids) \n",
    "#     pooled_output,_ = bert_model(inputs = [input_ids,input_mask,segment_ids]) \n",
    "#     pooled_output,temp = bert_model(inputs = {'input_ids':input_ids,'input_mask':input_mask,'segment_ids':segment_ids})\n",
    "#     pooled_output, _ = bert_model([input_word_ids, input_mask, input_type_ids])\n",
    "    pooled_output, _ = bert_model([input_ids, input_mask, segment_ids])\n",
    "    output = tf.keras.layers.Dropout(rate=bert_config.hidden_dropout_prob)(pooled_output)                                                       \n",
    "#     pooled_output = bert_model(inputs = {'input_ids':input_ids,'input_mask':input_mask,'segment_ids':segment_ids})\n",
    "#     output = tf.keras.layers.Dropout(rate = bert_config.hidden_dropout_prob)(pooled_output)\n",
    "#     pooled_output = bert_outputs['pooled_output']\n",
    "#     output = pooled_output\n",
    "#     print('pooled_output ',pooled_output,' temp '+temp)\n",
    "    print('pooled_output ',pooled_output)\n",
    "#     output = bert_outputs['pooled_output']\n",
    "    return output,bert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_config=albert_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " bert_model  <tensorflow_hub.keras_layer.KerasLayer object at 0x7f70b5d017d0>\n",
      "pooled_output  Tensor(\"keras_layer_15/Identity:0\", shape=(None, 1024), dtype=float32)\n",
      "bert_output Tensor(\"dropout_4/Identity:0\", shape=(None, 1024), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "classifier_model,core_model = combine_model(bert_config=albert_config,\n",
    "                                                               float_type=tf.float32,\n",
    "                                                              num_labels=num_classes,\n",
    "                                                              max_seq_length=max_seq_length,\n",
    "                                                               hub_module_url=albert_hub_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " bert_model  <tensorflow_hub.keras_layer.KerasLayer object at 0x7f70933facd0>\n",
      "pooled_output  Tensor(\"keras_layer_16/Identity:0\", shape=(None, 1024), dtype=float32)\n",
      "bert_output Tensor(\"dropout_5/Identity:0\", shape=(None, 1024), dtype=float32)\n",
      "1285/1285 [==============================] - 1647s 1s/step - loss: 4.3848 - test_accuracy: 0.0555 - val_loss: 4.2406 - val_test_accuracy: 0.0647\n"
     ]
    }
   ],
   "source": [
    "loss_multiplier = 1\n",
    "max_seq_length = input_meta_data['max_seq_length']\n",
    "num_classes = input_meta_data['num_labels']\n",
    "loss_fn = get_loss_fn(num_classes,loss_multiplier)\n",
    "initial_lr = learning_rate\n",
    "def _get_classifier_model():\n",
    "#     bert_models.classifier_model\n",
    "#     classifier_model,core_model = get_classifier_model(bert_config=bert_config,\n",
    "#                                                                float_type=tf.float32,\n",
    "#                                                               num_labels=num_classes,\n",
    "#                                                               max_seq_length=max_seq_length,\n",
    "#                                                                hub_module_url=albert_hub_url)\n",
    "    classifier_model,core_model = combine_model(bert_config=albert_config,\n",
    "                                                               float_type=tf.float32,\n",
    "                                                              num_labels=num_classes,\n",
    "                                                              max_seq_length=max_seq_length,\n",
    "                                                               hub_module_url=albert_hub_url)\n",
    "    classifier_model.optimizer = optimization.create_optimizer(init_lr=initial_lr,\n",
    "                                                               num_train_steps=steps_per_epoch*epochs,\n",
    "                                                               num_warmup_steps=warmup_steps)\n",
    "    return classifier_model,core_model\n",
    "\n",
    "trained_model = run_keras_compile_fit(model_dir=output_folder,strategy=strategy,model_fn=_get_classifier_model,\n",
    "                                     train_input_fn=train_input_fn,eval_input_fn=eval_input_fn,\n",
    "                                      loss_fn=loss_fn,metric_fn=metric_fn,\n",
    "                                     init_checkpoint=init_checkpoint,\n",
    "                                      epochs=epochs,\n",
    "                                      steps_per_epoch=steps_per_epoch,\n",
    "                                      steps_per_loop=steps_per_loop,\n",
    "                                      eval_steps=eval_steps\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://tfhub.dev/tensorflow/albert_en_large/1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The SavedModel at /tmp/tfhub_modules/d0ceaf43f67b8744561ebeeaea4c7c188a6e6f78 has one MetaGraph with tags ['serve'], but got an incompatible argument tags={'train'} to tf.saved_model.load. You may omit it, pass 'None', or pass matching tags.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-135-6425cb3f7ba7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m       \u001b[0meval_input_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m       \u001b[0mrun_eagerly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m       use_keras_compile_fit=False)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-114-71fcfd2258d0>\u001b[0m in \u001b[0;36mrun_bert_classifier\u001b[0;34m(strategy, bert_config, input_meta_data, model_dir, epochs, steps_per_epoch, steps_per_loop, eval_steps, warmup_steps, initial_lr, init_checkpoint, train_input_fn, eval_input_fn, custom_callbacks, run_eagerly, use_keras_compile_fit)\u001b[0m\n\u001b[1;32m     53\u001b[0m       \u001b[0mmetric_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m       \u001b[0mcustom_callbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_callbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m       run_eagerly=run_eagerly)\n\u001b[0m",
      "\u001b[0;32m/var/extra/users/jgeorge/tf2.0/git/models/official/modeling/model_training_utils.py\u001b[0m in \u001b[0;36mrun_customized_training_loop\u001b[0;34m(_sentinel, strategy, model_fn, loss_fn, scale_loss, model_dir, train_input_fn, steps_per_epoch, steps_per_loop, epochs, eval_input_fn, eval_steps, metric_fn, init_checkpoint, custom_callbacks, run_eagerly, sub_model_export_name, explicit_allreduce, pre_allreduce_callbacks, post_allreduce_callbacks)\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0;31m# To correctly place the model weights on accelerators,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;31m# model and optimizer should be created in scope.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'optimizer'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m       raise ValueError('User should set optimizer attribute to model '\n",
      "\u001b[0;32m<ipython-input-114-71fcfd2258d0>\u001b[0m in \u001b[0;36m_get_classifier_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m                                                                   \u001b[0mnum_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                                                                   \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                                                                    hub_module_url=albert_hub_url)\n\u001b[0m\u001b[1;32m     32\u001b[0m         classifier_model.optimizer = optimization.create_optimizer(init_lr=initial_lr,\n\u001b[1;32m     33\u001b[0m                                                                    \u001b[0mnum_train_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-84-2662d067a25b>\u001b[0m in \u001b[0;36mcombine_model\u001b[0;34m(bert_config, float_type, num_labels, max_seq_length, final_layer_initializer, hub_module_url)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcombine_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfloat_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfinal_layer_initializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhub_module_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msegment_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model_part1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfloat_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfinal_layer_initializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhub_module_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mbert_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbert_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model_bert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfloat_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfinal_layer_initializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhub_module_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msegment_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bert_output'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbert_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model_part3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfinal_layer_initializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-129-e276c6602062>\u001b[0m in \u001b[0;36mget_model_bert\u001b[0;34m(bert_config, float_type, num_labels, max_seq_length, final_layer_initializer, hub_module_url, input_ids, input_mask, segment_ids)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m### pooled_output will give the representation for [CLS]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m### sequence_output will give representations for all tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mbert_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKerasLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malbert_hub_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tokens'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pooled_output'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' bert_model '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbert_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m#     pooled_output,_ = bert_model([input_word_ids,input_mask,input_type_ids])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow_hub/keras_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, handle, trainable, arguments, _sentinel, tags, signature, signature_outputs_as_dict, output_key, output_shape, **kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m           _convert_nest_to_shapes(output_shape))\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_training_argument\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_has_training_argument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_hub_module_v1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_is_hub_module_v1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow_hub/keras_layer.py\u001b[0m in \u001b[0;36mload_module\u001b[0;34m(handle, tags)\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodule_v2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow_hub/module_v2.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(handle, tags)\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtags\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_hub_module_v1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m       \u001b[0mtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m   \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_v1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m   \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_hub_module_v1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_hub_module_v1\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(export_dir, tags)\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mdon\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0mt\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0ma\u001b[0m \u001b[0mMetaGraph\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mSavedModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m   \"\"\"\n\u001b[0;32m--> 578\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mload_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36mload_internal\u001b[0;34m(export_dir, tags, loader_cls)\u001b[0m\n\u001b[1;32m    597\u001b[0m            \u001b[0;34m\"incompatible argument tags={} to tf.saved_model.load. You may omit \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m            \"it, pass 'None', or pass matching tags.\")\n\u001b[0;32m--> 599\u001b[0;31m           .format(export_dir, meta_graph_def.meta_info_def.tags, tags))\n\u001b[0m\u001b[1;32m    600\u001b[0m     \u001b[0mobject_graph_proto\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta_graph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobject_graph_def\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The SavedModel at /tmp/tfhub_modules/d0ceaf43f67b8744561ebeeaea4c7c188a6e6f78 has one MetaGraph with tags ['serve'], but got an incompatible argument tags={'train'} to tf.saved_model.load. You may omit it, pass 'None', or pass matching tags."
     ]
    }
   ],
   "source": [
    "trained_model = run_bert_classifier(\n",
    "      strategy,\n",
    "      albert_config,\n",
    "      input_meta_data,\n",
    "      albert_model_dir,\n",
    "      epochs,\n",
    "      steps_per_epoch,\n",
    "      steps_per_loop,\n",
    "      eval_steps,\n",
    "      warmup_steps,\n",
    "      learning_rate,\n",
    "      init_checkpoint,\n",
    "      train_input_fn,\n",
    "      eval_input_fn,\n",
    "      run_eagerly=False,\n",
    "      use_keras_compile_fit=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "def get_model_part3(bert_output,num_labels,final_layer_initializer):\n",
    "    if final_layer_initializer is not None:  \n",
    "        initializer = final_layer_initializer\n",
    "    else:\n",
    "        initializer = tf.keras.initializers.TruncatedNormal(stddev=bert_config.initializer_range)\n",
    "    #     output = tf.keras.layers.Dropout(rate = bert_config.hidden_dropout_prob)(pooled_output)\n",
    "    output = tf.keras.layers.Dense(num_labels,kernel_initializer=initializer,name='output',dtype=tf.float32)(bert_output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def combine_model(bert_config,float_type,num_labels,max_seq_length,final_layer_initializer=None,hub_module_url=None):\n",
    "    input_ids,input_mask,segment_ids = get_model_part1(bert_config,float_type,num_labels,max_seq_length,final_layer_initializer,hub_module_url)\n",
    "    bert_output,bert_model = get_model_bert(bert_config,float_type,num_labels,max_seq_length,final_layer_initializer,hub_module_url,input_ids,input_mask,segment_ids)\n",
    "    print('bert_output',bert_output)\n",
    "    output = get_model_part3(bert_output,num_labels,final_layer_initializer)\n",
    "    return tf.keras.Model(inputs={'input_ids':input_ids,\n",
    "                                 'input_mask':input_mask,\n",
    "                                 'segment_ids':segment_ids},\n",
    "                         outputs=output),bert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "def get_classifier_model(bert_config,float_type,num_labels,max_seq_length,final_layer_initializer=None,hub_module_url=None):\n",
    "    if final_layer_initializer is not None:  \n",
    "        initializer = final_layer_initializer\n",
    "    else:\n",
    "        initializer = tf.keras.initializers.TruncatedNormal(stddev=bert_config.initializer_range)\n",
    "\n",
    "    if not hub_module_url:\n",
    "        #TODO\n",
    "        print(\"create the model\")\n",
    "        return None\n",
    "#     input_word_ids = tf.keras.Input(shape=(max_seq_length),dtype=tf.int32,name='input_word_ids')\n",
    "    input_ids = tf.keras.Input(shape=(max_seq_length),dtype=tf.int32,name='input_ids')\n",
    "    input_mask = tf.keras.Input(shape=(max_seq_length),dtype=tf.int32,name='input_mask')\n",
    "#     input_type_ids = tf.keras.Input(shape=(max_seq_length),dtype=tf.int32,name='input_type_ids')\n",
    "    segment_ids = tf.keras.Input(shape=(max_seq_length),dtype=tf.int32,name='segment_ids')\n",
    "    tags = set()\n",
    "    tags.add('train')\n",
    "#     bert_module = hub.Module(hub_module_url,trainable=True)\n",
    "    bert_module = hub.load(hub_module_url,tags=tags)\n",
    "    bert_inputs = {'input_ids':input_ids,\n",
    "                   'input_mask':input_mask,\n",
    "                   'segment_ids':segment_ids}\n",
    "    # https://www.tensorflow.org/hub/common_issues\n",
    "#     bert_outputs = bert_module.signatures['tokens'](\n",
    "#       inputs=bert_inputs,\n",
    "#       signature=\"tokens\",\n",
    "#       as_dict=True)\n",
    "\n",
    "#     bert_outputs = bert_module.signatures['tokens'](\n",
    "#       bert_inputs)\n",
    "    bert_outputs = bert_module.signatures['tokens'](input_ids=input_ids,input_mask=input_mask,segment_ids=segment_ids)\n",
    "    print(bert_outputs)\n",
    "    \n",
    "#     bert_model = hub.KerasLayer(albert_hub_url,trainable=True,tags=tags,signature='tokens',signature_outputs_as_dict=True)\n",
    "#     pooled_output,_ = bert_model([input_word_ids,input_mask,input_type_ids]) \n",
    "#     pooled_output,_ = bert_model([segment_ids,input_ids,input_mask]) \n",
    "#     output = tf.keras.layers.Dropout(rate = bert_config.hidden_dropout_prob)(pooled_output)\n",
    "#     pooled_output = bert_outputs['pooled_output']\n",
    "    output = bert_outputs['pooled_output']\n",
    "#     output = tf.keras.layers.Dropout(rate = bert_config.hidden_dropout_prob)(pooled_output)\n",
    "    output = tf.keras.layers.Dense(num_labels,kernel_initializer=initializer,name='output',dtype=tf.float32)(output)\n",
    "    return tf.keras.Model(inputs={'input_ids':input_ids,\n",
    "                                 'input_mask':input_mask,\n",
    "                                 'segment_ids':segment_ids},\n",
    "                         outputs=output),bert_model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clear the existing tensorflow graph\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_bert_classifier(strategy,\n",
    "                        bert_config,\n",
    "                        input_meta_data,\n",
    "                        model_dir,\n",
    "                        epochs,\n",
    "                        steps_per_epoch,\n",
    "                        steps_per_loop,\n",
    "                        eval_steps,\n",
    "                        warmup_steps,\n",
    "                        initial_lr,\n",
    "                        init_checkpoint,\n",
    "                        train_input_fn,\n",
    "                        eval_input_fn,\n",
    "                        custom_callbacks=None,\n",
    "                        run_eagerly=False,\n",
    "                        use_keras_compile_fit=False):\n",
    "    \"\"\"Run BERT classifier training using low-level API.\"\"\"\n",
    "    max_seq_length = input_meta_data['max_seq_length']\n",
    "    num_classes = input_meta_data['num_labels']\n",
    "    def _get_classifier_model():\n",
    "#     bert_models.classifier_model\n",
    "#     classifier_model,core_model = get_classifier_model(bert_config=bert_config,\n",
    "#                                                                float_type=tf.float32,\n",
    "#                                                               num_labels=num_classes,\n",
    "#                                                               max_seq_length=max_seq_length,\n",
    "#                                                                hub_module_url=albert_hub_url)\n",
    "        classifier_model,core_model = combine_model(bert_config=bert_config,\n",
    "                                                                   float_type=tf.float32,\n",
    "                                                                  num_labels=num_classes,\n",
    "                                                                  max_seq_length=max_seq_length,\n",
    "                                                                   hub_module_url=albert_hub_url)\n",
    "        classifier_model.optimizer = optimization.create_optimizer(init_lr=initial_lr,\n",
    "                                                                   num_train_steps=steps_per_epoch*epochs,\n",
    "                                                                   num_warmup_steps=warmup_steps)\n",
    "        return classifier_model,core_model\n",
    "    loss_multiplier = 1\n",
    "    loss_fn = get_loss_fn(num_classes,loss_multiplier)\n",
    "    \n",
    "    def metric_fn():\n",
    "        return tf.keras.metrics.SparseCategoricalAccuracy('test_accuracy',dtype=tf.float32)\n",
    "    return model_training_utils.run_customized_training_loop(\n",
    "      strategy=strategy,\n",
    "      model_fn=_get_classifier_model,\n",
    "      loss_fn=loss_fn,\n",
    "      model_dir=model_dir,\n",
    "      steps_per_epoch=steps_per_epoch,\n",
    "      steps_per_loop=steps_per_loop,\n",
    "      epochs=epochs,\n",
    "      train_input_fn=train_input_fn,\n",
    "      eval_input_fn=eval_input_fn,\n",
    "      eval_steps=eval_steps,\n",
    "      init_checkpoint=init_checkpoint,\n",
    "      metric_fn=metric_fn,\n",
    "      custom_callbacks=custom_callbacks,\n",
    "      run_eagerly=run_eagerly)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/var/extra/users/jgeorge/tf2.0/input/albert_base'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "albert_model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy.experimental_run_v2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_meta_data = {\n",
    "    'max_seq_length':128,\n",
    "    'num_labels':142,\n",
    "    'train_data_size':20574,\n",
    "    'eval_data_size':5144\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_meta_path = os.path.join(main_exp_folder,'input_metadata.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writing metadata file\n",
    "with open(input_meta_path,'w',encoding='utf-8') as jf:\n",
    "    jf.write(json.dumps(input_meta_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading metadata file\n",
    "with open(input_meta_path,'r',encoding='utf-8') as jf:\n",
    "#     input_meta_data = json.loads(jf.read())\n",
    "    input_meta_data = json.load(jf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_loop = 1\n",
    "learning_rate=1e-5\n",
    "epochs=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size=16\n",
    "eval_batch_size=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir_dish=os.path.join(main_exp_folder,'models')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/var/extra/users/jgeorge/tf2.0/input/dish/data/jan17_2020/models'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dir_dish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_checkpoint=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1285\n"
     ]
    }
   ],
   "source": [
    "train_data_size = input_meta_data['train_data_size']\n",
    "steps_per_epoch = int(train_data_size / train_batch_size)\n",
    "print(steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "warmup_steps = int(epochs * train_data_size * 0.1 / train_batch_size)\n",
    "eval_steps = int(math.ceil(input_meta_data['eval_data_size'] / eval_batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_record(record, name_to_features):\n",
    "  \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n",
    "  example = tf.io.parse_single_example(record, name_to_features)\n",
    "\n",
    "  # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n",
    "  # So cast all int64 to int32.\n",
    "  for name in list(example.keys()):\n",
    "    t = example[name]\n",
    "    if t.dtype == tf.int64:\n",
    "      t = tf.cast(t, tf.int32)\n",
    "    example[name] = t\n",
    "\n",
    "  return example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_file_dataset(input_file, name_to_features):\n",
    "  \"\"\"Creates a single-file dataset to be passed for BERT custom training.\"\"\"\n",
    "  # For training, we want a lot of parallel reading and shuffling.\n",
    "  # For eval, we want no shuffling and parallel reading doesn't matter.\n",
    "  d = tf.data.TFRecordDataset(input_file)\n",
    "  d = d.map(lambda record: decode_record(record, name_to_features))\n",
    "\n",
    "  # When `input_file` is a path to a single file or a list\n",
    "  # containing a single path, disable auto sharding so that\n",
    "  # same input file is sent to all workers.\n",
    "  if isinstance(input_file, str) or len(input_file) == 1:\n",
    "    options = tf.data.Options()\n",
    "    options.experimental_distribute.auto_shard_policy = (\n",
    "        tf.data.experimental.AutoShardPolicy.OFF)\n",
    "    d = d.with_options(options)\n",
    "  return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classifier_dataset(file_path,\n",
    "                              seq_length,\n",
    "                              batch_size,\n",
    "                              is_training=True,\n",
    "                              input_pipeline_context=None):\n",
    "  \"\"\"Creates input dataset from (tf)records files for train/eval.\"\"\"\n",
    "  name_to_features = {\n",
    "      'input_ids': tf.io.FixedLenFeature([seq_length], tf.int64),\n",
    "      'input_mask': tf.io.FixedLenFeature([seq_length], tf.int64),\n",
    "      'segment_ids': tf.io.FixedLenFeature([seq_length], tf.int64),\n",
    "      'label_id': tf.io.FixedLenFeature([], tf.int64)\n",
    "#       'is_real_example': tf.io.FixedLenFeature([], tf.int64),\n",
    "  }\n",
    "  dataset = single_file_dataset(file_path, name_to_features)\n",
    "\n",
    "  # The dataset is always sharded by number of hosts.\n",
    "  # num_input_pipelines is the number of hosts rather than number of cores.\n",
    "  if input_pipeline_context and input_pipeline_context.num_input_pipelines > 1:\n",
    "    dataset = dataset.shard(input_pipeline_context.num_input_pipelines,\n",
    "                            input_pipeline_context.input_pipeline_id)\n",
    "\n",
    "  def _select_data_from_record(record):\n",
    "#     x = {\n",
    "#         'input_word_ids': record['input_ids'],\n",
    "#         'input_mask': record['input_mask'],\n",
    "#         'input_type_ids': record['segment_ids']\n",
    "#     }\n",
    "    x = {\n",
    "        'input_ids': record['input_ids'],\n",
    "        'input_mask': record['input_mask'],\n",
    "        'segment_ids': record['segment_ids']\n",
    "    }\n",
    "    y = record['label_id']\n",
    "    return (x, y)\n",
    "\n",
    "  dataset = dataset.map(_select_data_from_record)\n",
    "\n",
    "  if is_training:\n",
    "    dataset = dataset.shuffle(100)\n",
    "    dataset = dataset.repeat()\n",
    "\n",
    "  dataset = dataset.batch(batch_size, drop_remainder=is_training)\n",
    "  dataset = dataset.prefetch(1024)\n",
    "  return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_fn(input_file_pattern, max_seq_length, global_batch_size,\n",
    "                   is_training):\n",
    "  \"\"\"Gets a closure to create a dataset.\"\"\"\n",
    "  def _dataset_fn(ctx=None):\n",
    "    \"\"\"Returns tf.data.Dataset for distributed BERT pretraining.\"\"\"\n",
    "    batch_size = ctx.get_per_replica_batch_size(\n",
    "        global_batch_size) if ctx else global_batch_size\n",
    "#     dataset = input_pipeline.create_classifier_dataset(\n",
    "    dataset = create_classifier_dataset(\n",
    "        input_file_pattern,\n",
    "        max_seq_length,\n",
    "        batch_size,\n",
    "        is_training=is_training,\n",
    "        input_pipeline_context=ctx)\n",
    "    return dataset\n",
    "  return _dataset_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_fn = get_dataset_fn(\n",
    "#       FLAGS.train_data_path,\n",
    "      train_data_path,\n",
    "      max_seq_length,\n",
    "#       FLAGS.train_batch_size,\n",
    "      train_batch_size,\n",
    "      is_training=True)\n",
    "eval_input_fn = get_dataset_fn(\n",
    "#       FLAGS.eval_data_path,\n",
    "      eval_data_path,\n",
    "      max_seq_length,\n",
    "#       FLAGS.eval_batch_size,\n",
    "      eval_batch_size,\n",
    "      is_training=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<official.nlp.albert.configs.AlbertConfig at 0x7f74029d7ad0>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "albert_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " bert_model  <tensorflow_hub.keras_layer.KerasLayer object at 0x7f7402ebb150>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    /opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow_hub/keras_layer.py:206 call  *\n        self._check_trainability()\n    /opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow_hub/keras_layer.py:265 _check_trainability  *\n        raise ValueError(\n\n    ValueError: Setting hub.KerasLayer.trainable = True is unsupported when loading from the hub.Module format of TensorFlow 1.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-115-6425cb3f7ba7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m       \u001b[0meval_input_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m       \u001b[0mrun_eagerly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m       use_keras_compile_fit=False)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-114-71fcfd2258d0>\u001b[0m in \u001b[0;36mrun_bert_classifier\u001b[0;34m(strategy, bert_config, input_meta_data, model_dir, epochs, steps_per_epoch, steps_per_loop, eval_steps, warmup_steps, initial_lr, init_checkpoint, train_input_fn, eval_input_fn, custom_callbacks, run_eagerly, use_keras_compile_fit)\u001b[0m\n\u001b[1;32m     53\u001b[0m       \u001b[0mmetric_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m       \u001b[0mcustom_callbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_callbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m       run_eagerly=run_eagerly)\n\u001b[0m",
      "\u001b[0;32m/var/extra/users/jgeorge/tf2.0/git/models/official/modeling/model_training_utils.py\u001b[0m in \u001b[0;36mrun_customized_training_loop\u001b[0;34m(_sentinel, strategy, model_fn, loss_fn, scale_loss, model_dir, train_input_fn, steps_per_epoch, steps_per_loop, epochs, eval_input_fn, eval_steps, metric_fn, init_checkpoint, custom_callbacks, run_eagerly, sub_model_export_name, explicit_allreduce, pre_allreduce_callbacks, post_allreduce_callbacks)\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0;31m# To correctly place the model weights on accelerators,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;31m# model and optimizer should be created in scope.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'optimizer'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m       raise ValueError('User should set optimizer attribute to model '\n",
      "\u001b[0;32m<ipython-input-114-71fcfd2258d0>\u001b[0m in \u001b[0;36m_get_classifier_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m                                                                   \u001b[0mnum_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                                                                   \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                                                                    hub_module_url=albert_hub_url)\n\u001b[0m\u001b[1;32m     32\u001b[0m         classifier_model.optimizer = optimization.create_optimizer(init_lr=initial_lr,\n\u001b[1;32m     33\u001b[0m                                                                    \u001b[0mnum_train_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-84-2662d067a25b>\u001b[0m in \u001b[0;36mcombine_model\u001b[0;34m(bert_config, float_type, num_labels, max_seq_length, final_layer_initializer, hub_module_url)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcombine_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfloat_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfinal_layer_initializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhub_module_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msegment_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model_part1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfloat_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfinal_layer_initializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhub_module_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mbert_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbert_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model_bert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfloat_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfinal_layer_initializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhub_module_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msegment_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bert_output'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbert_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model_part3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfinal_layer_initializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-82-0556aeefba5b>\u001b[0m in \u001b[0;36mget_model_bert\u001b[0;34m(bert_config, float_type, num_labels, max_seq_length, final_layer_initializer, hub_module_url, input_ids, input_mask, segment_ids)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m#     pooled_output,_ = bert_model(inputs = [input_ids,input_mask,segment_ids])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m#     pooled_output,temp = bert_model(inputs = {'input_ids':input_ids,'input_mask':input_mask,'segment_ids':segment_ids})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mpooled_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'input_mask'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0minput_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'segment_ids'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msegment_ids\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;31m#     output = tf.keras.layers.Dropout(rate = bert_config.hidden_dropout_prob)(pooled_output)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m#     pooled_output = bert_outputs['pooled_output']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    920\u001b[0m                     not base_layer_utils.is_in_eager_or_tf_function()):\n\u001b[1;32m    921\u001b[0m                   \u001b[0;32mwith\u001b[0m \u001b[0mauto_control_deps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAutomaticControlDependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0macd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 922\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    923\u001b[0m                     \u001b[0;31m# Wrap Tensors in `outputs` in `tf.identity` to avoid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m                     \u001b[0;31m# circular dependencies.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow_hub/keras_layer.py:206 call  *\n        self._check_trainability()\n    /opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow_hub/keras_layer.py:265 _check_trainability  *\n        raise ValueError(\n\n    ValueError: Setting hub.KerasLayer.trainable = True is unsupported when loading from the hub.Module format of TensorFlow 1.\n"
     ]
    }
   ],
   "source": [
    "trained_model = run_bert_classifier(\n",
    "      strategy,\n",
    "      albert_config,\n",
    "      input_meta_data,\n",
    "      albert_model_dir,\n",
    "      epochs,\n",
    "      steps_per_epoch,\n",
    "      steps_per_loop,\n",
    "      eval_steps,\n",
    "      warmup_steps,\n",
    "      learning_rate,\n",
    "      init_checkpoint,\n",
    "      train_input_fn,\n",
    "      eval_input_fn,\n",
    "      run_eagerly=False,\n",
    "      use_keras_compile_fit=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_and_labels(strategy, trained_model, eval_input_fn,\n",
    "                               eval_steps):\n",
    "  \"\"\"Obtains predictions of trained model on evaluation data.\n",
    "  Note that list of labels is returned along with the predictions because the\n",
    "  order changes on distributing dataset over TPU pods.\n",
    "  Args:\n",
    "    strategy: Distribution strategy.\n",
    "    trained_model: Trained model with preloaded weights.\n",
    "    eval_input_fn: Input function for evaluation data.\n",
    "    eval_steps: Number of evaluation steps.\n",
    "  Returns:\n",
    "    predictions: List of predictions.\n",
    "    labels: List of gold labels corresponding to predictions.\n",
    "  \"\"\"\n",
    "\n",
    "  @tf.function\n",
    "  def test_step(iterator):\n",
    "    \"\"\"Computes predictions on distributed devices.\"\"\"\n",
    "\n",
    "    def _test_step_fn(inputs):\n",
    "      \"\"\"Replicated predictions.\"\"\"\n",
    "      inputs, labels = inputs\n",
    "      model_outputs = trained_model(inputs, training=False)\n",
    "      return model_outputs, labels\n",
    "\n",
    "    outputs, labels = strategy.run(\n",
    "        _test_step_fn, args=(next(iterator),))\n",
    "    # outputs: current batch logits as a tuple of shard logits\n",
    "    outputs = tf.nest.map_structure(strategy.experimental_local_results,\n",
    "                                    outputs)\n",
    "    labels = tf.nest.map_structure(strategy.experimental_local_results, labels)\n",
    "    return outputs, labels\n",
    "\n",
    "  def _run_evaluation(test_iterator):\n",
    "    \"\"\"Runs evaluation steps.\"\"\"\n",
    "    preds, golds = list(), list()\n",
    "    for _ in range(eval_steps):\n",
    "      logits, labels = test_step(test_iterator)\n",
    "      for cur_logits, cur_labels in zip(logits, labels):\n",
    "        preds.extend(tf.math.argmax(cur_logits, axis=1).numpy())\n",
    "        golds.extend(cur_labels.numpy().tolist())\n",
    "    return preds, golds\n",
    "\n",
    "  test_iter = iter(\n",
    "      strategy.experimental_distribute_datasets_from_function(eval_input_fn))\n",
    "  predictions, labels = _run_evaluation(test_iter)\n",
    "\n",
    "  return predictions, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "322"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions,labels = get_predictions_and_labels(strategy,trained_model,eval_input_fn,eval_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128,\n",
       " 25,\n",
       " 127,\n",
       " 30,\n",
       " 35,\n",
       " 128,\n",
       " 41,\n",
       " 22,\n",
       " 32,\n",
       " 34,\n",
       " 22,\n",
       " 24,\n",
       " 103,\n",
       " 128,\n",
       " 37,\n",
       " 32,\n",
       " 91,\n",
       " 1,\n",
       " 12,\n",
       " 128,\n",
       " 4,\n",
       " 5,\n",
       " 88,\n",
       " 127,\n",
       " 131,\n",
       " 128,\n",
       " 103,\n",
       " 97,\n",
       " 51,\n",
       " 133,\n",
       " 91,\n",
       " 1,\n",
       " 22,\n",
       " 103,\n",
       " 22,\n",
       " 76,\n",
       " 32,\n",
       " 7,\n",
       " 88,\n",
       " 97,\n",
       " 43,\n",
       " 8,\n",
       " 4,\n",
       " 128,\n",
       " 128,\n",
       " 97,\n",
       " 1,\n",
       " 22,\n",
       " 44,\n",
       " 97,\n",
       " 1,\n",
       " 76,\n",
       " 25,\n",
       " 103,\n",
       " 17,\n",
       " 31,\n",
       " 37,\n",
       " 14,\n",
       " 26,\n",
       " 141,\n",
       " 30,\n",
       " 20,\n",
       " 40,\n",
       " 82,\n",
       " 32,\n",
       " 103,\n",
       " 122,\n",
       " 103,\n",
       " 30,\n",
       " 32,\n",
       " 1,\n",
       " 128,\n",
       " 90,\n",
       " 70,\n",
       " 45,\n",
       " 30,\n",
       " 37,\n",
       " 40,\n",
       " 136,\n",
       " 22,\n",
       " 88,\n",
       " 21,\n",
       " 89,\n",
       " 103,\n",
       " 73,\n",
       " 28,\n",
       " 97,\n",
       " 92,\n",
       " 128,\n",
       " 32,\n",
       " 65,\n",
       " 41,\n",
       " 93,\n",
       " 89,\n",
       " 90,\n",
       " 112,\n",
       " 1,\n",
       " 26,\n",
       " 80,\n",
       " 68,\n",
       " 125,\n",
       " 1,\n",
       " 26,\n",
       " 37,\n",
       " 103,\n",
       " 91,\n",
       " 30,\n",
       " 80,\n",
       " 20,\n",
       " 30,\n",
       " 30,\n",
       " 74,\n",
       " 127,\n",
       " 93,\n",
       " 63,\n",
       " 48,\n",
       " 83,\n",
       " 12,\n",
       " 106,\n",
       " 68,\n",
       " 59,\n",
       " 32,\n",
       " 69,\n",
       " 128,\n",
       " 35,\n",
       " 128,\n",
       " 32,\n",
       " 107,\n",
       " 34,\n",
       " 128,\n",
       " 23,\n",
       " 32,\n",
       " 37,\n",
       " 32,\n",
       " 36,\n",
       " 25,\n",
       " 24,\n",
       " 1,\n",
       " 20,\n",
       " 131,\n",
       " 103,\n",
       " 23,\n",
       " 91,\n",
       " 131,\n",
       " 1,\n",
       " 46,\n",
       " 112,\n",
       " 25,\n",
       " 22,\n",
       " 67,\n",
       " 92,\n",
       " 128,\n",
       " 1,\n",
       " 22,\n",
       " 31,\n",
       " 34,\n",
       " 91,\n",
       " 10,\n",
       " 43,\n",
       " 1,\n",
       " 93,\n",
       " 93,\n",
       " 41,\n",
       " 34,\n",
       " 17,\n",
       " 107,\n",
       " 112,\n",
       " 22,\n",
       " 128,\n",
       " 9,\n",
       " 21,\n",
       " 15,\n",
       " 114,\n",
       " 15,\n",
       " 22,\n",
       " 114,\n",
       " 29,\n",
       " 133,\n",
       " 103,\n",
       " 38,\n",
       " 34,\n",
       " 128,\n",
       " 74,\n",
       " 53,\n",
       " 30,\n",
       " 34,\n",
       " 90,\n",
       " 91,\n",
       " 0,\n",
       " 34,\n",
       " 30,\n",
       " 34,\n",
       " 106,\n",
       " 55,\n",
       " 22,\n",
       " 42,\n",
       " 88,\n",
       " 7,\n",
       " 71,\n",
       " 29,\n",
       " 22,\n",
       " 23,\n",
       " 1,\n",
       " 67,\n",
       " 103,\n",
       " 128,\n",
       " 91,\n",
       " 97,\n",
       " 25,\n",
       " 88,\n",
       " 36,\n",
       " 89,\n",
       " 30,\n",
       " 22,\n",
       " 39,\n",
       " 103,\n",
       " 30,\n",
       " 77,\n",
       " 1,\n",
       " 90,\n",
       " 128,\n",
       " 8,\n",
       " 97,\n",
       " 37,\n",
       " 128,\n",
       " 12,\n",
       " 128,\n",
       " 39,\n",
       " 83,\n",
       " 30,\n",
       " 96,\n",
       " 88,\n",
       " 20,\n",
       " 66,\n",
       " 114,\n",
       " 97,\n",
       " 97,\n",
       " 60,\n",
       " 1,\n",
       " 67,\n",
       " 1,\n",
       " 43,\n",
       " 128,\n",
       " 90,\n",
       " 1,\n",
       " 97,\n",
       " 93,\n",
       " 88,\n",
       " 30,\n",
       " 70,\n",
       " 22,\n",
       " 83,\n",
       " 76,\n",
       " 26,\n",
       " 1,\n",
       " 34,\n",
       " 95,\n",
       " 93,\n",
       " 56,\n",
       " 30,\n",
       " 128,\n",
       " 123,\n",
       " 69,\n",
       " 32,\n",
       " 81,\n",
       " 97,\n",
       " 30,\n",
       " 103,\n",
       " 114,\n",
       " 22,\n",
       " 22,\n",
       " 95,\n",
       " 101,\n",
       " 130,\n",
       " 89,\n",
       " 128,\n",
       " 127,\n",
       " 22,\n",
       " 127,\n",
       " 97,\n",
       " 14,\n",
       " 128,\n",
       " 69,\n",
       " 1,\n",
       " 67,\n",
       " 29,\n",
       " 103,\n",
       " 125,\n",
       " 5,\n",
       " 127,\n",
       " 1,\n",
       " 38,\n",
       " 93,\n",
       " 128,\n",
       " 25,\n",
       " 22,\n",
       " 22,\n",
       " 127,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 4,\n",
       " 1,\n",
       " 112,\n",
       " 34,\n",
       " 48,\n",
       " 123,\n",
       " 22,\n",
       " 71,\n",
       " 24,\n",
       " 32,\n",
       " 30,\n",
       " 128,\n",
       " 97,\n",
       " 30,\n",
       " 107,\n",
       " 32,\n",
       " 125,\n",
       " 133,\n",
       " 114,\n",
       " 131,\n",
       " 50,\n",
       " 34,\n",
       " 30,\n",
       " 1,\n",
       " 49,\n",
       " 132,\n",
       " 35,\n",
       " 12,\n",
       " 106,\n",
       " 30,\n",
       " 58,\n",
       " 90,\n",
       " 15,\n",
       " 106,\n",
       " 32,\n",
       " 92,\n",
       " 102,\n",
       " 40,\n",
       " 1,\n",
       " 103,\n",
       " 32,\n",
       " 53,\n",
       " 70,\n",
       " 40,\n",
       " 103,\n",
       " 97,\n",
       " 119,\n",
       " 1,\n",
       " 30,\n",
       " 33,\n",
       " 106,\n",
       " 30,\n",
       " 90,\n",
       " 1,\n",
       " 97,\n",
       " 33,\n",
       " 80,\n",
       " 132,\n",
       " 21,\n",
       " 22,\n",
       " 69,\n",
       " 103,\n",
       " 97,\n",
       " 23,\n",
       " 91,\n",
       " 34,\n",
       " 30,\n",
       " 89,\n",
       " 34,\n",
       " 44,\n",
       " 33,\n",
       " 43,\n",
       " 103,\n",
       " 102,\n",
       " 37,\n",
       " 88,\n",
       " 31,\n",
       " 34,\n",
       " 30,\n",
       " 47,\n",
       " 25,\n",
       " 103,\n",
       " 4,\n",
       " 22,\n",
       " 103,\n",
       " 112,\n",
       " 53,\n",
       " 127,\n",
       " 1,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 122,\n",
       " 94,\n",
       " 43,\n",
       " 22,\n",
       " 13,\n",
       " 110,\n",
       " 1,\n",
       " 37,\n",
       " 103,\n",
       " 89,\n",
       " 128,\n",
       " 26,\n",
       " 88,\n",
       " 97,\n",
       " 30,\n",
       " 92,\n",
       " 32,\n",
       " 40,\n",
       " 53,\n",
       " 141,\n",
       " 128,\n",
       " 33,\n",
       " 30,\n",
       " 30,\n",
       " 21,\n",
       " 37,\n",
       " 34,\n",
       " 103,\n",
       " 97,\n",
       " 92,\n",
       " 80,\n",
       " 93,\n",
       " 33,\n",
       " 10,\n",
       " 41,\n",
       " 97,\n",
       " 96,\n",
       " 112,\n",
       " 22,\n",
       " 30,\n",
       " 56,\n",
       " 83,\n",
       " 25,\n",
       " 72,\n",
       " 102,\n",
       " 35,\n",
       " 63,\n",
       " 32,\n",
       " 1,\n",
       " 32,\n",
       " 37,\n",
       " 108,\n",
       " 1,\n",
       " 89,\n",
       " 91,\n",
       " 103,\n",
       " 103,\n",
       " 35,\n",
       " 128,\n",
       " 90,\n",
       " 37,\n",
       " 30,\n",
       " 19,\n",
       " 33,\n",
       " 53,\n",
       " 93,\n",
       " 106,\n",
       " 1,\n",
       " 37,\n",
       " 25,\n",
       " 128,\n",
       " 29,\n",
       " 1,\n",
       " 114,\n",
       " 88,\n",
       " 92,\n",
       " 36,\n",
       " 84,\n",
       " 97,\n",
       " 1,\n",
       " 89,\n",
       " 22,\n",
       " 1,\n",
       " 25,\n",
       " 30,\n",
       " 106,\n",
       " 1,\n",
       " 37,\n",
       " 107,\n",
       " 69,\n",
       " 55,\n",
       " 134,\n",
       " 47,\n",
       " 131,\n",
       " 29,\n",
       " 131,\n",
       " 14,\n",
       " 96,\n",
       " 128,\n",
       " 133,\n",
       " 67,\n",
       " 1,\n",
       " 30,\n",
       " 11,\n",
       " 97,\n",
       " 106,\n",
       " 22,\n",
       " 30,\n",
       " 37,\n",
       " 40,\n",
       " 90,\n",
       " 108,\n",
       " 74,\n",
       " 35,\n",
       " 54,\n",
       " 21,\n",
       " 53,\n",
       " 9,\n",
       " 34,\n",
       " 29,\n",
       " 4,\n",
       " 48,\n",
       " 4,\n",
       " 139,\n",
       " 30,\n",
       " 32,\n",
       " 1,\n",
       " 97,\n",
       " 3,\n",
       " 90,\n",
       " 103,\n",
       " 26,\n",
       " 22,\n",
       " 32,\n",
       " 93,\n",
       " 32,\n",
       " 32,\n",
       " 103,\n",
       " 25,\n",
       " 132,\n",
       " 30,\n",
       " 128,\n",
       " 90,\n",
       " 76,\n",
       " 91,\n",
       " 47,\n",
       " 30,\n",
       " 29,\n",
       " 43,\n",
       " 22,\n",
       " 113,\n",
       " 97,\n",
       " 135,\n",
       " 30,\n",
       " 53,\n",
       " 52,\n",
       " 93,\n",
       " 17,\n",
       " 37,\n",
       " 40,\n",
       " 33,\n",
       " 29,\n",
       " 70,\n",
       " 8,\n",
       " 37,\n",
       " 36,\n",
       " 90,\n",
       " 131,\n",
       " 37,\n",
       " 126,\n",
       " 88,\n",
       " 83,\n",
       " 112,\n",
       " 97,\n",
       " 37,\n",
       " 32,\n",
       " 103,\n",
       " 91,\n",
       " 128,\n",
       " 30,\n",
       " 131,\n",
       " 43,\n",
       " 22,\n",
       " 76,\n",
       " 123,\n",
       " 88,\n",
       " 1,\n",
       " 5,\n",
       " 53,\n",
       " 0,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 103,\n",
       " 45,\n",
       " 47,\n",
       " 53,\n",
       " 1,\n",
       " 8,\n",
       " 23,\n",
       " 58,\n",
       " 1,\n",
       " 8,\n",
       " 107,\n",
       " 1,\n",
       " 91,\n",
       " 32,\n",
       " 22,\n",
       " 127,\n",
       " 95,\n",
       " 114,\n",
       " 37,\n",
       " 95,\n",
       " 1,\n",
       " 90,\n",
       " 40,\n",
       " 105,\n",
       " 14,\n",
       " 17,\n",
       " 90,\n",
       " 33,\n",
       " 131,\n",
       " 61,\n",
       " 15,\n",
       " 51,\n",
       " 32,\n",
       " 42,\n",
       " 37,\n",
       " 123,\n",
       " 103,\n",
       " 16,\n",
       " 33,\n",
       " 0,\n",
       " 116,\n",
       " 22,\n",
       " 125,\n",
       " 42,\n",
       " 30,\n",
       " 88,\n",
       " 88,\n",
       " 37,\n",
       " 136,\n",
       " 20,\n",
       " 131,\n",
       " 41,\n",
       " 103,\n",
       " 30,\n",
       " 128,\n",
       " 125,\n",
       " 128,\n",
       " 26,\n",
       " 91,\n",
       " 90,\n",
       " 21,\n",
       " 103,\n",
       " 11,\n",
       " 97,\n",
       " 24,\n",
       " 37,\n",
       " 65,\n",
       " 33,\n",
       " 103,\n",
       " 103,\n",
       " 32,\n",
       " 128,\n",
       " 1,\n",
       " 90,\n",
       " 39,\n",
       " 93,\n",
       " 97,\n",
       " 92,\n",
       " 91,\n",
       " 103,\n",
       " 91,\n",
       " 53,\n",
       " 37,\n",
       " 1,\n",
       " 25,\n",
       " 24,\n",
       " 107,\n",
       " 25,\n",
       " 123,\n",
       " 47,\n",
       " 93,\n",
       " 41,\n",
       " 25,\n",
       " 0,\n",
       " 106,\n",
       " 128,\n",
       " 30,\n",
       " 26,\n",
       " 81,\n",
       " 30,\n",
       " 30,\n",
       " 24,\n",
       " 40,\n",
       " 25,\n",
       " 40,\n",
       " 17,\n",
       " 14,\n",
       " 97,\n",
       " 136,\n",
       " 69,\n",
       " 37,\n",
       " 7,\n",
       " 88,\n",
       " 84,\n",
       " 1,\n",
       " 128,\n",
       " 90,\n",
       " 128,\n",
       " 95,\n",
       " 112,\n",
       " 90,\n",
       " 29,\n",
       " 103,\n",
       " 24,\n",
       " 88,\n",
       " 136,\n",
       " 15,\n",
       " 131,\n",
       " 97,\n",
       " 83,\n",
       " 26,\n",
       " 26,\n",
       " 32,\n",
       " 97,\n",
       " 37,\n",
       " 29,\n",
       " 15,\n",
       " 95,\n",
       " 91,\n",
       " 26,\n",
       " 103,\n",
       " 93,\n",
       " 39,\n",
       " 63,\n",
       " 35,\n",
       " 30,\n",
       " 30,\n",
       " 128,\n",
       " 11,\n",
       " 125,\n",
       " 139,\n",
       " 138,\n",
       " 137,\n",
       " 106,\n",
       " 77,\n",
       " 1,\n",
       " 41,\n",
       " 33,\n",
       " 36,\n",
       " 1,\n",
       " 90,\n",
       " 30,\n",
       " 60,\n",
       " 22,\n",
       " 29,\n",
       " 30,\n",
       " 22,\n",
       " 131,\n",
       " 53,\n",
       " 22,\n",
       " 1,\n",
       " 91,\n",
       " 133,\n",
       " 30,\n",
       " 128,\n",
       " 34,\n",
       " 134,\n",
       " 114,\n",
       " 30,\n",
       " 95,\n",
       " 17,\n",
       " 130,\n",
       " 25,\n",
       " 22,\n",
       " 9,\n",
       " 26,\n",
       " 90,\n",
       " 37,\n",
       " 4,\n",
       " 107,\n",
       " 22,\n",
       " 103,\n",
       " 37,\n",
       " 25,\n",
       " 22,\n",
       " 97,\n",
       " 36,\n",
       " 42,\n",
       " 75,\n",
       " 95,\n",
       " 36,\n",
       " 112,\n",
       " 32,\n",
       " 103,\n",
       " 89,\n",
       " 103,\n",
       " 1,\n",
       " 128,\n",
       " 34,\n",
       " 5,\n",
       " 34,\n",
       " 37,\n",
       " 43,\n",
       " 128,\n",
       " 107,\n",
       " 37,\n",
       " 40,\n",
       " 26,\n",
       " 91,\n",
       " 1,\n",
       " 36,\n",
       " 91,\n",
       " 71,\n",
       " 1,\n",
       " 89,\n",
       " 1,\n",
       " 37,\n",
       " 88,\n",
       " 131,\n",
       " 69,\n",
       " 97,\n",
       " 0,\n",
       " 10,\n",
       " 83,\n",
       " 12,\n",
       " 88,\n",
       " 25,\n",
       " 95,\n",
       " 47,\n",
       " 37,\n",
       " 93,\n",
       " 93,\n",
       " 43,\n",
       " 126,\n",
       " 76,\n",
       " 32,\n",
       " 95,\n",
       " 36,\n",
       " 103,\n",
       " 132,\n",
       " 101,\n",
       " 93,\n",
       " 21,\n",
       " 37,\n",
       " 97,\n",
       " 97,\n",
       " 132,\n",
       " 91,\n",
       " 24,\n",
       " 9,\n",
       " 69,\n",
       " 128,\n",
       " 139,\n",
       " 25,\n",
       " 4,\n",
       " 116,\n",
       " 30,\n",
       " 37,\n",
       " 103,\n",
       " 23,\n",
       " 31,\n",
       " 49,\n",
       " 42,\n",
       " 141,\n",
       " 97,\n",
       " 42,\n",
       " 39,\n",
       " 37,\n",
       " 93,\n",
       " 63,\n",
       " 24,\n",
       " 32,\n",
       " 1,\n",
       " 1,\n",
       " 40,\n",
       " 20,\n",
       " 40,\n",
       " 30,\n",
       " 16,\n",
       " 25,\n",
       " 53,\n",
       " 12,\n",
       " 32,\n",
       " 37,\n",
       " 30,\n",
       " 13,\n",
       " 25,\n",
       " 103,\n",
       " 33,\n",
       " 128,\n",
       " 4,\n",
       " 22,\n",
       " 103,\n",
       " 16,\n",
       " 102,\n",
       " 22,\n",
       " 106,\n",
       " 114,\n",
       " 21,\n",
       " 96,\n",
       " 128,\n",
       " 88,\n",
       " 37,\n",
       " 132,\n",
       " 106,\n",
       " 22,\n",
       " 23,\n",
       " 93,\n",
       " 66,\n",
       " 37,\n",
       " 36,\n",
       " 90,\n",
       " 81,\n",
       " 16,\n",
       " 30,\n",
       " 34,\n",
       " 90,\n",
       " 23,\n",
       " 15,\n",
       " 119,\n",
       " 30,\n",
       " 31,\n",
       " 34,\n",
       " 32,\n",
       " 0,\n",
       " 97,\n",
       " 90,\n",
       " 9,\n",
       " 93,\n",
       " 8,\n",
       " 1,\n",
       " 128,\n",
       " 97,\n",
       " 36,\n",
       " 8,\n",
       " 37,\n",
       " 128,\n",
       " 128,\n",
       " 67,\n",
       " 11,\n",
       " 128,\n",
       " 105,\n",
       " 88,\n",
       " 55,\n",
       " 38,\n",
       " 37,\n",
       " 93,\n",
       " 30,\n",
       " 33,\n",
       " 103,\n",
       " 32,\n",
       " 12,\n",
       " 6,\n",
       " 131,\n",
       " 53,\n",
       " 88,\n",
       " 128,\n",
       " 125,\n",
       " 1,\n",
       " 0,\n",
       " 31,\n",
       " 21,\n",
       " 4,\n",
       " 59,\n",
       " 17,\n",
       " 1,\n",
       " 30,\n",
       " 1,\n",
       " 90,\n",
       " 30,\n",
       " 89,\n",
       " 135,\n",
       " 88,\n",
       " 106,\n",
       " 26,\n",
       " 93,\n",
       " 125,\n",
       " 23,\n",
       " 8,\n",
       " 34,\n",
       " 131,\n",
       " 1,\n",
       " 1,\n",
       " 125,\n",
       " 26,\n",
       " 83,\n",
       " 41,\n",
       " 1,\n",
       " 127,\n",
       " 103,\n",
       " 97,\n",
       " 42,\n",
       " 127,\n",
       " 91,\n",
       " 114,\n",
       " 97,\n",
       " 30,\n",
       " 25,\n",
       " 34,\n",
       " 48,\n",
       " 77,\n",
       " 1,\n",
       " 25,\n",
       " 103,\n",
       " ...]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 1,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 1,\n",
       " 30,\n",
       " 1,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 1,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 1,\n",
       " 30,\n",
       " 1,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 1,\n",
       " 1,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 1,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 1,\n",
       " 1,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 1,\n",
       " 30,\n",
       " 1,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 1,\n",
       " 1,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 1,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 1,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 1,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 1,\n",
       " 1,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 1,\n",
       " 30,\n",
       " 1,\n",
       " 1,\n",
       " 30,\n",
       " 1,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 22,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 1,\n",
       " 37,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 1,\n",
       " 1,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 1,\n",
       " 1,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 1,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 1,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 1,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " ...]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to find data adapter that can handle input: <class 'function'>, <class 'NoneType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-81e796e74545>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrained_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_input_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.1/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    928\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    931\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m   def predict(self,\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.1/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, model, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m         use_multiprocessing=use_multiprocessing, **kwargs)\n\u001b[0m\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m   def predict(self, model, x, batch_size=None, verbose=0, steps=None,\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.1/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_model_iteration\u001b[0;34m(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    424\u001b[0m           \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m           use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    427\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m       \u001b[0muse_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_samples\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.1/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[0;34m(model, mode, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    634\u001b[0m                     use_multiprocessing=False):\n\u001b[1;32m    635\u001b[0m   \u001b[0;34m\"\"\"Process the inputs for fit/eval/predict().\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 636\u001b[0;31m   \u001b[0madapter_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_adapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_data_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    637\u001b[0m   standardize = functools.partial(\n\u001b[1;32m    638\u001b[0m       \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.1/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mselect_data_adapter\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    996\u001b[0m         \u001b[0;34m\"Failed to find data adapter that can handle \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    997\u001b[0m         \"input: {}, {}\".format(\n\u001b[0;32m--> 998\u001b[0;31m             _type_name(x), _type_name(y)))\n\u001b[0m\u001b[1;32m    999\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter_cls\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1000\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to find data adapter that can handle input: <class 'function'>, <class 'NoneType'>"
     ]
    }
   ],
   "source": [
    "trained_model.evaluate(eval_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_fn():\n",
    "    return tf.keras.metrics.SparseCategoricalAccuracy('test_accuracy',dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = metric_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(eval_input_fn,model):\n",
    "    eval_iter = iter(strategy.experimental_distribute_datasets_from_function(eval_input_fn))\n",
    "    \n",
    "    def _test_step_fn(inputs,label):\n",
    "#         inputs,label = inputs\n",
    "        model_outputs = model(inputs,training=False)\n",
    "        metric.update_state(label,model_outputs)\n",
    "    strategy.experimental_run_v2(_test_step_fn,args=(next(eval_iter)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(eval_input_fn,trained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.125>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_export_dir = '/var/extra/users/jgeorge/tf2.0/input/albert_base_custom/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spm_model = os.path.join(model_dir, \"assets\", \"30k-clean.model\")\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(spm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model.sp_model_file = tf.saved_model.Asset(spm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.tracking.Asset at 0x7f54e0bc8310>"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model.sp_model_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/extra/users/jgeorge/tf2.0/input/albert_base_custom/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/extra/users/jgeorge/tf2.0/input/albert_base_custom/assets\n"
     ]
    }
   ],
   "source": [
    "trained_model.save(model_export_dir,include_optimizer=False,save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.125>"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown signature default in https://tfhub.dev/google/albert_base/2 (available signatures: _SignatureMap({'tokenization_info': <tensorflow.python.eager.wrap_function.WrappedFunction object at 0x7f550be41450>, 'mlm': <tensorflow.python.eager.wrap_function.WrappedFunction object at 0x7f57ec038a50>, 'tokens': <tensorflow.python.eager.wrap_function.WrappedFunction object at 0x7f55086cf990>})).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-345-a847214142b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbert_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKerasLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malbert_hub_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.1/lib/python3.7/site-packages/tensorflow_hub/keras_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, handle, trainable, arguments, _sentinel, tags, signature, signature_outputs_as_dict, output_key, output_shape, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m                        \"a signature (or using a legacy Hub.Module).\")\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.1/lib/python3.7/site-packages/tensorflow_hub/keras_layer.py\u001b[0m in \u001b[0;36m_get_callable\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_signature\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignatures\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m       raise ValueError(\"Unknown signature %s in %s (available signatures: %s).\"\n\u001b[0;32m--> 250\u001b[0;31m                        % (self._signature, self._handle, self._func.signatures))\n\u001b[0m\u001b[1;32m    251\u001b[0m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_signature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown signature default in https://tfhub.dev/google/albert_base/2 (available signatures: _SignatureMap({'tokenization_info': <tensorflow.python.eager.wrap_function.WrappedFunction object at 0x7f550be41450>, 'mlm': <tensorflow.python.eager.wrap_function.WrappedFunction object at 0x7f57ec038a50>, 'tokens': <tensorflow.python.eager.wrap_function.WrappedFunction object at 0x7f55086cf990>}))."
     ]
    }
   ],
   "source": [
    "bert_model = hub.KerasLayer(albert_hub_url,trainable=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = hub.KerasLayer(albert_hub_url,trainable=True,signature='tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "in converted code:\n\n    /opt/custom/python/anaconda3/envs/tf2.1/lib/python3.7/site-packages/tensorflow_hub/keras_layer.py:209 call  *\n        result = f()\n    /opt/custom/python/anaconda3/envs/tf2.1/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py:1551 __call__  *\n        return self._call_impl(args, kwargs)\n    /opt/custom/python/anaconda3/envs/tf2.1/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py:1570 _call_impl\n        ).format(self._num_positional_args, self._arg_keywords, args))\n\n    TypeError: Expected at most 0 positional arguments (and the rest keywords, of ['segment_ids', 'input_ids', 'input_mask']), got ([<tf.Tensor 'segment_ids:0' shape=(None, 128) dtype=int32>, <tf.Tensor 'input_ids:0' shape=(None, 128) dtype=int32>, <tf.Tensor 'input_mask:0' shape=(None, 128) dtype=int32>],). When calling a concrete function, positional arguments may not be bound to Tensors within nested structures.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-126-49c96daf56c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m       \u001b[0meval_input_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m       \u001b[0mrun_eagerly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m       use_keras_compile_fit=False)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-123-6373bbe5532a>\u001b[0m in \u001b[0;36mrun_bert_classifier\u001b[0;34m(strategy, bert_config, input_meta_data, model_dir, epochs, steps_per_epoch, steps_per_loop, eval_steps, warmup_steps, initial_lr, init_checkpoint, train_input_fn, eval_input_fn, custom_callbacks, run_eagerly, use_keras_compile_fit)\u001b[0m\n\u001b[1;32m     23\u001b[0m                                                               \u001b[0mnum_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                                                               \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                                                                hub_module_url=albert_hub_url)\n\u001b[0m\u001b[1;32m     26\u001b[0m     classifier_model.optimizer = optimization.create_optimizer(init_lr=initial_lr,\n\u001b[1;32m     27\u001b[0m                                                                \u001b[0mnum_train_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-121-fef5ea442f65>\u001b[0m in \u001b[0;36mget_classifier_model\u001b[0;34m(bert_config, float_type, num_labels, max_seq_length, final_layer_initializer, hub_module_url)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mbert_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKerasLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malbert_hub_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tokens'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msignature_outputs_as_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m#     pooled_output,_ = bert_model([input_word_ids,input_mask,input_type_ids])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mpooled_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msegment_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_dropout_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpooled_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkernel_initializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'output'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.1/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    771\u001b[0m                     not base_layer_utils.is_in_eager_or_tf_function()):\n\u001b[1;32m    772\u001b[0m                   \u001b[0;32mwith\u001b[0m \u001b[0mauto_control_deps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAutomaticControlDependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0macd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 773\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    774\u001b[0m                     \u001b[0;31m# Wrap Tensors in `outputs` in `tf.identity` to avoid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m                     \u001b[0;31m# circular dependencies.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.1/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    235\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: in converted code:\n\n    /opt/custom/python/anaconda3/envs/tf2.1/lib/python3.7/site-packages/tensorflow_hub/keras_layer.py:209 call  *\n        result = f()\n    /opt/custom/python/anaconda3/envs/tf2.1/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py:1551 __call__  *\n        return self._call_impl(args, kwargs)\n    /opt/custom/python/anaconda3/envs/tf2.1/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py:1570 _call_impl\n        ).format(self._num_positional_args, self._arg_keywords, args))\n\n    TypeError: Expected at most 0 positional arguments (and the rest keywords, of ['segment_ids', 'input_ids', 'input_mask']), got ([<tf.Tensor 'segment_ids:0' shape=(None, 128) dtype=int32>, <tf.Tensor 'input_ids:0' shape=(None, 128) dtype=int32>, <tf.Tensor 'input_mask:0' shape=(None, 128) dtype=int32>],). When calling a concrete function, positional arguments may not be bound to Tensors within nested structures.\n"
     ]
    }
   ],
   "source": [
    "trained_model = run_bert_classifier(\n",
    "      strategy,\n",
    "      bert_config,\n",
    "      input_meta_data,\n",
    "      albert_model_dir,\n",
    "      epochs,\n",
    "      steps_per_epoch,\n",
    "      steps_per_loop,\n",
    "      eval_steps,\n",
    "      warmup_steps,\n",
    "      learning_rate,\n",
    "      init_checkpoint,\n",
    "      train_input_fn,\n",
    "      eval_input_fn,\n",
    "      run_eagerly=False,\n",
    "      use_keras_compile_fit=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = set()\n",
    "tags.add('train')\n",
    "\n",
    "loaded_albert_model = tf.saved_model.load(albert_model_dir,tags=tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_SignatureMap({'mlm': <tensorflow.python.eager.wrap_function.WrappedFunction object at 0x7f551de1d9d0>, 'tokens': <tensorflow.python.eager.wrap_function.WrappedFunction object at 0x7f551d98e090>, 'tokenization_info': <tensorflow.python.eager.wrap_function.WrappedFunction object at 0x7f551d5d5050>})"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_albert_model.signatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.eager.wrap_function.WrappedFunction at 0x7f551de1d9d0>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_albert_model.signatures['mlm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 - tensorflow 2.2 (tf2.2)",
   "language": "python",
   "name": "tf2.2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
