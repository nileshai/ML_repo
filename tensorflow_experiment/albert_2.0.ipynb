{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "# import bert\n",
    "# from bert import BertModelLayer\n",
    "\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import sentencepiece as spm\n",
    "import tf_sentencepiece as tfs\n",
    "import sys\n",
    "sys.path.extend([\"/var/extra/users/jgeorge/tf2.0/git/models/\"])\n",
    "\n",
    "\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import csv\n",
    "import collections\n",
    "import pandas as pd\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from absl import logging\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# pylint: disable=g-import-not-at-top,redefined-outer-name,reimported\n",
    "from official.modeling import model_training_utils\n",
    "from official.nlp import bert_modeling as modeling\n",
    "from official.nlp import bert_models\n",
    "from official.nlp import optimization\n",
    "from official.nlp.bert import common_flags\n",
    "from official.nlp.bert import input_pipeline\n",
    "from official.nlp.bert import model_saving_utils\n",
    "from official.utils.misc import distribution_utils\n",
    "from official.utils.misc import keras_utils\n",
    "from official.nlp.bert import tokenization\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.engine import network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:4', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:5', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:6', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:7', device_type='GPU')]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.experimental.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "##restricting no of gpus\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "device_to_use = gpus[2]\n",
    "tf.config.experimental.set_memory_growth(device_to_use,True)\n",
    "tf.config.experimental.set_visible_devices(device_to_use, 'GPU')\n",
    "print(tf.config.experimental.get_visible_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "dish_data_path='/var/extra/users/jgeorge/tf2.0/input/dish/data/jan17_2020/dishDataJan17.txt'\n",
    "df = pd.read_csv(dish_data_path,sep='\\t',header=None,names=['filename','text','granular_intent','ru_intent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>text</th>\n",
       "      <th>granular_intent</th>\n",
       "      <th>ru_intent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INT-sv1appis14-1504137880316-305603_4567</td>\n",
       "      <td>can you send my bill to my mail?</td>\n",
       "      <td>billing-preferences</td>\n",
       "      <td>billing-preferences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>INT-va1appis15-1504373018548-41332</td>\n",
       "      <td>My Wally receiver has lost Satellite signal in...</td>\n",
       "      <td>comp_part_signal_loss-issue</td>\n",
       "      <td>comp_part_signal_loss-issue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INT-sv1appis12-1503954587819-263368</td>\n",
       "      <td>I need a payment extension so i don't get my s...</td>\n",
       "      <td>payment_extension-request</td>\n",
       "      <td>payment_extension-request</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8ed8e9b1-10f5-438d-c409-e616c3ff9ede</td>\n",
       "      <td>how can i find my local channels. it seems i d...</td>\n",
       "      <td>channel_package-issue</td>\n",
       "      <td>channel_package-issue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>INT-sv1appis13-1504099917638-293735</td>\n",
       "      <td>Wanted to speak with someone about my bill</td>\n",
       "      <td>representative-request</td>\n",
       "      <td>representative-request</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   filename  \\\n",
       "0  INT-sv1appis14-1504137880316-305603_4567   \n",
       "1        INT-va1appis15-1504373018548-41332   \n",
       "2       INT-sv1appis12-1503954587819-263368   \n",
       "3      8ed8e9b1-10f5-438d-c409-e616c3ff9ede   \n",
       "4       INT-sv1appis13-1504099917638-293735   \n",
       "\n",
       "                                                text  \\\n",
       "0                   can you send my bill to my mail?   \n",
       "1  My Wally receiver has lost Satellite signal in...   \n",
       "2  I need a payment extension so i don't get my s...   \n",
       "3  how can i find my local channels. it seems i d...   \n",
       "4         Wanted to speak with someone about my bill   \n",
       "\n",
       "               granular_intent                    ru_intent  \n",
       "0          billing-preferences          billing-preferences  \n",
       "1  comp_part_signal_loss-issue  comp_part_signal_loss-issue  \n",
       "2    payment_extension-request    payment_extension-request  \n",
       "3        channel_package-issue        channel_package-issue  \n",
       "4       representative-request       representative-request  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train,df_val,y_train,y_val = train_test_split(df,df['granular_intent'],train_size=0.8,random_state=42,stratify=df['granular_intent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_exp_folder = '/space/users/jgeorge/projects/k/tensorflow2-question-answering/input/dish/data/jan17_2020/'\n",
    "main_exp_folder = '/var/extra/users/jgeorge/tf2.0/input/dish/data/jan17_2020/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_csv_file='/var/extra/users/jgeorge/tf2.0/input/dish/data/jan17_2020/train.txt'\n",
    "eval_csv_file='/var/extra/users/jgeorge/tf2.0/input/dish/data/jan17_2020/val.txt'\n",
    "\n",
    "df_train.to_csv(train_csv_file,sep='\\t',header=False,index=False)\n",
    "df_val.to_csv(eval_csv_file,sep='\\t',header=False,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_file = '/var/extra/users/jgeorge/tf2.0/input/dish/data/jan17_2020/intentlist.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(intent_file,'w',encoding='utf-8') as out_f:\n",
    "    for intent in sorted(df_train['granular_intent'].unique()):\n",
    "        out_f.write(intent+'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_list = []\n",
    "with open(intent_file,'r') as inp_f:\n",
    "    for intent in inp_f:\n",
    "        intent_list.append(intent.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bytes_feature(value):\n",
    "  \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "  if isinstance(value, type(tf.constant(0))):\n",
    "    print(\"type constant \",type(tf.constant(0)))\n",
    "    value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
    "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _int64_feature_list(values):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path=os.path.join(main_exp_folder,'train.tfrecords')\n",
    "eval_data_path=os.path.join(main_exp_folder,'eval.tfrecords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "    def __init__(self,text,label):\n",
    "        self.text = text\n",
    "        self.label = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeatures(object):\n",
    "    def __init__(self,input_ids,input_mask,\n",
    "                segment_ids,label_id):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_single_example(ex_index, example, label_list, max_seq_length,sentence_piece):\n",
    "    CLS_ID = 2\n",
    "    SEP_ID = 3\n",
    "    # -2 for [CLS], [SEP]\n",
    "    max_word_ids = max_seq_length # -2\n",
    "    input_ids = []\n",
    "    segment_ids = []\n",
    "    input_mask = []\n",
    "#     input_ids.append(CLS_ID)\n",
    "    word_ids = sentence_piece.encode_as_ids(example.text)\n",
    "    word_ids = word_ids[:max_word_ids]\n",
    "    input_ids.extend(word_ids)\n",
    "#     input_ids.append(SEP_ID)\n",
    "    segment_ids = [0]*len(input_ids)\n",
    "    input_mask = [1]*len(input_ids)\n",
    "    if len(input_ids)<max_seq_length:\n",
    "        diff = max_seq_length - len(input_ids)\n",
    "        input_ids.extend([0]*diff)\n",
    "        segment_ids.extend([0]*diff)\n",
    "        input_mask.extend([0]*diff)\n",
    "    assert(len(input_ids)==max_seq_length)\n",
    "    assert(len(segment_ids)==max_seq_length)\n",
    "    assert(len(input_mask)==max_seq_length)\n",
    "    label_map = {label:i for i,label in enumerate(label_list)}\n",
    "    label_id = label_map[example.label]\n",
    "    feature = InputFeatures(input_ids,input_mask,segment_ids,label_id)\n",
    "    return feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 128\n",
    "# max_seq_length = bert_config.max_position_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "albert_model_dir = '/var/extra/users/jgeorge/tf2.0/input/albert_base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = albert_model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "albert_config='/var/extra/users/jgeorge/tf2.0/input/albert_base/assets/albert_config.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_config = modeling.AlbertConfig.from_json_file(FLAGS.bert_config_file)\n",
    "\n",
    "bert_config = modeling.AlbertConfig.from_json_file(albert_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vocab_size': 30000,\n",
       " 'hidden_size': 768,\n",
       " 'num_hidden_layers': 12,\n",
       " 'num_attention_heads': 12,\n",
       " 'hidden_act': 'gelu',\n",
       " 'intermediate_size': 3072,\n",
       " 'hidden_dropout_prob': 0,\n",
       " 'attention_probs_dropout_prob': 0,\n",
       " 'max_position_embeddings': 512,\n",
       " 'type_vocab_size': 2,\n",
       " 'initializer_range': 0.02,\n",
       " 'backward_compatible': True,\n",
       " 'embedding_size': 128,\n",
       " 'num_hidden_groups': 1,\n",
       " 'net_structure_type': 0,\n",
       " 'gap_size': 0,\n",
       " 'num_memory_blocks': 0,\n",
       " 'inner_group_num': 1,\n",
       " 'down_scale_factor': 1}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_config.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_training_data(input_csv_file,output_file,delimiter='\\t',max_seq_length=128):\n",
    "    spm_model = os.path.join(model_dir, \"assets\", \"30k-clean.model\")\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.load(spm_model)\n",
    "   \n",
    "    with open(input_csv_file,'r',encoding='utf-8') as csv_file, tf.io.TFRecordWriter(output_file) as tf_record_writer:\n",
    "        #tried using binary format for bytes_list, but csv_reader requires text format\n",
    "#     with open(input_csv_file,'rb') as csv_file, tf.io.TFRecordWriter(output_file) as tf_record_writer:\n",
    "        csv_reader = csv.reader(csv_file,delimiter=delimiter,quotechar='\"')\n",
    "        for i,cols in enumerate(csv_reader):\n",
    "            ##skipping filename & granular tag (granular tag & final tag are the same here)\n",
    "#             yield cols[1:-1]\n",
    "            features = collections.OrderedDict()\n",
    "            \n",
    "            filename = cols[0]\n",
    "            text = cols[1]\n",
    "            if(len(cols)<4):\n",
    "                print('filename ',filename)\n",
    "            intent = cols[2]\n",
    "            input_example = InputExample(text,intent)\n",
    "            feature = convert_single_example(ex_index=i,example=input_example,label_list=intent_list,\n",
    "                                             max_seq_length=max_seq_length,sentence_piece=sp)\n",
    "#             text_encoded = sp.encode_as_ids(text)\n",
    "#             features['filename'] = _bytes_feature(filename.encode())\n",
    "#             features['text'] = _int64_feature_list(text_encoded)\n",
    "#             features['intent']  = _bytes_feature(intent.encode())\n",
    "\n",
    "            features[\"input_ids\"] = _int64_feature_list(feature.input_ids)\n",
    "            features[\"input_mask\"] = _int64_feature_list(feature.input_mask)\n",
    "            features[\"segment_ids\"] = _int64_feature_list(feature.segment_ids)\n",
    "            features[\"label_id\"] = _int64_feature(feature.label_id)\n",
    "#             features[\"is_real_example\"]\n",
    "            tf_example = tf.train.Example(features=tf.train.Features(feature=features))\n",
    "            tf_record_writer.write(tf_example.SerializeToString())\n",
    "\n",
    "#     tf_record_writer.close()\n",
    "write_training_data(train_csv_file,train_data_path,max_seq_length=max_seq_length)\n",
    "write_training_data(eval_csv_file,eval_data_path,max_seq_length=max_seq_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_ds = tf.data.TFRecordDataset(train_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _decode_data(record):\n",
    "#     features = {\n",
    "#         'filename':tf.io.FixedLenFeature([],tf.string),\n",
    "#         'text':tf.io.VarLenFeature(tf.int64),\n",
    "#         'intent':tf.io.FixedLenFeature([],tf.string)\n",
    "#     }\n",
    "    features = {\n",
    "        \"input_ids\":tf.io.FixedLenFeature([max_seq_length],tf.int64),\n",
    "        \"input_mask\":tf.io.FixedLenFeature([max_seq_length],tf.int64),\n",
    "        \"segment_ids\":tf.io.FixedLenFeature([max_seq_length],tf.int64),\n",
    "        \"label_id\":tf.io.FixedLenFeature([],tf.int64)\n",
    "    }\n",
    "    return tf.io.parse_single_example(record,features=features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = raw_ds.map(_decode_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': <tf.Tensor: shape=(128,), dtype=int64, numpy=\n",
      "array([  483,   144,    51,  1071,  1839,    34, 14737,    37,   236,\n",
      "         818,  1071,    60,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0])>, 'input_mask': <tf.Tensor: shape=(128,), dtype=int64, numpy=\n",
      "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])>, 'label_id': <tf.Tensor: shape=(), dtype=int64, numpy=103>, 'segment_ids': <tf.Tensor: shape=(128,), dtype=int64, numpy=\n",
      "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])>}\n",
      "{'input_ids': <tf.Tensor: shape=(128,), dtype=int64, numpy=\n",
      "array([  13,    1,  259,   20, 3547,   14, 1889, 2440, 3607,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0])>, 'input_mask': <tf.Tensor: shape=(128,), dtype=int64, numpy=\n",
      "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])>, 'label_id': <tf.Tensor: shape=(), dtype=int64, numpy=34>, 'segment_ids': <tf.Tensor: shape=(128,), dtype=int64, numpy=\n",
      "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])>}\n"
     ]
    }
   ],
   "source": [
    "for line in processed_data.take(2):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "albert_hub_url = \"https://tfhub.dev/google/albert_base/2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TFHUB_CACHE_DIR\"] = '/var/extra/engineering/tfhub_modules'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strategy = distribution_utils.get_distribution_strategy('mirrored',num_gpus=1)\n",
    "\n",
    "# strategy = tf.distribute.OneDeviceStrategy(\"device:GPU:2\")\n",
    "#since devices to use is set to 2 already, only 1 device is visible which is 0\n",
    "strategy = tf.distribute.OneDeviceStrategy(\"device:GPU:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_fn(num_classes, loss_factor=1.0):\n",
    "  \"\"\"Gets the classification loss function.\"\"\"\n",
    "\n",
    "  def classification_loss_fn(labels, logits):\n",
    "    \"\"\"Classification loss.\"\"\"\n",
    "    labels = tf.squeeze(labels)\n",
    "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "    one_hot_labels = tf.one_hot(\n",
    "        tf.cast(labels, dtype=tf.int32), depth=num_classes, dtype=tf.float32)\n",
    "    per_example_loss = -tf.reduce_sum(\n",
    "        tf.cast(one_hot_labels, dtype=tf.float32) * log_probs, axis=-1)\n",
    "    loss = tf.reduce_mean(per_example_loss)\n",
    "    loss *= loss_factor\n",
    "    return loss\n",
    "  return classification_loss_fn  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "albert_hub_url3='https://tfhub.dev/google/albert_base/3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://tfhub.dev/google/albert_base/2\n"
     ]
    }
   ],
   "source": [
    "print(albert_hub_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/custom/python/anaconda3/envs/tf2.1/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/custom/python/anaconda3/envs/tf2.1/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "tags = set()\n",
    "tags.add('train')\n",
    "bert_module = hub.load(albert_hub_url3,tags=tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.eager.wrap_function.WrappedFunction at 0x7f1e04315510>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_module.signatures['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "def get_model_part1(bert_config,float_type,num_labels,max_seq_length,final_layer_initializer=None,hub_module_url=None):\n",
    "#     if final_layer_initializer is not None:  \n",
    "#         initializer = final_layer_initializer\n",
    "#     else:\n",
    "#         initializer = tf.keras.initializers.TruncatedNormal(stddev=bert_config.initializer_range)\n",
    "\n",
    "    if not hub_module_url:\n",
    "        #TODO\n",
    "        print(\"create the model\")\n",
    "        return None\n",
    "#     input_word_ids = tf.keras.Input(shape=(max_seq_length),dtype=tf.int32,name='input_word_ids')\n",
    "    input_ids = tf.keras.Input(shape=(max_seq_length),dtype=tf.int32,name='input_ids')\n",
    "    input_mask = tf.keras.Input(shape=(max_seq_length),dtype=tf.int32,name='input_mask')\n",
    "#     input_type_ids = tf.keras.Input(shape=(max_seq_length),dtype=tf.int32,name='input_type_ids')\n",
    "    segment_ids = tf.keras.Input(shape=(max_seq_length),dtype=tf.int32,name='segment_ids')\n",
    "    return input_ids,input_mask,segment_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_model_bert(bert_config,float_type,num_labels,max_seq_length,final_layer_initializer,hub_module_url,\n",
    "                   input_ids,input_mask,segment_ids):\n",
    "    tags = set()\n",
    "    tags.add('train')\n",
    "#     bert_module = hub.Module(hub_module_url,trainable=True)\n",
    "    bert_module = hub.load(hub_module_url,tags=tags)\n",
    "    bert_inputs = {'input_ids':input_ids,\n",
    "                   'input_mask':input_mask,\n",
    "                   'segment_ids':segment_ids}\n",
    "    # https://www.tensorflow.org/hub/common_issues\n",
    "#     bert_outputs = bert_module.signatures['tokens'](\n",
    "#       inputs=bert_inputs,\n",
    "#       signature=\"tokens\",\n",
    "#       as_dict=True)\n",
    "\n",
    "#     bert_outputs = bert_module.signatures['tokens'](bert_inputs) #bert_outputs\n",
    "#     bert_outputs = bert_module.signatures['tokens'](input_ids=input_ids,input_mask=input_mask,segment_ids=segment_ids)\n",
    "#     print(bert_outputs)\n",
    "    \n",
    "#     bert_model = hub.KerasLayer(albert_hub_url,trainable=True,tags=tags,signature='tokens',signature_outputs_as_dict=True)\n",
    "    \n",
    "    ### pooled_output will give the representation for [CLS]\n",
    "    ### sequence_output will give representations for all tokens\n",
    "    bert_model = hub.KerasLayer(albert_hub_url,trainable=True,tags=tags,signature='tokens',output_key='pooled_output')\n",
    "    print(' bert_model ',bert_model)\n",
    "#     pooled_output,_ = bert_model([input_word_ids,input_mask,input_type_ids]) \n",
    "#     pooled_output,_ = bert_model(input_ids=input_ids,input_mask=input_mask,segment_ids=segment_ids) \n",
    "#     pooled_output,_ = bert_model(inputs = [input_ids,input_mask,segment_ids]) \n",
    "#     pooled_output,temp = bert_model(inputs = {'input_ids':input_ids,'input_mask':input_mask,'segment_ids':segment_ids})\n",
    "    pooled_output = bert_model(inputs = {'input_ids':input_ids,'input_mask':input_mask,'segment_ids':segment_ids})\n",
    "#     output = tf.keras.layers.Dropout(rate = bert_config.hidden_dropout_prob)(pooled_output)\n",
    "#     pooled_output = bert_outputs['pooled_output']\n",
    "    output = pooled_output\n",
    "#     print('pooled_output ',pooled_output,' temp '+temp)\n",
    "    print('pooled_output ',pooled_output)\n",
    "#     output = bert_outputs['pooled_output']\n",
    "    return output,bert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "def get_model_part3(bert_output,num_labels,final_layer_initializer):\n",
    "    if final_layer_initializer is not None:  \n",
    "        initializer = final_layer_initializer\n",
    "    else:\n",
    "        initializer = tf.keras.initializers.TruncatedNormal(stddev=bert_config.initializer_range)\n",
    "    #     output = tf.keras.layers.Dropout(rate = bert_config.hidden_dropout_prob)(pooled_output)\n",
    "    output = tf.keras.layers.Dense(num_labels,kernel_initializer=initializer,name='output',dtype=tf.float32)(bert_output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def combine_model(bert_config,float_type,num_labels,max_seq_length,final_layer_initializer=None,hub_module_url=None):\n",
    "    input_ids,input_mask,segment_ids = get_model_part1(bert_config,float_type,num_labels,max_seq_length,final_layer_initializer,hub_module_url)\n",
    "    bert_output,bert_model = get_model_bert(bert_config,float_type,num_labels,max_seq_length,final_layer_initializer,hub_module_url,input_ids,input_mask,segment_ids)\n",
    "    print('bert_output',bert_output)\n",
    "    output = get_model_part3(bert_output,num_labels,final_layer_initializer)\n",
    "    return tf.keras.Model(inputs={'input_ids':input_ids,\n",
    "                                 'input_mask':input_mask,\n",
    "                                 'segment_ids':segment_ids},\n",
    "                         outputs=output),bert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "def get_classifier_model(bert_config,float_type,num_labels,max_seq_length,final_layer_initializer=None,hub_module_url=None):\n",
    "    if final_layer_initializer is not None:  \n",
    "        initializer = final_layer_initializer\n",
    "    else:\n",
    "        initializer = tf.keras.initializers.TruncatedNormal(stddev=bert_config.initializer_range)\n",
    "\n",
    "    if not hub_module_url:\n",
    "        #TODO\n",
    "        print(\"create the model\")\n",
    "        return None\n",
    "#     input_word_ids = tf.keras.Input(shape=(max_seq_length),dtype=tf.int32,name='input_word_ids')\n",
    "    input_ids = tf.keras.Input(shape=(max_seq_length),dtype=tf.int32,name='input_ids')\n",
    "    input_mask = tf.keras.Input(shape=(max_seq_length),dtype=tf.int32,name='input_mask')\n",
    "#     input_type_ids = tf.keras.Input(shape=(max_seq_length),dtype=tf.int32,name='input_type_ids')\n",
    "    segment_ids = tf.keras.Input(shape=(max_seq_length),dtype=tf.int32,name='segment_ids')\n",
    "    tags = set()\n",
    "    tags.add('train')\n",
    "#     bert_module = hub.Module(hub_module_url,trainable=True)\n",
    "    bert_module = hub.load(hub_module_url,tags=tags)\n",
    "    bert_inputs = {'input_ids':input_ids,\n",
    "                   'input_mask':input_mask,\n",
    "                   'segment_ids':segment_ids}\n",
    "    # https://www.tensorflow.org/hub/common_issues\n",
    "#     bert_outputs = bert_module.signatures['tokens'](\n",
    "#       inputs=bert_inputs,\n",
    "#       signature=\"tokens\",\n",
    "#       as_dict=True)\n",
    "\n",
    "#     bert_outputs = bert_module.signatures['tokens'](\n",
    "#       bert_inputs)\n",
    "    bert_outputs = bert_module.signatures['tokens'](input_ids=input_ids,input_mask=input_mask,segment_ids=segment_ids)\n",
    "    print(bert_outputs)\n",
    "    \n",
    "#     bert_model = hub.KerasLayer(albert_hub_url,trainable=True,tags=tags,signature='tokens',signature_outputs_as_dict=True)\n",
    "#     pooled_output,_ = bert_model([input_word_ids,input_mask,input_type_ids]) \n",
    "#     pooled_output,_ = bert_model([segment_ids,input_ids,input_mask]) \n",
    "#     output = tf.keras.layers.Dropout(rate = bert_config.hidden_dropout_prob)(pooled_output)\n",
    "#     pooled_output = bert_outputs['pooled_output']\n",
    "    output = bert_outputs['pooled_output']\n",
    "#     output = tf.keras.layers.Dropout(rate = bert_config.hidden_dropout_prob)(pooled_output)\n",
    "    output = tf.keras.layers.Dense(num_labels,kernel_initializer=initializer,name='output',dtype=tf.float32)(output)\n",
    "    return tf.keras.Model(inputs={'input_ids':input_ids,\n",
    "                                 'input_mask':input_mask,\n",
    "                                 'segment_ids':segment_ids},\n",
    "                         outputs=output),bert_model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clear the existing tensorflow graph\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_bert_classifier(strategy,\n",
    "                        bert_config,\n",
    "                        input_meta_data,\n",
    "                        model_dir,\n",
    "                        epochs,\n",
    "                        steps_per_epoch,\n",
    "                        steps_per_loop,\n",
    "                        eval_steps,\n",
    "                        warmup_steps,\n",
    "                        initial_lr,\n",
    "                        init_checkpoint,\n",
    "                        train_input_fn,\n",
    "                        eval_input_fn,\n",
    "                        custom_callbacks=None,\n",
    "                        run_eagerly=False,\n",
    "                        use_keras_compile_fit=False):\n",
    "    \"\"\"Run BERT classifier training using low-level API.\"\"\"\n",
    "    max_seq_length = input_meta_data['max_seq_length']\n",
    "    num_classes = input_meta_data['num_labels']\n",
    "    def _get_classifier_model():\n",
    "#     bert_models.classifier_model\n",
    "#     classifier_model,core_model = get_classifier_model(bert_config=bert_config,\n",
    "#                                                                float_type=tf.float32,\n",
    "#                                                               num_labels=num_classes,\n",
    "#                                                               max_seq_length=max_seq_length,\n",
    "#                                                                hub_module_url=albert_hub_url)\n",
    "        classifier_model,core_model = combine_model(bert_config=bert_config,\n",
    "                                                                   float_type=tf.float32,\n",
    "                                                                  num_labels=num_classes,\n",
    "                                                                  max_seq_length=max_seq_length,\n",
    "                                                                   hub_module_url=albert_hub_url)\n",
    "        classifier_model.optimizer = optimization.create_optimizer(init_lr=initial_lr,\n",
    "                                                                   num_train_steps=steps_per_epoch*epochs,\n",
    "                                                                   num_warmup_steps=warmup_steps)\n",
    "        return classifier_model,core_model\n",
    "    loss_multiplier = 1\n",
    "    loss_fn = get_loss_fn(num_classes,loss_multiplier)\n",
    "    \n",
    "    def metric_fn():\n",
    "        return tf.keras.metrics.SparseCategoricalAccuracy('test_accuracy',dtype=tf.float32)\n",
    "    return model_training_utils.run_customized_training_loop(\n",
    "      strategy=strategy,\n",
    "      model_fn=_get_classifier_model,\n",
    "      loss_fn=loss_fn,\n",
    "      model_dir=model_dir,\n",
    "      steps_per_epoch=steps_per_epoch,\n",
    "      steps_per_loop=steps_per_loop,\n",
    "      epochs=epochs,\n",
    "      train_input_fn=train_input_fn,\n",
    "      eval_input_fn=eval_input_fn,\n",
    "      eval_steps=eval_steps,\n",
    "      init_checkpoint=init_checkpoint,\n",
    "      metric_fn=metric_fn,\n",
    "      custom_callbacks=custom_callbacks,\n",
    "      run_eagerly=run_eagerly)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/var/extra/users/jgeorge/tf2.0/input/albert_base'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "albert_model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy.experimental_run_v2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_meta_data = {\n",
    "    'max_seq_length':128,\n",
    "    'num_labels':142,\n",
    "    'train_data_size':20574,\n",
    "    'eval_data_size':5144\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_meta_path = os.path.join(main_exp_folder,'input_metadata.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writing metadata file\n",
    "with open(input_meta_path,'w',encoding='utf-8') as jf:\n",
    "    jf.write(json.dumps(input_meta_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading metadata file\n",
    "with open(input_meta_path,'r',encoding='utf-8') as jf:\n",
    "#     input_meta_data = json.loads(jf.read())\n",
    "    input_meta_data = json.load(jf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_loop = 1\n",
    "learning_rate=1e-5\n",
    "epochs=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size=16\n",
    "eval_batch_size=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir_dish=os.path.join(main_exp_folder,'models')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_checkpoint=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1285\n"
     ]
    }
   ],
   "source": [
    "train_data_size = input_meta_data['train_data_size']\n",
    "steps_per_epoch = int(train_data_size / train_batch_size)\n",
    "print(steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "warmup_steps = int(epochs * train_data_size * 0.1 / train_batch_size)\n",
    "eval_steps = int(math.ceil(input_meta_data['eval_data_size'] / eval_batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_record(record, name_to_features):\n",
    "  \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n",
    "  example = tf.io.parse_single_example(record, name_to_features)\n",
    "\n",
    "  # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n",
    "  # So cast all int64 to int32.\n",
    "  for name in list(example.keys()):\n",
    "    t = example[name]\n",
    "    if t.dtype == tf.int64:\n",
    "      t = tf.cast(t, tf.int32)\n",
    "    example[name] = t\n",
    "\n",
    "  return example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_file_dataset(input_file, name_to_features):\n",
    "  \"\"\"Creates a single-file dataset to be passed for BERT custom training.\"\"\"\n",
    "  # For training, we want a lot of parallel reading and shuffling.\n",
    "  # For eval, we want no shuffling and parallel reading doesn't matter.\n",
    "  d = tf.data.TFRecordDataset(input_file)\n",
    "  d = d.map(lambda record: decode_record(record, name_to_features))\n",
    "\n",
    "  # When `input_file` is a path to a single file or a list\n",
    "  # containing a single path, disable auto sharding so that\n",
    "  # same input file is sent to all workers.\n",
    "  if isinstance(input_file, str) or len(input_file) == 1:\n",
    "    options = tf.data.Options()\n",
    "    options.experimental_distribute.auto_shard_policy = (\n",
    "        tf.data.experimental.AutoShardPolicy.OFF)\n",
    "    d = d.with_options(options)\n",
    "  return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classifier_dataset(file_path,\n",
    "                              seq_length,\n",
    "                              batch_size,\n",
    "                              is_training=True,\n",
    "                              input_pipeline_context=None):\n",
    "  \"\"\"Creates input dataset from (tf)records files for train/eval.\"\"\"\n",
    "  name_to_features = {\n",
    "      'input_ids': tf.io.FixedLenFeature([seq_length], tf.int64),\n",
    "      'input_mask': tf.io.FixedLenFeature([seq_length], tf.int64),\n",
    "      'segment_ids': tf.io.FixedLenFeature([seq_length], tf.int64),\n",
    "      'label_id': tf.io.FixedLenFeature([], tf.int64)\n",
    "#       'is_real_example': tf.io.FixedLenFeature([], tf.int64),\n",
    "  }\n",
    "  dataset = single_file_dataset(file_path, name_to_features)\n",
    "\n",
    "  # The dataset is always sharded by number of hosts.\n",
    "  # num_input_pipelines is the number of hosts rather than number of cores.\n",
    "  if input_pipeline_context and input_pipeline_context.num_input_pipelines > 1:\n",
    "    dataset = dataset.shard(input_pipeline_context.num_input_pipelines,\n",
    "                            input_pipeline_context.input_pipeline_id)\n",
    "\n",
    "  def _select_data_from_record(record):\n",
    "#     x = {\n",
    "#         'input_word_ids': record['input_ids'],\n",
    "#         'input_mask': record['input_mask'],\n",
    "#         'input_type_ids': record['segment_ids']\n",
    "#     }\n",
    "    x = {\n",
    "        'input_ids': record['input_ids'],\n",
    "        'input_mask': record['input_mask'],\n",
    "        'segment_ids': record['segment_ids']\n",
    "    }\n",
    "    y = record['label_id']\n",
    "    return (x, y)\n",
    "\n",
    "  dataset = dataset.map(_select_data_from_record)\n",
    "\n",
    "  if is_training:\n",
    "    dataset = dataset.shuffle(100)\n",
    "    dataset = dataset.repeat()\n",
    "\n",
    "  dataset = dataset.batch(batch_size, drop_remainder=is_training)\n",
    "  dataset = dataset.prefetch(1024)\n",
    "  return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_fn(input_file_pattern, max_seq_length, global_batch_size,\n",
    "                   is_training):\n",
    "  \"\"\"Gets a closure to create a dataset.\"\"\"\n",
    "  def _dataset_fn(ctx=None):\n",
    "    \"\"\"Returns tf.data.Dataset for distributed BERT pretraining.\"\"\"\n",
    "    batch_size = ctx.get_per_replica_batch_size(\n",
    "        global_batch_size) if ctx else global_batch_size\n",
    "#     dataset = input_pipeline.create_classifier_dataset(\n",
    "    dataset = create_classifier_dataset(\n",
    "        input_file_pattern,\n",
    "        max_seq_length,\n",
    "        batch_size,\n",
    "        is_training=is_training,\n",
    "        input_pipeline_context=ctx)\n",
    "    return dataset\n",
    "  return _dataset_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_fn = get_dataset_fn(\n",
    "#       FLAGS.train_data_path,\n",
    "      train_data_path,\n",
    "      max_seq_length,\n",
    "#       FLAGS.train_batch_size,\n",
    "      train_batch_size,\n",
    "      is_training=True)\n",
    "eval_input_fn = get_dataset_fn(\n",
    "#       FLAGS.eval_data_path,\n",
    "      eval_data_path,\n",
    "      max_seq_length,\n",
    "#       FLAGS.eval_batch_size,\n",
    "      eval_batch_size,\n",
    "      is_training=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " bert_model  <tensorflow_hub.keras_layer.KerasLayer object at 0x7f1ed83b7610>\n",
      "pooled_output  Tensor(\"keras_layer/Identity:0\", shape=(None, 768), dtype=float32)\n",
      "bert_output Tensor(\"keras_layer/Identity:0\", shape=(None, 768), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "trained_model = run_bert_classifier(\n",
    "      strategy,\n",
    "      bert_config,\n",
    "      input_meta_data,\n",
    "      albert_model_dir,\n",
    "      epochs,\n",
    "      steps_per_epoch,\n",
    "      steps_per_loop,\n",
    "      eval_steps,\n",
    "      warmup_steps,\n",
    "      learning_rate,\n",
    "      init_checkpoint,\n",
    "      train_input_fn,\n",
    "      eval_input_fn,\n",
    "      run_eagerly=False,\n",
    "      use_keras_compile_fit=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to find data adapter that can handle input: <class 'function'>, <class 'NoneType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-81e796e74545>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrained_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_input_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.1/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    928\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    931\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m   def predict(self,\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.1/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, model, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m         use_multiprocessing=use_multiprocessing, **kwargs)\n\u001b[0m\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m   def predict(self, model, x, batch_size=None, verbose=0, steps=None,\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.1/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_model_iteration\u001b[0;34m(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    424\u001b[0m           \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m           use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    427\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m       \u001b[0muse_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_samples\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.1/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[0;34m(model, mode, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    634\u001b[0m                     use_multiprocessing=False):\n\u001b[1;32m    635\u001b[0m   \u001b[0;34m\"\"\"Process the inputs for fit/eval/predict().\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 636\u001b[0;31m   \u001b[0madapter_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_adapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_data_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    637\u001b[0m   standardize = functools.partial(\n\u001b[1;32m    638\u001b[0m       \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.1/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mselect_data_adapter\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    996\u001b[0m         \u001b[0;34m\"Failed to find data adapter that can handle \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    997\u001b[0m         \"input: {}, {}\".format(\n\u001b[0;32m--> 998\u001b[0;31m             _type_name(x), _type_name(y)))\n\u001b[0m\u001b[1;32m    999\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter_cls\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1000\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to find data adapter that can handle input: <class 'function'>, <class 'NoneType'>"
     ]
    }
   ],
   "source": [
    "trained_model.evaluate(eval_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_fn():\n",
    "    return tf.keras.metrics.SparseCategoricalAccuracy('test_accuracy',dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = metric_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(eval_input_fn,model):\n",
    "    eval_iter = iter(strategy.experimental_distribute_datasets_from_function(eval_input_fn))\n",
    "    \n",
    "    def _test_step_fn(inputs,label):\n",
    "#         inputs,label = inputs\n",
    "        model_outputs = model(inputs,training=False)\n",
    "        metric.update_state(label,model_outputs)\n",
    "    strategy.experimental_run_v2(_test_step_fn,args=(next(eval_iter)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(eval_input_fn,trained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.125>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_export_dir = '/var/extra/users/jgeorge/tf2.0/input/albert_base_custom/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spm_model = os.path.join(model_dir, \"assets\", \"30k-clean.model\")\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(spm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model.sp_model_file = tf.saved_model.Asset(spm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.tracking.Asset at 0x7f54e0bc8310>"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model.sp_model_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/extra/users/jgeorge/tf2.0/input/albert_base_custom/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/extra/users/jgeorge/tf2.0/input/albert_base_custom/assets\n"
     ]
    }
   ],
   "source": [
    "trained_model.save(model_export_dir,include_optimizer=False,save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.125>"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown signature default in https://tfhub.dev/google/albert_base/2 (available signatures: _SignatureMap({'tokenization_info': <tensorflow.python.eager.wrap_function.WrappedFunction object at 0x7f550be41450>, 'mlm': <tensorflow.python.eager.wrap_function.WrappedFunction object at 0x7f57ec038a50>, 'tokens': <tensorflow.python.eager.wrap_function.WrappedFunction object at 0x7f55086cf990>})).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-345-a847214142b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbert_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKerasLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malbert_hub_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.1/lib/python3.7/site-packages/tensorflow_hub/keras_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, handle, trainable, arguments, _sentinel, tags, signature, signature_outputs_as_dict, output_key, output_shape, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m                        \"a signature (or using a legacy Hub.Module).\")\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.1/lib/python3.7/site-packages/tensorflow_hub/keras_layer.py\u001b[0m in \u001b[0;36m_get_callable\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_signature\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignatures\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m       raise ValueError(\"Unknown signature %s in %s (available signatures: %s).\"\n\u001b[0;32m--> 250\u001b[0;31m                        % (self._signature, self._handle, self._func.signatures))\n\u001b[0m\u001b[1;32m    251\u001b[0m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_signature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown signature default in https://tfhub.dev/google/albert_base/2 (available signatures: _SignatureMap({'tokenization_info': <tensorflow.python.eager.wrap_function.WrappedFunction object at 0x7f550be41450>, 'mlm': <tensorflow.python.eager.wrap_function.WrappedFunction object at 0x7f57ec038a50>, 'tokens': <tensorflow.python.eager.wrap_function.WrappedFunction object at 0x7f55086cf990>}))."
     ]
    }
   ],
   "source": [
    "bert_model = hub.KerasLayer(albert_hub_url,trainable=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = hub.KerasLayer(albert_hub_url,trainable=True,signature='tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "in converted code:\n\n    /opt/custom/python/anaconda3/envs/tf2.1/lib/python3.7/site-packages/tensorflow_hub/keras_layer.py:209 call  *\n        result = f()\n    /opt/custom/python/anaconda3/envs/tf2.1/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py:1551 __call__  *\n        return self._call_impl(args, kwargs)\n    /opt/custom/python/anaconda3/envs/tf2.1/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py:1570 _call_impl\n        ).format(self._num_positional_args, self._arg_keywords, args))\n\n    TypeError: Expected at most 0 positional arguments (and the rest keywords, of ['segment_ids', 'input_ids', 'input_mask']), got ([<tf.Tensor 'segment_ids:0' shape=(None, 128) dtype=int32>, <tf.Tensor 'input_ids:0' shape=(None, 128) dtype=int32>, <tf.Tensor 'input_mask:0' shape=(None, 128) dtype=int32>],). When calling a concrete function, positional arguments may not be bound to Tensors within nested structures.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-126-49c96daf56c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m       \u001b[0meval_input_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m       \u001b[0mrun_eagerly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m       use_keras_compile_fit=False)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-123-6373bbe5532a>\u001b[0m in \u001b[0;36mrun_bert_classifier\u001b[0;34m(strategy, bert_config, input_meta_data, model_dir, epochs, steps_per_epoch, steps_per_loop, eval_steps, warmup_steps, initial_lr, init_checkpoint, train_input_fn, eval_input_fn, custom_callbacks, run_eagerly, use_keras_compile_fit)\u001b[0m\n\u001b[1;32m     23\u001b[0m                                                               \u001b[0mnum_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                                                               \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                                                                hub_module_url=albert_hub_url)\n\u001b[0m\u001b[1;32m     26\u001b[0m     classifier_model.optimizer = optimization.create_optimizer(init_lr=initial_lr,\n\u001b[1;32m     27\u001b[0m                                                                \u001b[0mnum_train_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-121-fef5ea442f65>\u001b[0m in \u001b[0;36mget_classifier_model\u001b[0;34m(bert_config, float_type, num_labels, max_seq_length, final_layer_initializer, hub_module_url)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mbert_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKerasLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malbert_hub_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tokens'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msignature_outputs_as_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m#     pooled_output,_ = bert_model([input_word_ids,input_mask,input_type_ids])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mpooled_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msegment_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_dropout_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpooled_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkernel_initializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'output'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.1/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    771\u001b[0m                     not base_layer_utils.is_in_eager_or_tf_function()):\n\u001b[1;32m    772\u001b[0m                   \u001b[0;32mwith\u001b[0m \u001b[0mauto_control_deps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAutomaticControlDependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0macd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 773\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    774\u001b[0m                     \u001b[0;31m# Wrap Tensors in `outputs` in `tf.identity` to avoid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m                     \u001b[0;31m# circular dependencies.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.1/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    235\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: in converted code:\n\n    /opt/custom/python/anaconda3/envs/tf2.1/lib/python3.7/site-packages/tensorflow_hub/keras_layer.py:209 call  *\n        result = f()\n    /opt/custom/python/anaconda3/envs/tf2.1/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py:1551 __call__  *\n        return self._call_impl(args, kwargs)\n    /opt/custom/python/anaconda3/envs/tf2.1/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py:1570 _call_impl\n        ).format(self._num_positional_args, self._arg_keywords, args))\n\n    TypeError: Expected at most 0 positional arguments (and the rest keywords, of ['segment_ids', 'input_ids', 'input_mask']), got ([<tf.Tensor 'segment_ids:0' shape=(None, 128) dtype=int32>, <tf.Tensor 'input_ids:0' shape=(None, 128) dtype=int32>, <tf.Tensor 'input_mask:0' shape=(None, 128) dtype=int32>],). When calling a concrete function, positional arguments may not be bound to Tensors within nested structures.\n"
     ]
    }
   ],
   "source": [
    "trained_model = run_bert_classifier(\n",
    "      strategy,\n",
    "      bert_config,\n",
    "      input_meta_data,\n",
    "      albert_model_dir,\n",
    "      epochs,\n",
    "      steps_per_epoch,\n",
    "      steps_per_loop,\n",
    "      eval_steps,\n",
    "      warmup_steps,\n",
    "      learning_rate,\n",
    "      init_checkpoint,\n",
    "      train_input_fn,\n",
    "      eval_input_fn,\n",
    "      run_eagerly=False,\n",
    "      use_keras_compile_fit=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = set()\n",
    "tags.add('train')\n",
    "\n",
    "loaded_albert_model = tf.saved_model.load(albert_model_dir,tags=tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_SignatureMap({'mlm': <tensorflow.python.eager.wrap_function.WrappedFunction object at 0x7f551de1d9d0>, 'tokens': <tensorflow.python.eager.wrap_function.WrappedFunction object at 0x7f551d98e090>, 'tokenization_info': <tensorflow.python.eager.wrap_function.WrappedFunction object at 0x7f551d5d5050>})"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_albert_model.signatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.eager.wrap_function.WrappedFunction at 0x7f551de1d9d0>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_albert_model.signatures['mlm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 - tensorflow 2.1 (tf2.1)",
   "language": "python",
   "name": "tf2.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
