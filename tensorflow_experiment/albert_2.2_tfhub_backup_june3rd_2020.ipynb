{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "# import bert\n",
    "# from bert import BertModelLayer\n",
    "import functools\n",
    "\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import sentencepiece as spm\n",
    "\n",
    "import tf_sentencepiece as tfs\n",
    "import tensorflow_text as tftext\n",
    "import sys\n",
    "sys.path.extend([\"/var/extra/users/jgeorge/tf2.0/git/models/\"])\n",
    "\n",
    "\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import csv\n",
    "import collections\n",
    "import pandas as pd\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from absl import logging\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# pylint: disable=g-import-not-at-top,redefined-outer-name,reimported\n",
    "from official.modeling import model_training_utils\n",
    "\n",
    "from official.nlp.modeling.models import bert_classifier, bert_pretrainer\n",
    "# from official.nlp.modeling.models.bert_classifier import BertClassifier\n",
    "# from official.nlp.modeling.models.bert_pretrainer import BertPretrainer\n",
    "# from official.nlp import bert_modeling as modeling\n",
    "# from official.nlp import bert_models\n",
    "from official.nlp import optimization\n",
    "from official.nlp.bert import common_flags\n",
    "from official.nlp.bert import input_pipeline\n",
    "from official.nlp.bert import model_saving_utils\n",
    "from official.utils.misc import distribution_utils\n",
    "from official.utils.misc import keras_utils\n",
    "from official.nlp.bert import tokenization\n",
    "\n",
    "from official.nlp.albert import configs as albert_configs\n",
    "from official.nlp.bert import run_classifier as run_classifier_bert\n",
    "\n",
    "from tensorflow.python.keras.utils import tf_utils\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'tf_utils' from 'tensorflow.keras.utils' (/opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/keras/utils/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-14f6ce359f15>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'tf_utils' from 'tensorflow.keras.utils' (/opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/keras/utils/__init__.py)"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import tf_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.engine import network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAGS = flags.FLAGS    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:4', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:5', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:6', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:7', device_type='GPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.experimental.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "##restricting no of gpus\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "device_to_use = gpus[3]\n",
    "tf.config.experimental.set_memory_growth(device_to_use,True)\n",
    "tf.config.experimental.set_visible_devices(device_to_use, 'GPU')\n",
    "print(tf.config.experimental.get_visible_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##setting direcory for downloading tfhub modules\n",
    "os.environ[\"TFHUB_CACHE_DIR\"] = '/space/engineering/tfhub_modules'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dish_data_path='/var/extra/users/jgeorge/tf2.0/input/dish/data/jan17_2020/dishDataJan17.txt'\n",
    "df = pd.read_csv(dish_data_path,sep='\\t',header=None,names=['filename','text','granular_intent','ru_intent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>text</th>\n",
       "      <th>granular_intent</th>\n",
       "      <th>ru_intent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INT-sv1appis14-1504137880316-305603_4567</td>\n",
       "      <td>can you send my bill to my mail?</td>\n",
       "      <td>billing-preferences</td>\n",
       "      <td>billing-preferences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>INT-va1appis15-1504373018548-41332</td>\n",
       "      <td>My Wally receiver has lost Satellite signal in...</td>\n",
       "      <td>comp_part_signal_loss-issue</td>\n",
       "      <td>comp_part_signal_loss-issue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INT-sv1appis12-1503954587819-263368</td>\n",
       "      <td>I need a payment extension so i don't get my s...</td>\n",
       "      <td>payment_extension-request</td>\n",
       "      <td>payment_extension-request</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8ed8e9b1-10f5-438d-c409-e616c3ff9ede</td>\n",
       "      <td>how can i find my local channels. it seems i d...</td>\n",
       "      <td>channel_package-issue</td>\n",
       "      <td>channel_package-issue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>INT-sv1appis13-1504099917638-293735</td>\n",
       "      <td>Wanted to speak with someone about my bill</td>\n",
       "      <td>representative-request</td>\n",
       "      <td>representative-request</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   filename  \\\n",
       "0  INT-sv1appis14-1504137880316-305603_4567   \n",
       "1        INT-va1appis15-1504373018548-41332   \n",
       "2       INT-sv1appis12-1503954587819-263368   \n",
       "3      8ed8e9b1-10f5-438d-c409-e616c3ff9ede   \n",
       "4       INT-sv1appis13-1504099917638-293735   \n",
       "\n",
       "                                                text  \\\n",
       "0                   can you send my bill to my mail?   \n",
       "1  My Wally receiver has lost Satellite signal in...   \n",
       "2  I need a payment extension so i don't get my s...   \n",
       "3  how can i find my local channels. it seems i d...   \n",
       "4         Wanted to speak with someone about my bill   \n",
       "\n",
       "               granular_intent                    ru_intent  \n",
       "0          billing-preferences          billing-preferences  \n",
       "1  comp_part_signal_loss-issue  comp_part_signal_loss-issue  \n",
       "2    payment_extension-request    payment_extension-request  \n",
       "3        channel_package-issue        channel_package-issue  \n",
       "4       representative-request       representative-request  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train,df_val,y_train,y_val = train_test_split(df,df['granular_intent'],train_size=0.8,random_state=42,stratify=df['granular_intent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_exp_folder = '/space/users/jgeorge/projects/k/tensorflow2-question-answering/input/dish/data/jan17_2020/'\n",
    "main_exp_folder = '/var/extra/users/jgeorge/tf2.0/input/dish/data/jan17_2020/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = '/var/extra/users/jgeorge/tf2.0/input/dish/models/albert_en_large'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_csv_file='/var/extra/users/jgeorge/tf2.0/input/dish/data/jan17_2020/train.txt'\n",
    "eval_csv_file='/var/extra/users/jgeorge/tf2.0/input/dish/data/jan17_2020/val.txt'\n",
    "\n",
    "df_train.to_csv(train_csv_file,sep='\\t',header=False,index=False)\n",
    "df_val.to_csv(eval_csv_file,sep='\\t',header=False,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_file = '/var/extra/users/jgeorge/tf2.0/input/dish/data/jan17_2020/intentlist.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(intent_file,'w',encoding='utf-8') as out_f:\n",
    "    for intent in sorted(df_train['granular_intent'].unique()):\n",
    "        out_f.write(intent+'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_list = []\n",
    "with open(intent_file,'r') as inp_f:\n",
    "    for intent in inp_f:\n",
    "        intent_list.append(intent.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_meta_data = {\n",
    "    'max_seq_length':128,\n",
    "    'num_labels':142,\n",
    "    'train_data_size':20574,\n",
    "    'eval_data_size':5144\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_meta_path = os.path.join(main_exp_folder,'input_metadata.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writing metadata file\n",
    "with open(input_meta_path,'w',encoding='utf-8') as jf:\n",
    "    jf.write(json.dumps(input_meta_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading metadata file\n",
    "with open(input_meta_path,'r',encoding='utf-8') as jf:\n",
    "#     input_meta_data = json.loads(jf.read())\n",
    "    input_meta_data = json.load(jf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing & feeding into graph\n",
    "We will be reading the regular text file as input & encoding the text \n",
    "using sentence piece encoder provided along with albert & then\n",
    "writing it as tfrecord. Other than the sentencepiece encoder these models does not require any preprocessing like regex-replacement, lemmatization etc \n",
    "\n",
    "Writing as tfrecord is not necessary, but it is a more optimized file format for reading into \n",
    "tf.data api (you can even base your tf dataset on a textfile or even a python iterator)\n",
    "Also it's not necessary to use tf.data apis but this is much more optimized, like preloading data into gpu memory & optimization required if you are running across systems & all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence piece encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "##albert hub url\n",
    "bert_hub_url='https://tfhub.dev/tensorflow/albert_en_large/1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to load the used sentence piece model from the hub module, \n",
    "it will be present in the assests folder of the downloaded albert model\n",
    "Since we are using hub here & not directly downloading it, the path can be fetch from hub layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_hub_loaded = hub.load(bert_hub_url)\n",
    "sentencepiece_path = bert_hub_loaded.sp_model_file.asset_path.numpy()\n",
    "\n",
    "# or if you are using hub.KeraLayer\n",
    "# bert_model = hub.KerasLayer(bert_hub_url,trainable=True)\n",
    "# sentencepiece_path = bert_model.resolved_object.sp_model_file.asset_path.numpy()\n",
    "\n",
    "##if you have directly downloaded the model you could do this\n",
    "# sentencepiece_path = os.path.join(model_dir, \"assets\", \"30k-clean.model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "##loading the sentence piece model into memory\n",
    "sp_model_proto = tf.io.gfile.GFile(sentencepiece_path, 'rb').read()\n",
    "\n",
    "# sp_model = sp_model_proto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For sentence piece we can use either the       \n",
    "1) tf_sentencepiece (import as tfs here)     \n",
    "2) sentencepiece (imported as spm) package        \n",
    "3) sentencepiece from tensorflow_text    \n",
    "4) tokenization.FullSentencePieceTokenizer - a wrapper on top of 2) sentencepiece, code is available in models/official/nlp/bert/tokenization.py      \n",
    "\n",
    "We will eventually just use tf_sentencepiece as that's the one which is currently, available to export as a graph.      \n",
    "But for the 1st step of creating training data (in tf records format) either of them will do\n",
    "but i'll illustrate how to use others as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded_text  SentencepieceEncodeDense(values=<tf.Tensor: shape=(1, 16), dtype=int32, numpy=\n",
      "array([[  590,    55,  7876,    20,  6557,  3284,  5477, 11969,   357,\n",
      "            8,  3099,     8,  1323,  1433,   159, 10114]], dtype=int32)>, sequence_length=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([16], dtype=int32)>) , decoded text  tf.Tensor([b'give me directions to nearest restaurant randomtext 15-08-2019 year 2019'], shape=(1,), dtype=string) \n",
      "\n",
      "encoded_text2  [590, 55, 7876, 20, 6557, 3284, 5477, 11969, 357, 8, 3099, 8, 1323, 1433, 159, 10114] , decoded text2  give me directions to nearest restaurant randomtext 15-08-2019 year 2019 \n",
      "\n",
      "encoded_text3  tf.Tensor(\n",
      "[  590    55  7876    20  6557  3284  5477 11969   357     8  3099     8\n",
      "  1323  1433   159 10114], shape=(16,), dtype=int32) , decoded text3  tf.Tensor(b'give me directions to nearest restaurant randomtext 15-08-2019 year 2019', shape=(), dtype=string) \n",
      "\n",
      "tokens4 ['▁give', '▁me', '▁directions', '▁to', '▁nearest', '▁restaurant', '▁random', 'text', '▁15', '-', '08', '-', '20', '19', '▁year', '▁2019'] \n",
      "ids4  [590, 55, 7876, 20, 6557, 3284, 5477, 11969, 357, 8, 3099, 8, 1323, 1433, 159, 10114]\n"
     ]
    }
   ],
   "source": [
    "actual_sentence = 'give me directions to nearest restaurant randomtext 15-08-2019 year 2019'\n",
    "#1st method\n",
    "encoded_text1 = tfs.encode([actual_sentence],model_proto=sp_model_proto)\n",
    "##This method return\n",
    "#   pieces: A dense 2D tensor representing the tokenized sentences. key = values\n",
    "#   sequence_length: A 1D tensor representing the length of pieces.\n",
    "\n",
    "encoded_value1 = encoded_text1.values.numpy()\n",
    "sequence_length=encoded_text1.sequence_length.numpy()[0]\n",
    "# sequence_length=len(encoded_value1[0])\n",
    "print('encoded_text ',encoded_text1, ', decoded text ',\n",
    "      tfs.decode(encoded_value1,sequence_length=[sequence_length],model_proto=sp_model_proto),'\\n')\n",
    "\n",
    "# 2nd method\n",
    "sp2 = spm.SentencePieceProcessor()\n",
    "sp2.load(model_proto=sp_model_proto)\n",
    "encoded_text2 = sp2.encode_as_ids(actual_sentence)\n",
    "print('encoded_text2 ',encoded_text2, ', decoded text2 ',\n",
    "      sp2.decode_ids(encoded_text2),'\\n')\n",
    "\n",
    "###3rd methond\n",
    "sp3 = tftext.SentencepieceTokenizer(model=sp_model_proto)\n",
    "encoded_text3 = sp3.tokenize(actual_sentence)\n",
    "print('encoded_text3 ',encoded_text3, ', decoded text3 ',sp3.detokenize(encoded_text3.numpy()),'\\n')\n",
    "\n",
    "\n",
    "#method 4\n",
    "#this is just another wrapper around sentencepiece\n",
    "sp4 = tokenization.FullSentencePieceTokenizer(sentencepiece_path)\n",
    "tokens4= sp4.tokenize(actual_sentence)\n",
    "ids4 = sp4.convert_tokens_to_ids(tokens4)\n",
    "print('tokens4',tokens4, '\\nids4 ',ids4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You would have noticed that in method 1) tf_sentencepiece i have to pass sequence_length for decoding\n",
    "Printing output from couple of other functions in sentencepiece, just to get a better understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pieces1  ['▁give', '▁me', '▁directions', '▁to', '▁nearest', '▁restaurant', '▁random', 'text', '▁15', '-', '08', '-', '20', '19', '▁year', '▁2019'] \n",
      "ids1  [590, 55, 7876, 20, 6557, 3284, 5477, 11969, 357, 8, 3099, 8, 1323, 1433, 159, 10114] \n",
      "\n",
      "pieces2  ['▁give', '▁me', '▁directions', '▁to', '▁nearest', 'rest', 'au', 'rant'] \n",
      "ids2  [590, 55, 7876, 20, 6557, 11466, 1346, 7874] \n",
      "pieces2_2  ['▁give', '▁me', '▁directions', '▁to', '▁nearest', 'rest', 'au', 'rant'] \n",
      "decoded_piece  give me directions to nearestrestaurant\n"
     ]
    }
   ],
   "source": [
    "pieces1 = sp2.encode_as_pieces(actual_sentence)\n",
    "ids1 = sp2.piece_to_id(pieces1)\n",
    "print('pieces1 ', pieces1 , '\\nids1 ',ids1,'\\n')\n",
    "\n",
    "pieces2 = sp2.encode_as_pieces('give me directions to nearestrestaurant')\n",
    "ids2 = sp2.piece_to_id(pieces2)\n",
    "pieces2_2 = sp2.id_to_piece(ids2)\n",
    "decoded_piece = sp2.decode_pieces(pieces2)\n",
    "\n",
    "print('pieces2 ', pieces2, '\\nids2 ',ids2,'\\npieces2_2 ',pieces2_2, '\\ndecoded_piece ',decoded_piece )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad>', '<unk>', '[CLS]']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp2.id_to_piece([0,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 <pad>\n",
      "1 <unk>\n",
      "2 [CLS]\n",
      "3 [SEP]\n",
      "4 [MASK]\n",
      "5 (\n",
      "6 )\n",
      "7 \"\n",
      "8 -\n",
      "9 .\n",
      "10 –\n",
      "11 £\n",
      "12 €\n",
      "13 ▁\n",
      "14 ▁the\n",
      "15 ,\n",
      "16 ▁of\n",
      "17 ▁and\n",
      "18 s\n",
      "19 ▁in\n",
      "20 ▁to\n",
      "21 ▁a\n",
      "22 '\n",
      "23 ▁was\n",
      "24 ▁he\n",
      "25 ▁is\n",
      "26 ▁for\n",
      "27 ▁on\n",
      "28 ▁as\n",
      "29 ▁with\n",
      "30 ▁that\n",
      "31 ▁i\n",
      "32 ▁it\n",
      "33 ▁his\n",
      "34 ▁by\n",
      "35 ▁at\n",
      "36 ▁her\n",
      "37 ▁from\n",
      "38 t\n",
      "39 ▁she\n",
      "40 ▁an\n",
      "41 ▁had\n",
      "42 ▁you\n",
      "43 d\n",
      "44 ▁be\n",
      "45 :\n",
      "46 ▁were\n",
      "47 ▁but\n",
      "48 ▁this\n",
      "49 i\n",
      "50 ▁are\n",
      "51 ▁my\n",
      "52 ▁not\n",
      "53 ▁one\n",
      "54 ▁or\n",
      "55 ▁me\n",
      "56 ▁which\n",
      "57 ▁have\n",
      "58 a\n",
      "59 ▁they\n",
      "60 ?\n",
      "61 ▁him\n",
      "62 e\n",
      "63 ▁has\n",
      "64 ▁first\n",
      "65 ▁all\n",
      "66 ▁their\n",
      "67 ▁also\n",
      "68 ing\n",
      "69 ed\n",
      "70 ▁out\n",
      "71 ▁up\n",
      "72 ▁who\n",
      "73 ;\n",
      "74 ▁been\n",
      "75 ▁after\n",
      "76 ▁when\n",
      "77 ▁into\n",
      "78 ▁new\n",
      "79 m\n",
      "80 ▁there\n",
      "81 ▁two\n",
      "82 ▁its\n",
      "83 ▁would\n",
      "84 ▁over\n",
      "85 ▁time\n",
      "86 ▁so\n",
      "87 ▁said\n",
      "88 ▁about\n",
      "89 ▁other\n",
      "90 ▁no\n",
      "91 ▁more\n",
      "92 ▁can\n",
      "93 y\n",
      "94 ▁then\n",
      "95 ▁we\n",
      "96 th\n",
      "97 ▁back\n",
      "98 ▁what\n",
      "99 re\n"
     ]
    }
   ],
   "source": [
    "#printing out 1st 100 tokens, just to get a feel of what all entries are present\n",
    "for index,token in enumerate (sp2.id_to_piece([*range(100)])):\n",
    "    print(index,token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the data into TFRecords\n",
    "For following functionalities refer the file  - models/official/nlp/data/classifier_data_lib.py       \n",
    "This includes even the sentence piece tokenization part\n",
    "#### Defining an object for the 1 row of text input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#in our case we are doing just classification so we only need 1 text value = text_a\n",
    "class InputExample(object):\n",
    "    def __init__(self,uuid,text_a,text_b=None,label=None):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "        Args:\n",
    "          uuid: Unique id for the example.\n",
    "          text_a: string. The untokenized text of the first sequence. For single\n",
    "            sequence tasks, only this sequence must be specified.\n",
    "          text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "            Only must be specified for sequence pair tasks.\n",
    "          label: (Optional) string. The label of the example. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "        \"\"\"\n",
    "        self.uuid = uuid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### defining an object for the 1 row of input feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "### in the official one there was something called is_real_example as feature, here i skipped it since it's \n",
    "#only classification\n",
    "class InputFeatures(object):\n",
    "    def __init__(self,input_ids,input_mask,\n",
    "                segment_ids,label_id):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### functions for converting values to train features so that it can be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bytes_feature(value):\n",
    "  \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "  if isinstance(value, type(tf.constant(0))):\n",
    "    print(\"type constant \",type(tf.constant(0)))\n",
    "    value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
    "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _int64_feature(values):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\n",
    "      #Note that the Feature requires a list as input\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path=os.path.join(main_exp_folder,'train.tfrecords')\n",
    "eval_data_path=os.path.join(main_exp_folder,'eval.tfrecords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLS_ID  2 , SEP_ID  3\n"
     ]
    }
   ],
   "source": [
    "#special tokens\n",
    "[CLS_ID,SEP_ID]  = tfs.piece_to_id(['[CLS]','[SEP]'],model_proto=sp_model_proto).numpy()\n",
    "print('CLS_ID ', CLS_ID, ', SEP_ID ', SEP_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function which converts a single input to Features, one of the feature being the sentencepiece encoded array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_single_example(ex_index, example, label_list, max_seq_length,sentence_piece):\n",
    "    ## please refer to functionality in file models/official/nlp/data/classifier_data_lib.py \n",
    "    ##this function is based on an older version & slightly different as in it's taking care of only \n",
    "    # the classification part\n",
    "    \n",
    "    # The convention in BERT is:\n",
    "    # (a) For sequence pairs:\n",
    "    #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "    #  type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1\n",
    "    # (b) For single sequences:\n",
    "    #  tokens:   [CLS] the dog is hairy . [SEP]\n",
    "    #  type_ids: 0     0   0   0  0     0 0\n",
    "    #\n",
    "    # Where \"type_ids\" are used to indicate whether this is the first\n",
    "    # sequence or the second sequence. The embedding vectors for `type=0` and\n",
    "    # `type=1` were learned during pre-training and are added to the wordpiece\n",
    "    # embedding vector (and position vector). This is not *strictly* necessary\n",
    "    # since the [SEP] token unambiguously separates the sequences, but it makes\n",
    "    # it easier for the model to learn the concept of sequences.\n",
    "    #\n",
    "    # For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "    # used as the \"sentence vector\". Note that this only makes sense because\n",
    "    # the entire model is fine-tuned.\n",
    "    \n",
    "#     CLS_ID = 2\n",
    "#     SEP_ID = 3\n",
    "    [CLS_ID,SEP_ID]  = tfs.piece_to_id(['[CLS]','[SEP]'],model_proto=sp_model).numpy()\n",
    "    # -2 for [CLS], [SEP] \n",
    "    # we need to add these tokens in addition to the sentence words is while keeping the max sequence length limit\n",
    "    # check Bert paper for this [CLS] is added at the beginning & [SEP] after 1st sentence (in our classificatin example \n",
    "    # we don't differentiate the sentences here, so [SEP] is at the end. For challenges like SQUAD 2 sentences is \n",
    "    # where i have seen people use 2 sentences , 1st question & 2nd answer )\n",
    "    \n",
    "    max_word_ids = max_seq_length -2\n",
    "    input_ids = []\n",
    "    segment_ids = []\n",
    "    input_mask = []\n",
    "    input_ids.append(CLS_ID)\n",
    "    \n",
    "    word_ids = tfs.encode([example.text_a],model_proto=sentence_piece).values[0]\n",
    "    \n",
    "#     print('word ids ',word_ids)\n",
    "# trimming to the max size\n",
    "    word_ids = word_ids[:max_word_ids]\n",
    "    input_ids.extend(word_ids)\n",
    "#     print(\"input text \",example.text)\n",
    "    input_ids.append(SEP_ID)\n",
    "    segment_ids = [0]*len(input_ids)\n",
    "    input_mask = [1]*len(input_ids)\n",
    "    if len(input_ids)<max_seq_length:\n",
    "        diff = max_seq_length - len(input_ids)\n",
    "        input_ids.extend([0]*diff)\n",
    "        segment_ids.extend([0]*diff)\n",
    "        input_mask.extend([0]*diff)\n",
    "    assert(len(input_ids)==max_seq_length)\n",
    "    assert(len(segment_ids)==max_seq_length)\n",
    "    assert(len(input_mask)==max_seq_length)\n",
    "    label_map = {label:i for i,label in enumerate(label_list)}\n",
    "    label_id = label_map[example.label]\n",
    "    if ex_index < 5:\n",
    "        logging.info(\"*** Example ***\")\n",
    "        logging.info(\"uuid: %s\", (example.uuid))\n",
    "        logging.info(\"tokens: %s\",\n",
    "                     \" \".join([str(x) for x in tfs.id_to_piece(input_ids,model_proto=sentence_piece).numpy()]))\n",
    "        logging.info(\"input_ids: %s\", \" \".join([str(x) for x in input_ids]))\n",
    "        logging.info(\"input_mask: %s\", \" \".join([str(x) for x in input_mask]))\n",
    "        logging.info(\"segment_ids: %s\", \" \".join([str(x) for x in segment_ids]))\n",
    "        logging.info(\"label: %s (id = %d)\", example.label, label_id)\n",
    "\n",
    "    feature = InputFeatures(input_ids,input_mask,segment_ids,label_id)\n",
    "    return feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_config_dict = {\n",
    "    \"hidden_dropout_prob\":0,\n",
    "    'initializer_range': 0.02\n",
    "}\n",
    "\n",
    "# bert_config = modeling.AlbertConfig.from_json_file(FLAGS.bert_config_file)\n",
    "# bert_config = albert_configs.AlbertConfig.from_json_file(albert_config_file)\n",
    "bert_config = albert_configs.AlbertConfig.from_dict(bert_config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 128\n",
    "# max_seq_length = bert_config.max_position_embeddings"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "albert_config_file='/var/extra/users/jgeorge/tf2.0/input/albert_base/assets/albert_config.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vocab_size': 30000,\n",
       " 'hidden_size': 768,\n",
       " 'num_hidden_layers': 12,\n",
       " 'num_attention_heads': 12,\n",
       " 'hidden_act': 'gelu',\n",
       " 'intermediate_size': 3072,\n",
       " 'hidden_dropout_prob': 0,\n",
       " 'attention_probs_dropout_prob': 0,\n",
       " 'max_position_embeddings': 512,\n",
       " 'type_vocab_size': 2,\n",
       " 'initializer_range': 0.02,\n",
       " 'backward_compatible': True,\n",
       " 'embedding_size': 128,\n",
       " 'num_hidden_groups': 1,\n",
       " 'net_structure_type': 0,\n",
       " 'gap_size': 0,\n",
       " 'num_memory_blocks': 0,\n",
       " 'inner_group_num': 1,\n",
       " 'down_scale_factor': 1}"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "albert_config.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is the code which reads data line by line converts & writes the data in tfrecord format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#refer file_based_convert_examples_to_features function in models/official/nlp/data/classifier_data_lib.py\n",
    "def write_training_data(input_csv_file,output_file,sentencepiece_path=None,sp_proto=None,delimiter='\\t',max_seq_length=128):\n",
    "    ##loading the sentence piece model into memory\n",
    "    if sp_proto ==None:\n",
    "        sp_proto = tf.io.gfile.GFile(sentencepiece_path, 'rb').read()\n",
    "   \n",
    "    with open(input_csv_file,'r',encoding='utf-8') as csv_file, tf.io.TFRecordWriter(output_file) as tf_record_writer:\n",
    "        #tried using binary format for bytes_list, but csv_reader requires text format\n",
    "#     with open(input_csv_file,'rb') as csv_file, tf.io.TFRecordWriter(output_file) as tf_record_writer:\n",
    "        csv_reader = csv.reader(csv_file,delimiter=delimiter,quotechar='\"')\n",
    "        for i,cols in enumerate(csv_reader):\n",
    "            ##skipping filename & granular tag (granular tag & final tag are the same here)\n",
    "#             yield cols[1:-1]\n",
    "            features = collections.OrderedDict()\n",
    "            \n",
    "            uuid = cols[0]\n",
    "            text = cols[1]\n",
    "            if(len(cols)<4):\n",
    "                print('uuid ',uuid)\n",
    "            intent = cols[2]\n",
    "            input_example = InputExample(uuid,text,label=intent)\n",
    "            #this will encode the text,label into ids \n",
    "            feature = convert_single_example(ex_index=i,example=input_example,label_list=intent_list,\n",
    "                                             max_seq_length=max_seq_length,sentence_piece=sp_proto)\n",
    "            features[\"input_ids\"] = _int64_feature(feature.input_ids)\n",
    "            features[\"input_mask\"] = _int64_feature(feature.input_mask)\n",
    "            features[\"segment_ids\"] = _int64_feature(feature.segment_ids)\n",
    "            #Note making it as a list & passing it\n",
    "            features[\"label_id\"] = _int64_feature([feature.label_id])\n",
    "#             features[\"is_real_example\"]\n",
    "            tf_example = tf.train.Example(features=tf.train.Features(feature=features))\n",
    "            tf_record_writer.write(tf_example.SerializeToString())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_training_data(train_csv_file,train_data_path,sp_proto=sp_model,max_seq_length=max_seq_length)\n",
    "write_training_data(eval_csv_file,eval_data_path,sp_proto=sp_model,max_seq_length=max_seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the data from tfrecord to provide to training graph\n",
    "Before we write the actual code to read it let's just explore some of the functions in tf.data api & on how to decode the tfrecord data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_ds = tf.data.TFRecordDataset(train_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _decode_data(record):\n",
    "    features = {\n",
    "        \"input_ids\":tf.io.FixedLenFeature([max_seq_length],tf.int64),\n",
    "        \"input_mask\":tf.io.FixedLenFeature([max_seq_length],tf.int64),\n",
    "        \"segment_ids\":tf.io.FixedLenFeature([max_seq_length],tf.int64),\n",
    "        \"label_id\":tf.io.FixedLenFeature([],tf.int64)\n",
    "    }\n",
    "#     features = {\n",
    "#         'uuid':tf.io.FixedLenFeature([],tf.string),\n",
    "#         'text':tf.io.VarLenFeature(tf.int64),\n",
    "#         'intent':tf.io.FixedLenFeature([],tf.string)\n",
    "#     }    \n",
    "    return tf.io.parse_single_example(record,features=features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## map function processes line by line, similar to spark,scala map or pandas apply fucntion\n",
    "processed_data = raw_ds.map(_decode_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': <tf.Tensor: shape=(128,), dtype=int64, numpy=\n",
      "array([    2,   483,   144,    51,  1071,  1839,    34, 14737,    37,\n",
      "         236,   818,  1071,    60,     3,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0])>, 'input_mask': <tf.Tensor: shape=(128,), dtype=int64, numpy=\n",
      "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])>, 'label_id': <tf.Tensor: shape=(), dtype=int64, numpy=103>, 'segment_ids': <tf.Tensor: shape=(128,), dtype=int64, numpy=\n",
      "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])>}\n",
      "{'input_ids': <tf.Tensor: shape=(128,), dtype=int64, numpy=\n",
      "array([   2,   13,    1,  259,   20, 3547,   14, 1889, 2440, 3607,    3,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0])>, 'input_mask': <tf.Tensor: shape=(128,), dtype=int64, numpy=\n",
      "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])>, 'label_id': <tf.Tensor: shape=(), dtype=int64, numpy=34>, 'segment_ids': <tf.Tensor: shape=(128,), dtype=int64, numpy=\n",
      "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])>}\n"
     ]
    }
   ],
   "source": [
    "## taking only 2 rows & iterating & viewing output\n",
    "for line in processed_data.take(2):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function for reading & creating a dataset object from tfrecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classifier_dataset(input_file,seq_length,batch_size,is_training=True):\n",
    "# this is a simplified version & slightly less optimized version of what is used in official bert training\n",
    "# refer to function create_classifier_dataset in models/official/nlp/data/create_finetuning_data.py\n",
    "\n",
    "\n",
    "    # create a tf_data set out of the tfrecords file\n",
    "    dataset = tf.data.TFRecordDataset(input_file)\n",
    "    name_to_features = {\n",
    "        \"input_ids\":tf.io.FixedLenFeature([max_seq_length],tf.int64),\n",
    "        \"input_mask\":tf.io.FixedLenFeature([max_seq_length],tf.int64),\n",
    "        \"segment_ids\":tf.io.FixedLenFeature([max_seq_length],tf.int64),\n",
    "        \"label_id\":tf.io.FixedLenFeature([],tf.int64)\n",
    "    }\n",
    "    ## map function processes line by line, similar to spark,scala map or pandas apply fucntion\n",
    "    dataset = dataset.map(lambda record: tf.io.parse_single_example(record, name_to_features))\n",
    "#     or could even do this\n",
    "#     dataset = dataset.map(_decode_data)\n",
    "    \n",
    "#     now separating out the features & label\n",
    "    def _select_data_from_record(record):\n",
    "#         x contains the features\n",
    "#         y is your prediction\n",
    "#This dataset will be passed to keras's model.fit refer to it's documentation for further details\n",
    "# a short snippet from that documentation\n",
    "\n",
    "# A `tf.data` dataset. Should return a tuple\n",
    "#         of either `(inputs, targets)` or\n",
    "#         `(inputs, targets, sample_weights)`.\n",
    "\n",
    "\n",
    "        x = {\n",
    "            'input_ids': record['input_ids'],\n",
    "            'input_mask': record['input_mask'],\n",
    "            'segment_ids': record['segment_ids']\n",
    "        }\n",
    "        y = record['label_id']\n",
    "        return (x, y)\n",
    "    \n",
    "    dataset = dataset.map(_select_data_from_record)\n",
    "    if is_training:\n",
    "        dataset = dataset.shuffle(100)\n",
    "        dataset = dataset.repeat()\n",
    "    \n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(1024)\n",
    "    return dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1285\n"
     ]
    }
   ],
   "source": [
    "steps_per_loop = 1\n",
    "learning_rate=1e-5\n",
    "epochs=10\n",
    "\n",
    "train_batch_size=16\n",
    "eval_batch_size=16\n",
    "\n",
    "train_data_size = input_meta_data['train_data_size']\n",
    "steps_per_epoch = int(train_data_size / train_batch_size)\n",
    "print(steps_per_epoch)\n",
    "\n",
    "warmup_steps = int(epochs * train_data_size * 0.1 / train_batch_size)\n",
    "eval_steps = int(math.ceil(input_meta_data['eval_data_size'] / eval_batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = get_classifier_dataset(train_data_path,max_seq_length,train_batch_size,is_training=True)\n",
    "evaluation_dataset = get_classifier_dataset(eval_data_path,max_seq_length,eval_batch_size,is_training=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model code, graph creation\n",
    "This section is the core logic of the model, here we are using tensorflow hub url for albert model. Using hub simplifies the code a lot      \n",
    "It's a slightly simplied version of official bert code, that code have functionality to load a non hub model. \n",
    "#### refer to models/official/nlp/bert/run_classifier.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strategy = distribution_utils.get_distribution_strategy('mirrored',num_gpus=1)\n",
    "\n",
    "# strategy = tf.distribute.OneDeviceStrategy(\"device:GPU:2\")\n",
    "#since devices to use is set to 2 already, only 1 device is visible which is 0\n",
    "strategy = tf.distribute.OneDeviceStrategy(\"device:GPU:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vocab_size': 30000,\n",
       " 'hidden_size': 768,\n",
       " 'num_hidden_layers': 12,\n",
       " 'num_attention_heads': 12,\n",
       " 'hidden_act': 'gelu',\n",
       " 'intermediate_size': 3072,\n",
       " 'hidden_dropout_prob': 0,\n",
       " 'attention_probs_dropout_prob': 0,\n",
       " 'max_position_embeddings': 512,\n",
       " 'type_vocab_size': 2,\n",
       " 'initializer_range': 0.02,\n",
       " 'backward_compatible': True,\n",
       " 'embedding_size': 128,\n",
       " 'num_hidden_groups': 1,\n",
       " 'net_structure_type': 0,\n",
       " 'gap_size': 0,\n",
       " 'num_memory_blocks': 0,\n",
       " 'inner_group_num': 1,\n",
       " 'down_scale_factor': 1}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'vocab_size': 30000,\n",
    " 'hidden_size': 768,\n",
    " 'num_hidden_layers': 12,\n",
    " 'num_attention_heads': 12,\n",
    " 'hidden_act': 'gelu',\n",
    " 'intermediate_size': 3072,\n",
    " 'hidden_dropout_prob': 0,\n",
    " 'attention_probs_dropout_prob': 0,\n",
    " 'max_position_embeddings': 512,\n",
    " 'type_vocab_size': 2,\n",
    " 'initializer_range': 0.02,\n",
    " 'backward_compatible': True,\n",
    " 'embedding_size': 128,\n",
    " 'num_hidden_groups': 1,\n",
    " 'net_structure_type': 0,\n",
    " 'gap_size': 0,\n",
    " 'num_memory_blocks': 0,\n",
    " 'inner_group_num': 1,\n",
    " 'down_scale_factor': 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the graph\n",
    "It's just this function & your graph definition is done :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_albert_model(bert_config,num_labels,max_seq_length,hub_url):\n",
    "    \"\"\"This function return our classification model based on bert + original bert model\"\"\"\n",
    "    ##TODO check the name thing Note that name parameter specified here is the same as the feature name keys in the dictionary in tf.data dataset  \n",
    "    input_ids = tf.keras.Input(shape=(max_seq_length),dtype=tf.int32,name='input_ids')\n",
    "    input_mask = tf.keras.Input(shape=(max_seq_length),dtype=tf.int32,name='input_mask')\n",
    "    segment_ids = tf.keras.Input(shape=(max_seq_length),dtype=tf.int32,name='segment_ids')\n",
    "    \n",
    "    bert_model = hub.KerasLayer(hub_url,trainable=True,tags=None)\n",
    "    ### pooled_output will give the representation for [CLS]\n",
    "    ### sequence_output will give representations for all tokens\n",
    "    ##since it's classification task we will just use pooled_output\n",
    "    pooled_output, sequence_output = bert_model([input_ids, input_mask, segment_ids])\n",
    "    bert_output = tf.keras.layers.Dropout(rate=bert_config.hidden_dropout_prob)(pooled_output) \n",
    "    \n",
    "    initializer = tf.keras.initializers.TruncatedNormal(stddev=bert_config.initializer_range)\n",
    "    output = tf.keras.layers.Dense(num_labels,kernel_initializer=initializer,name='output',dtype=tf.float32)(bert_output)\n",
    "    \n",
    "    return tf.keras.Model(inputs={'input_ids':input_ids,\n",
    "                                 'input_mask':input_mask,\n",
    "                                 'segment_ids':segment_ids},\n",
    "                         outputs=output),bert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "[CLS_ID,SEP_ID]  = tfs.piece_to_id(['[CLS]','[SEP]'],model_proto=sp_model_proto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=2>"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CLS_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=int32, numpy=array([2, 3, 0], dtype=int32)>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfs.piece_to_id(['[CLS]','[SEP]','<pad>'],model_proto=sp_model_proto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.expand_dims(CLS_ID,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=int32, numpy=array([2, 2], dtype=int32)>"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.repeat(CLS_ID,repeats=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1), dtype=int32, numpy=\n",
       "array([[2],\n",
       "       [2]], dtype=int32)>"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.expand_dims(tf.repeat(CLS_ID,repeats=tf.shape(encoded.values)[0]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1), dtype=int32, numpy=\n",
       "array([[2],\n",
       "       [2]], dtype=int32)>"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.broadcast_to(CLS_ID,(tf.shape(encoded.values)[0],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=2>"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.shape(encoded.values)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.tile()\n",
    "tf.stack()\n",
    "tf.repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 7), dtype=int32, numpy=\n",
       "array([[4148,   48,   25,   21, 1289, 5123,    2],\n",
       "       [ 328, 1289, 5123,    0,    0,    0,    2]], dtype=int32)>"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.concat([encoded.values,tf.expand_dims(tf.repeat(CLS_ID,repeats=2),axis=1)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentencepieceEncodeDense(values=<tf.Tensor: shape=(2, 6), dtype=int32, numpy=\n",
       "array([[4148,   48,   25,   21, 1289, 5123],\n",
       "       [ 328, 1289, 5123,    0,    0,    0]], dtype=int32)>, sequence_length=<tf.Tensor: shape=(2,), dtype=int32, numpy=array([6, 3], dtype=int32)>)"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = tfs.encode(['hi this is a test sentence','next test sentence'],model_proto=sp_model_proto)\n",
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 6), dtype=int32, numpy=array([[4148,   48,   25,   21, 1289, 5123]], dtype=int32)>"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_single = tfs.encode(['next test sentence'],model_proto=sp_model_proto)\n",
    "tf.pad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sentences = ['hi this is a test sentence','next test sentence']\n",
    "e_sparse  = tfs.encode_sparse(sample_sentences,model_proto=sp_model_proto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 6), dtype=int32, numpy=\n",
       "array([[4148,   48,   25,   21, 1289, 5123],\n",
       "       [ 328, 1289, 5123,    0,    0,    0]], dtype=int32)>"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 6), dtype=string, numpy=\n",
       "array([[b'\\xe2\\x96\\x81hi', b'\\xe2\\x96\\x81this', b'\\xe2\\x96\\x81is',\n",
       "        b'\\xe2\\x96\\x81a', b'\\xe2\\x96\\x81test', b'\\xe2\\x96\\x81sentence'],\n",
       "       [b'\\xe2\\x96\\x81next', b'\\xe2\\x96\\x81test',\n",
       "        b'\\xe2\\x96\\x81sentence', b'<pad>', b'<pad>', b'<pad>']],\n",
       "      dtype=object)>"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfs.id_to_piece(encoded.values.numpy(),model_proto=sp_model_proto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[4148, 48, 25, 21, 1289, 5123], [328, 1289, 5123]]>"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp3 = tftext.SentencepieceTokenizer(model=sp_model_proto)\n",
    "encoded_text3 = sp3.tokenize(sample_sentences)\n",
    "encoded_text3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentencepieceEncodeDense(values=<tf.Tensor: shape=(2, 6), dtype=int32, numpy=\n",
       "array([[4148,   48,   25,   21, 1289, 5123],\n",
       "       [ 328, 1289, 5123,    0,    0,    0]], dtype=int32)>, sequence_length=<tf.Tensor: shape=(2,), dtype=int32, numpy=array([6, 3], dtype=int32)>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = tfs.encode(['hi this is a test sentence','next test sentence'],model_proto=sp_model_proto)\n",
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(1, 6), dtype=int32, numpy=array([[4148,   48,   25,   21, 1289, 5123]], dtype=int32)>,\n",
       " <tf.Tensor: shape=(1, 6), dtype=int32, numpy=array([[ 328, 1289, 5123,    0,    0,    0]], dtype=int32)>]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.split(encoded.values,num_or_size_splits=encoded.values.shape[0],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 8), dtype=int32, numpy=\n",
       "array([[   2, 4148,   48,   25,   21, 1289, 5123,    3],\n",
       "       [   2,  328, 1289, 5123,    0,    0,    0,    3]], dtype=int32)>"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatenated_values(CLS_ID,encoded.values,SEP_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenated_values(CLS_ID,encoded_values,SEP_ID):\n",
    "    batch_size = encoded_values.shape[0]\n",
    "#     Note that we are not using tf.shape to get the shape as it doesn't get the runtime shape\n",
    "#     batch_size = tf.shape(encoded_values)[0]\n",
    "    ##we need the CLS_ID to be prepended & SEP_ID to be appended to encoded_values from sentence_piece\n",
    "    ## the encoded_values have shape = (batch_size,sequence_length)\n",
    "    \n",
    "    ## to concat values we need all dimension except the ones where are concat is happening to be the same\n",
    "    ## here it is 2d with 1st dimension being batch_size, so getting tensors of shape (batch_size,1)\n",
    "    CLS_IDS = tf.broadcast_to(CLS_ID,(batch_size,1))\n",
    "    SEP_IDS = tf.broadcast_to(SEP_ID,(batch_size,1))\n",
    "#     or another way to get shape = (batch_size,1)\n",
    "#     CLS_IDS = tf.expand_dims(tf.repeat(CLS_ID,repeats=batch_size),axis=1)\n",
    "#     SEP_IDS = tf.expand_dims(tf.repeat(SEP_ID,repeats=batch_size),axis=1)\n",
    "\n",
    "    return tf.concat([CLS_IDS,encoded_values,SEP_IDS],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 8), dtype=int32, numpy=\n",
       "array([[   2, 4148,   48,   25,   21, 1289, 5123,    3],\n",
       "       [   2,  328, 1289, 5123,    0,    0,    0,    3]], dtype=int32)>"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatenated_values(CLS_ID,encoded.values,SEP_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'input_1:0' shape=(None, None, 10) dtype=float32>"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.keras.layers.Input((None, 10))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Shape:0' shape=(3,) dtype=int32>"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.shape(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, None, 10])"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.keras.utils.register_keras_serializable(package=\"Text\")\n",
    "\n",
    "class SentencepieceTokenization(tf.keras.layers.Layer):\n",
    "#     def __init__(self,model_proto):\n",
    "    def __init__(self,model_path):\n",
    "#         super(SentencepieceTokenization, self).__init__(trainable=False,dynamic=True,dtype=tf.int32)\n",
    "        super(SentencepieceTokenization, self).__init__(trainable=False,dtype=tf.int32)\n",
    "        #     self.sp_model_proto = model_proto\n",
    "#         self.model_path = tf.saved_model.Asset(model_path)\n",
    "        self.model_path = model_path\n",
    "        \n",
    "#         model_proto = tf.io.gfile.GFile(self.model_path, 'rb').read()\n",
    "#         self.sp_model = tftext.SentencepieceTokenizer(model=model_proto)\n",
    "    def build(self,input_shape):\n",
    "#         self.model_proto = tf.io.gfile.GFile(self.model_path.asset_path, 'rb').read()\n",
    "#         self.model_proto = tf.io.gfile.GFile(self.model_path, 'rb').read()\n",
    "#         self.sp_model = tftext.SentencepieceTokenizer(model=self.model_proto)\n",
    "        ## pad_id is usually 0 \n",
    "        [self.CLS_ID,self.SEP_ID,self.PAD_ID]  = tfs.piece_to_id(['[CLS]','[SEP]','<pad>'],model_file=self.model_path)\n",
    "        self.built=True\n",
    "        \n",
    "#     @tf.function    \n",
    "    def call(self, input_text):\n",
    "#         encoded_text = self.sp_model.tokenize(input_text).to_tensor()\n",
    "        ##tensorflow sentence piece works while exporting to graph while, tf_text sentencepiece doesn't\n",
    "#         encoded_text = tfs.encode(input_text,model_proto=self.model_proto)\n",
    "        model_proto = tf.io.gfile.GFile(self.model_path, 'rb').read()\n",
    "        encoded_text = self.get_encoded_text(input_text,model_proto=model_proto,max_sequence_length=20)\n",
    "        \n",
    "#         encoded_text = tf.RaggedTensor.from_tensor(encoded_text.values)\n",
    "#         return encoded_text.to_tensor()        \n",
    "        return encoded_text\n",
    "\n",
    "\n",
    "    def get_encoded_text(self,input_text_batch,model_proto,max_sequence_length):\n",
    "#         tf.print(\"input_text_batch is \",input_text_batch)\n",
    "#         tf.print(\"type of input_text_batch \",type(input_text_batch))\n",
    "\n",
    "        def process_invidual_line_encoding(x):\n",
    "#             tf sentencepiece requires a list as input, while the individual value that we get here\n",
    "#             is a single sentence, so adding one more dimension (i.e adding batch dimension = 1)\n",
    "            list_x = tf.expand_dims(x,axis=0)\n",
    "            sp_encoded = tfs.encode(list_x,model_proto=model_proto)\n",
    "    #         removing the batch dim with size=1 (which we added in the previous step)\n",
    "            values = tf.squeeze(sp_encoded.values,name='squeezed_values')\n",
    "            sequence_length = tf.squeeze(sp_encoded.sequence_length)\n",
    "#             tf.print('squeezed_values ' ,values)\n",
    "    #         trimming to max_length-2 (-2 to incorporate [CLS], [SEP])\n",
    "            trimmed_max_length = max_sequence_length-2\n",
    "    #         values_trimmed = tf.slice(values,begin=[0],size=[trimmed_max_length],name='trimmed_out')\n",
    "            values_trimmed = tf.cond(tf.greater(sequence_length,trimmed_max_length), \n",
    "                                    lambda : tf.slice(values,begin=[0],size=[trimmed_max_length],name='trimmed_out'),lambda : values)\n",
    "#             tf.print('values_trimmed ',values_trimmed)\n",
    "            concat = tf.concat([[self.CLS_ID],values_trimmed,[self.SEP_ID]],axis=0,name='concat_out')\n",
    "#             tf.print('concat_out ',concat)\n",
    "#             tf.print('size of concat ',tf.size(concat))\n",
    "#             tf.print('shape of concat ',tf.shape(concat))\n",
    "    #         actual_token_length = tf.size(concat) #this would also work since we are processing line by line in this function\n",
    "            actual_token_length = tf.shape(concat)[-1]\n",
    "    #         need not prepend anything so 0 for 1st entry in padding, \n",
    "    #         & next value for padding is how many dimensions required at the end of tensor\n",
    "            padded = tf.pad(concat,paddings=[[0,max_sequence_length-actual_token_length]],name='input_ids')\n",
    "    #         segment_ids = tf.zeros(shape=tf.shape(padded),dtype=tf.int32)\n",
    "    #         or\n",
    "            segment_ids = tf.zeros_like(padded,dtype=tf.int32,name='segment_ids')\n",
    "\n",
    "            input_mask = tf.scatter_nd(indices=tf.expand_dims(tf.range(0,actual_token_length),axis=1),\n",
    "                                       updates=tf.ones(shape=[actual_token_length],dtype=tf.int32),\n",
    "                                       shape=[max_sequence_length],name='input_mask')\n",
    "            return (padded,segment_ids,input_mask)\n",
    "    #     Issue running map_fn on gpu https://github.com/tensorflow/tensorflow/issues/28007 \n",
    "    #     https://www.tensorflow.org/api_docs/python/tf/device\n",
    "        with tf.device('/device:CPU:0'):\n",
    "            encoded = tf.map_fn(lambda x: process_invidual_line_encoding(x),input_text_batch,dtype=(tf.int32,tf.int32,tf.int32))\n",
    "#         tf.print('encoded map_fn ',encoded)\n",
    "#         tf.print('encoded map_fn type ',type(encoded))\n",
    "#         tf.print('encoded map_fn shape ',tf.shape(encoded))\n",
    "    #     stacked = tf.stack(encoded)\n",
    "        stacked = encoded\n",
    "#         tf.print('stacked size ',tf.size(stacked))\n",
    "#         tf.print('stacked shape ',tf.shape(stacked))\n",
    "        return stacked\n",
    "    \n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(SentencepieceTokenization, self).get_config()\n",
    "        config.update({'model_path': self.model_path})\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_fn(num_classes, loss_factor=1.0):\n",
    "    \"\"\"Gets the classification loss function.\"\"\"\n",
    "\n",
    "    def classification_loss_fn(labels, logits):\n",
    "        \"\"\"Classification loss.\"\"\"\n",
    "        labels = tf.squeeze(labels)\n",
    "        log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "        one_hot_labels = tf.one_hot(\n",
    "            tf.cast(labels, dtype=tf.int32), depth=num_classes, dtype=tf.float32)\n",
    "        per_example_loss = -tf.reduce_sum(\n",
    "            tf.cast(one_hot_labels, dtype=tf.float32) * log_probs, axis=-1)\n",
    "        #batch loss\n",
    "        loss = tf.reduce_mean(per_example_loss)\n",
    "        loss *= loss_factor\n",
    "        return loss\n",
    "    return classification_loss_fn  \n",
    "\n",
    "\n",
    "def metric_fn():\n",
    "    return tf.keras.metrics.SparseCategoricalAccuracy('test_accuracy',dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_keras_compile_fit(model_dir,\n",
    "                          strategy,\n",
    "                          model_fn,\n",
    "                          training_dataset,\n",
    "                          evaluation_dataset,\n",
    "                          loss_fn,\n",
    "                          metric_fn,\n",
    "                          init_checkpoint,\n",
    "                          epochs,\n",
    "                          steps_per_epoch,\n",
    "                          steps_per_loop,\n",
    "                          eval_steps,\n",
    "                          custom_callbacks=None):\n",
    "    \"\"\"Runs BERT classifier model using Keras compile/fit API.\"\"\"\n",
    "    ###slightly simplied version of official bert code \n",
    "    # refer to models/official/nlp/bert/run_classifier.py -   function run_keras_compile_fit\n",
    "    \n",
    "    with strategy.scope():\n",
    "        bert_model, sub_model = model_fn()\n",
    "        optimizer = bert_model.optimizer\n",
    "\n",
    "        if init_checkpoint:\n",
    "            checkpoint = tf.train.Checkpoint(model=sub_model)\n",
    "            checkpoint.restore(init_checkpoint).assert_existing_objects_matched()\n",
    "\n",
    "        bert_model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=loss_fn,\n",
    "            metrics=[metric_fn()])\n",
    "    #     ,experimental_steps_per_execution=steps_per_loop)\n",
    "\n",
    "        summary_dir = os.path.join(model_dir, 'summaries')\n",
    "        summary_callback = tf.keras.callbacks.TensorBoard(summary_dir)\n",
    "        checkpoint_path = os.path.join(model_dir, 'checkpoint')\n",
    "        checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "            checkpoint_path, save_weights_only=True)\n",
    "\n",
    "        if custom_callbacks is not None:\n",
    "            custom_callbacks += [summary_callback, checkpoint_callback]\n",
    "        else:\n",
    "            custom_callbacks = [summary_callback, checkpoint_callback]\n",
    "#       Note that we are only passing x & not y in fit function\n",
    "#       Refer to keras model.fit documentation for further details\n",
    "\n",
    "#       If `x` is a dataset, generator,\n",
    "#       or `keras.utils.Sequence` instance, `y` should\n",
    "#       not be specified (since targets will be obtained from `x`).\n",
    "        bert_model.fit(\n",
    "            x=training_dataset,\n",
    "            validation_data=evaluation_dataset,\n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            epochs=epochs,\n",
    "            validation_steps=eval_steps,\n",
    "            callbacks=custom_callbacks)\n",
    "\n",
    "        return bert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classifier_model,core_model = get_albert_model(bert_config=bert_config,\n",
    "                                                              num_labels=num_classes,\n",
    "                                                              max_seq_length=max_seq_length,\n",
    "                                                               hub_url=bert_hub_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'model_3',\n",
       " 'layers': [{'class_name': 'InputLayer',\n",
       "   'config': {'batch_input_shape': (None, 128),\n",
       "    'dtype': 'int32',\n",
       "    'sparse': False,\n",
       "    'ragged': False,\n",
       "    'name': 'input_ids'},\n",
       "   'name': 'input_ids',\n",
       "   'inbound_nodes': []},\n",
       "  {'class_name': 'InputLayer',\n",
       "   'config': {'batch_input_shape': (None, 128),\n",
       "    'dtype': 'int32',\n",
       "    'sparse': False,\n",
       "    'ragged': False,\n",
       "    'name': 'input_mask'},\n",
       "   'name': 'input_mask',\n",
       "   'inbound_nodes': []},\n",
       "  {'class_name': 'InputLayer',\n",
       "   'config': {'batch_input_shape': (None, 128),\n",
       "    'dtype': 'int32',\n",
       "    'sparse': False,\n",
       "    'ragged': False,\n",
       "    'name': 'segment_ids'},\n",
       "   'name': 'segment_ids',\n",
       "   'inbound_nodes': []},\n",
       "  {'class_name': 'KerasLayer',\n",
       "   'config': {'name': 'keras_layer_3',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'handle': 'https://tfhub.dev/tensorflow/albert_en_large/1'},\n",
       "   'name': 'keras_layer_3',\n",
       "   'inbound_nodes': [[['input_ids', 0, 0, {}],\n",
       "     ['input_mask', 0, 0, {}],\n",
       "     ['segment_ids', 0, 0, {}]]]},\n",
       "  {'class_name': 'Dropout',\n",
       "   'config': {'name': 'dropout_3',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'rate': 0,\n",
       "    'noise_shape': None,\n",
       "    'seed': None},\n",
       "   'name': 'dropout_3',\n",
       "   'inbound_nodes': [[['keras_layer_3', 0, 0, {}]]]},\n",
       "  {'class_name': 'Dense',\n",
       "   'config': {'name': 'output',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'units': 142,\n",
       "    'activation': 'linear',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'TruncatedNormal',\n",
       "     'config': {'mean': 0.0, 'stddev': 0.02, 'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'name': 'output',\n",
       "   'inbound_nodes': [[['dropout_3', 0, 0, {}]]]}],\n",
       " 'input_layers': {'input_ids': ['input_ids', 0, 0],\n",
       "  'input_mask': ['input_mask', 0, 0],\n",
       "  'segment_ids': ['segment_ids', 0, 0]},\n",
       " 'output_layers': [['output', 0, 0]]}"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_model.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_mask (InputLayer)         [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        [(None, 1024), (None 17683968    input_ids[0][0]                  \n",
      "                                                                 input_mask[0][0]                 \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 1024)         0           keras_layer[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 142)          145550      dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 17,829,518\n",
      "Trainable params: 17,829,518\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classifier_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'keras_layer_3',\n",
       " 'trainable': True,\n",
       " 'dtype': 'float32',\n",
       " 'handle': 'https://tfhub.dev/tensorflow/albert_en_large/1'}"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "core_model.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_mask (InputLayer)         [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer_2 (KerasLayer)      [(None, 1024), (None 17683968    input_ids[0][0]                  \n",
      "                                                                 input_mask[0][0]                 \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 1024)         0           keras_layer_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 142)          145550      dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 17,829,518\n",
      "Trainable params: 17,829,518\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "trained_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/var/extra/users/jgeorge/tf2.0/input/dish/models/albert_en_large'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1285/1285 [==============================] - 967s 753ms/step - loss: 4.0452 - test_accuracy: 0.1739 - val_loss: 2.8512 - val_test_accuracy: 0.3991\n"
     ]
    }
   ],
   "source": [
    "loss_multiplier = 1\n",
    "max_seq_length = input_meta_data['max_seq_length']\n",
    "num_classes = input_meta_data['num_labels']\n",
    "loss_fn = get_loss_fn(num_classes,loss_multiplier)\n",
    "epochs=1\n",
    "initial_lr = learning_rate\n",
    "def _get_classifier_model():\n",
    "    classifier_model,core_model = get_albert_model(bert_config=bert_config,\n",
    "                                                              num_labels=num_classes,\n",
    "                                                              max_seq_length=max_seq_length,\n",
    "                                                               hub_url=bert_hub_url)\n",
    "    ##This is basically Adam optimizer with weight decay after set no of warm up steps & \n",
    "    # before that an increasing learning rate from 0 to initial_lr\n",
    "    # refer to models/official/nlp/optimization.py for more details\n",
    "    classifier_model.optimizer = optimization.create_optimizer(init_lr=initial_lr,\n",
    "                                                               num_train_steps=steps_per_epoch*epochs,\n",
    "                                                               num_warmup_steps=warmup_steps)\n",
    "    return classifier_model,core_model\n",
    "\n",
    "trained_model = run_keras_compile_fit(model_dir=output_folder,strategy=strategy,model_fn=_get_classifier_model,\n",
    "                                     training_dataset=training_dataset,evaluation_dataset=evaluation_dataset,\n",
    "                                      loss_fn=loss_fn,metric_fn=metric_fn,\n",
    "                                     init_checkpoint=None,\n",
    "                                      epochs=epochs,\n",
    "                                      steps_per_epoch=steps_per_epoch,\n",
    "                                      steps_per_loop=steps_per_loop,\n",
    "                                      eval_steps=eval_steps\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelLookup(tf.keras.layers.Layer):\n",
    "#     def __init__(self,model_proto):\n",
    "    def __init__(self,filepath,default):\n",
    "        \"\"\" \n",
    "        filepath - path to the text file with 1 label per line\n",
    "        \"\"\"\n",
    "        super(LabelLookup, self).__init__(trainable=False,dtype=tf.int32)\n",
    "        self.filepath = filepath\n",
    "        self.default=default\n",
    "    def build(self,input_shape):\n",
    "        self.table = tf.lookup.StaticHashTable(\n",
    "            tf.lookup.TextFileInitializer(self.filepath,\n",
    "                                          key_dtype=tf.int64,key_index=tf.lookup.TextFileIndex.LINE_NUMBER,\n",
    "                                     value_dtype=tf.string,value_index=tf.lookup.TextFileIndex.WHOLE_LINE),self.default)\n",
    "        self.built=True\n",
    "        \n",
    "    def call(self, input_text):\n",
    "        word_ids = self.table.lookup(input_text)\n",
    "        return word_ids\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(LabelLookup, self).get_config()\n",
    "        config.update({'filepath': self.filepath})\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_albert_model_saving(bert_config,num_labels,max_seq_length,hub_url,checkpoint_path):\n",
    "    \"\"\"This function return our classification model based on bert + original bert model\"\"\"\n",
    "    ##TODO check the name thing Note that name parameter specified here is the same as the feeature name keys in the dictionary in tf.data dataset  \n",
    "#     input_ids = tf.keras.Input(shape=(max_seq_length),dtype=tf.int32,name='input_ids')\n",
    "#     input_mask = tf.keras.Input(shape=(max_seq_length),dtype=tf.int32,name='input_mask')\n",
    "#     segment_ids = tf.keras.Input(shape=(max_seq_length),dtype=tf.int32,name='segment_ids')\n",
    "    \n",
    "    #alternative way of giving input by specifying the entire text & do the preprocessing in the graph\n",
    "    input_text = tf.keras.Input(shape=(),dtype=tf.string,name='input_text')\n",
    "    sentence_piece_layer = SentencepieceTokenization(model_path=sentencepiece_path)\n",
    "    input_ids_processed,segment_ids_processed,input_mask_processed = sentence_piece_layer(input_text)\n",
    "    \n",
    "    \n",
    "    bert_model = hub.KerasLayer(hub_url,trainable=True,tags=None)\n",
    "    ### pooled_output will give the representation for [CLS]\n",
    "    ### sequence_output will give representations for all tokens\n",
    "    ##since it's classification task we will just use pooled_output\n",
    "    pooled_output, sequence_output = bert_model([input_ids_processed, input_mask_processed, segment_ids_processed])\n",
    "#     bert_output = tf.keras.layers.Dropout(rate=bert_config.hidden_dropout_prob)(pooled_output) \n",
    "    \n",
    "    initializer = tf.keras.initializers.TruncatedNormal(stddev=bert_config.initializer_range)\n",
    "#     output = tf.keras.layers.Dense(num_labels,kernel_initializer=initializer,name='output',dtype=tf.float32)(bert_output)\n",
    "    output = tf.keras.layers.Dense(num_labels,kernel_initializer=initializer,name='output',dtype=tf.float32)(pooled_output)\n",
    "    \n",
    "    model  = tf.keras.Model(inputs={'input_text':input_text},\n",
    "                         outputs=output)\n",
    "#     checkpoint = tf.train.Checkpoint(model=model)\n",
    "#     checkpoint.restore(checkpoint_path).assert_existing_objects_matched()\n",
    "    model.load_weights(checkpoint_path).assert_existing_objects_matched()\n",
    "    return model,bert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/var/extra/users/jgeorge/tf2.0/input/dish/models/albert_en_large'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_folder = output_folder\n",
    "checkpoint_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/var/extra/users/jgeorge/tf2.0/input/dish/models/albert_en_large/checkpoint'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tensorflow_hub.keras_layer.KerasLayer object at 0x7f7879336c50> and <tensorflow.python.keras.layers.core.Dense object at 0x7f7879336790>).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tensorflow_hub.keras_layer.KerasLayer object at 0x7f7879336c50> and <tensorflow.python.keras.layers.core.Dense object at 0x7f7879336790>).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tensorflow.python.keras.layers.core.Dense object at 0x7f7879336790> and <tensorflow.python.keras.engine.base_layer.TensorFlowOpLayer object at 0x7f787502d990>).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tensorflow.python.keras.layers.core.Dense object at 0x7f7879336790> and <tensorflow.python.keras.engine.base_layer.TensorFlowOpLayer object at 0x7f787502d990>).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f78793367d0>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_length = input_meta_data['max_seq_length']\n",
    "num_classes = input_meta_data['num_labels']\n",
    "num_labels = num_classes\n",
    "initial_lr = learning_rate\n",
    "default_label_index = 1\n",
    "default_label='other-other'\n",
    "checkpoint_path=tf.train.latest_checkpoint(checkpoint_folder)\n",
    "# classifier_model,core_model = get_albert_model_saving(bert_config=bert_config,\n",
    "#                                                               num_labels=num_classes,\n",
    "#                                                               max_seq_length=max_seq_length,\n",
    "#                                                                hub_url=bert_hub_url,checkpoint_path=tf.train.latest_checkpoint(checkpoint_folder))\n",
    "\n",
    "input_text = tf.keras.Input(shape=(),dtype=tf.string,name='input_text')\n",
    "sentence_piece_layer = SentencepieceTokenization(model_path=sentencepiece_path)\n",
    "input_ids_processed,segment_ids_processed,input_mask_processed = sentence_piece_layer(input_text)\n",
    "\n",
    "\n",
    "bert_model = hub.KerasLayer(bert_hub_url,trainable=True,tags=None)\n",
    "### pooled_output will give the representation for [CLS]\n",
    "### sequence_output will give representations for all tokens\n",
    "##since it's classification task we will just use pooled_output\n",
    "pooled_output, sequence_output = bert_model([input_ids_processed, input_mask_processed, segment_ids_processed])\n",
    "#     bert_output = tf.keras.layers.Dropout(rate=bert_config.hidden_dropout_prob)(pooled_output) \n",
    "\n",
    "initializer = tf.keras.initializers.TruncatedNormal(stddev=bert_config.initializer_range)\n",
    "#     output = tf.keras.layers.Dense(num_labels,kernel_initializer=initializer,name='output',dtype=tf.float32)(bert_output)\n",
    "# dense_output = tf.keras.layers.Dense(num_labels,kernel_initializer=initializer,name='output',dtype=tf.float32,activation='softmax')(pooled_output)\n",
    "dense_output = tf.keras.layers.Dense(num_labels,kernel_initializer=initializer,name='output',dtype=tf.float32)(pooled_output)\n",
    "# softmax_output = tf.nn.softmax()\n",
    "log_probs = tf.nn.log_softmax(dense_output, axis=-1)\n",
    "label_id = tf.argmax(log_probs,axis=-1)\n",
    "#need to convert label index to label name. So keeping line no as key & whole text line as label\n",
    "# issue with using table lookup (both file based & key value tensorbased) in tf2.2 (tf2.x) - https://github.com/tensorflow/tensorflow/issues/38305\n",
    "# table_init = tf.lookup.TextFileInitializer(intent_file,\n",
    "#                                       key_dtype=tf.int64,key_index=tf.lookup.TextFileIndex.LINE_NUMBER,\n",
    "#                                      value_dtype=tf.string,value_index=tf.lookup.TextFileIndex.WHOLE_LINE)\n",
    "# label_lookup = tf.lookup.StaticHashTable(table_init\n",
    "#                                         ,default_value=default_label)\n",
    "# label_lookup.init.run()\n",
    "# label_lookup.initialize()\n",
    "\n",
    "\n",
    "# label = label_lookup.lookup(label_id)\n",
    "\n",
    "# table_init = tf.lookup.KeyValueTensorInitializer(range(len(intent_list)),intent_list,\n",
    "#                                                 key_dtype=tf.int64,value_dtype=tf.string)\n",
    "# label_lookup = tf.lookup.StaticHashTable(table_init\n",
    "#                                         ,default_value=default_label)\n",
    "# label = label_lookup.lookup(label_id)\n",
    "\n",
    "label = LabelLookup(filepath=intent_file,default='other-other')(label_id)\n",
    "\n",
    "loaded_model  = tf.keras.Model(inputs={'input_text':input_text},\n",
    "                     outputs=label)\n",
    "# loaded_model.compile()\n",
    "#     checkpoint = tf.train.Checkpoint(model=model)\n",
    "#     checkpoint.restore(checkpoint_path).assert_existing_objects_matched()\n",
    "loaded_model.load_weights(checkpoint_path).assert_existing_objects_matched()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensorflow.python.ops.lookup_ops.StaticHashTable,\n",
       " tensorflow.python.ops.lookup_ops.InitializableLookupTableBase,\n",
       " tensorflow.python.ops.lookup_ops.LookupInterface,\n",
       " tensorflow.python.training.tracking.tracking.TrackableResource,\n",
       " tensorflow.python.training.tracking.tracking.CapturableResource,\n",
       " tensorflow.python.training.tracking.base.Trackable,\n",
       " object]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.lookup.StaticHashTable.mro()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['account-address_change',\n",
       " 'account-cancel',\n",
       " 'account-name_change',\n",
       " 'account-transfer',\n",
       " 'account-vague',\n",
       " 'account_cancel-fee',\n",
       " 'account_cancel-promotion_seek',\n",
       " 'account_cancel-query',\n",
       " 'account_info-get_help',\n",
       " 'app-issue',\n",
       " 'app-query',\n",
       " 'appointment-cancel',\n",
       " 'appointment-query',\n",
       " 'appointment-reschedule',\n",
       " 'appointment-schedule',\n",
       " 'appointment-tech_get_help',\n",
       " 'authorization-get_help',\n",
       " 'autopay-cancel',\n",
       " 'autopay-get_help',\n",
       " 'autopay-query',\n",
       " 'autopay-setup',\n",
       " 'billing-balance_query',\n",
       " 'billing-issue',\n",
       " 'billing-preferences',\n",
       " 'billing-query',\n",
       " 'billing-vague',\n",
       " 'blue_black_snowy-issue',\n",
       " 'bundle-query',\n",
       " 'channel-guide',\n",
       " 'channel-list',\n",
       " 'channel_package-cancel',\n",
       " 'channel_package-change',\n",
       " 'channel_package-issue',\n",
       " 'channel_package-query',\n",
       " 'channel_package-upgrade',\n",
       " 'channel_package-vague',\n",
       " 'channel_price-query',\n",
       " 'comp_part_signal_loss-issue',\n",
       " 'contract-cancel_charge',\n",
       " 'contract-query',\n",
       " 'contract_expiry-query',\n",
       " 'dish_move-query',\n",
       " 'dish_pause-cancel',\n",
       " 'dish_pause-query',\n",
       " 'dish_protection-query',\n",
       " 'dvr-get_help',\n",
       " 'dvr-query',\n",
       " 'equipment-add_receiver',\n",
       " 'equipment-issue',\n",
       " 'equipment-place_order',\n",
       " 'equipment-price_query',\n",
       " 'equipment-query',\n",
       " 'equipment-setup',\n",
       " 'equipment-upgrade',\n",
       " 'equipment-vague',\n",
       " 'equipment_setup-get_help',\n",
       " 'fee-waive_charge',\n",
       " 'game_popup-issue',\n",
       " 'get-help',\n",
       " 'greet-goodbye',\n",
       " 'greet-hello',\n",
       " 'greet-thank_you',\n",
       " 'hbocinemax-takedown',\n",
       " 'hopper-issues',\n",
       " 'installation-vague',\n",
       " 'internet-cancel',\n",
       " 'internet-check_availability',\n",
       " 'internet-get_help',\n",
       " 'internet-price_query',\n",
       " 'internet-query',\n",
       " 'internet_data-query',\n",
       " 'joey-issues',\n",
       " 'joey-vague',\n",
       " 'movie-place_order',\n",
       " 'movie_order-get_help',\n",
       " 'no-give',\n",
       " 'on_demand-query',\n",
       " 'online_account-forgot_id_password',\n",
       " 'online_account-forgot_login',\n",
       " 'online_account-forgot_password',\n",
       " 'online_account-login_locked',\n",
       " 'online_account-password_reset',\n",
       " 'online_account-setup',\n",
       " 'online_account-unable_login',\n",
       " 'order-cancel',\n",
       " 'order-get_help',\n",
       " 'order-query',\n",
       " 'order-status',\n",
       " 'other-other',\n",
       " 'payment-get_help',\n",
       " 'payment-make',\n",
       " 'payment-query',\n",
       " 'payment-reinstate_service',\n",
       " 'payment-status',\n",
       " 'payment-vague',\n",
       " 'payment_arrangement-vague',\n",
       " 'payment_extension-already_done',\n",
       " 'payment_extension-request',\n",
       " 'payment_schedule-query',\n",
       " 'ppv-cancel',\n",
       " 'ppv-issue',\n",
       " 'ppv-order',\n",
       " 'ppv-query',\n",
       " 'price-increase',\n",
       " 'price-vague',\n",
       " 'promotion-get_help',\n",
       " 'promotion-query',\n",
       " 'reactivate-reinstate',\n",
       " 'receiver-activate',\n",
       " 'receiver-get_help',\n",
       " 'receiver-place_order',\n",
       " 'receiver-query',\n",
       " 'receiver-remove',\n",
       " 'receiver-replacement',\n",
       " 'receiver_issue-get_help',\n",
       " 'receiver_upgrade-query',\n",
       " 'recording-issue',\n",
       " 'recording-query',\n",
       " 'redeem_offer-get_help',\n",
       " 'redeem_offer-query',\n",
       " 'referral-code_not_received',\n",
       " 'referral-how_to',\n",
       " 'refund-get_help',\n",
       " 'refund-query',\n",
       " 'refund-vague',\n",
       " 'remote-not_working',\n",
       " 'remote-order_status',\n",
       " 'remote-place_order',\n",
       " 'remote-setup',\n",
       " 'remote-vague',\n",
       " 'renewal-query',\n",
       " 'representative-request',\n",
       " 'return-get_help',\n",
       " 'return-query',\n",
       " 'return-status',\n",
       " 'security_code-query',\n",
       " 'spanish-query',\n",
       " 'streaming-issues',\n",
       " 'streaming-query',\n",
       " 'tech-issue',\n",
       " 'univision-query',\n",
       " 'yes-give']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intent_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'sentencepiece_tokenization_2/Identity:0' shape=(None, None) dtype=int32>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'word_embeddings/embeddings:0' shape=(30000, 128) dtype=float32, numpy=\n",
       " array([[ 0.05184961,  0.02554743,  0.07820095, ...,  0.13242374,\n",
       "         -0.11135858, -0.08149511],\n",
       "        [-0.02176668,  0.08186491, -0.03806052, ..., -0.01067084,\n",
       "          0.08668516,  0.07543599],\n",
       "        [ 0.0389083 , -0.04519133,  0.02249125, ...,  0.01007368,\n",
       "         -0.01035299,  0.01529716],\n",
       "        ...,\n",
       "        [-0.05189878,  0.05257143, -0.06086004, ...,  0.02348912,\n",
       "         -0.03792891,  0.11682445],\n",
       "        [-0.08751736,  0.01797831,  0.04328997, ..., -0.02513361,\n",
       "         -0.02745478, -0.05742126],\n",
       "        [-0.0004206 ,  0.0100141 , -0.10399412, ...,  0.02441824,\n",
       "         -0.13680063, -0.0241577 ]], dtype=float32)>,\n",
       " <tf.Variable 'position_embedding/embeddings:0' shape=(512, 128) dtype=float32, numpy=\n",
       " array([[ 0.04160105, -0.05979723,  0.00529569, ..., -0.00559605,\n",
       "         -0.006269  ,  0.01913644],\n",
       "        [ 0.01773208, -0.00867868,  0.01477922, ..., -0.2193397 ,\n",
       "         -0.02091319,  0.05058509],\n",
       "        [-0.02857368, -0.0150751 ,  0.02078645, ..., -0.16699198,\n",
       "         -0.03039407,  0.06745994],\n",
       "        ...,\n",
       "        [ 0.05314923,  0.0010823 ,  0.01054795, ..., -0.11002688,\n",
       "          0.00282836, -0.01345524],\n",
       "        [ 0.0256253 , -0.06600939, -0.01717645, ..., -0.13026012,\n",
       "         -0.00394599,  0.0430429 ],\n",
       "        [ 0.02484118, -0.03633421, -0.03465232, ..., -0.08623239,\n",
       "         -0.00122309,  0.00660774]], dtype=float32)>,\n",
       " <tf.Variable 'type_embeddings/embeddings:0' shape=(2, 128) dtype=float32, numpy=\n",
       " array([[ 0.02987524,  0.00401912,  0.0095237 ,  0.01743334,  0.02235013,\n",
       "          0.00437002, -0.00267635, -0.03190668, -0.00559026,  0.01673611,\n",
       "         -0.01215199,  0.02899342,  0.0191346 , -0.00330778, -0.00999817,\n",
       "          0.00042365,  0.07514642, -0.01144328, -0.00524436,  0.00516515,\n",
       "         -0.014307  ,  0.00484243,  0.00467683, -0.00040814,  0.00341662,\n",
       "         -0.01212941,  0.00565656, -0.00621834,  0.01125331, -0.00433797,\n",
       "         -0.02057247, -0.01661093, -0.0119646 , -0.00403616,  0.02685729,\n",
       "         -0.09081781, -0.01550122, -0.01048156, -0.00548974,  0.01041744,\n",
       "          0.06273306, -0.00047157,  0.00526512, -0.00127566, -0.00672255,\n",
       "         -0.00672735, -0.02150482,  0.00564766, -0.01554017, -0.01056029,\n",
       "         -0.00256703,  0.02311023,  0.01017125, -0.00244417, -0.00480226,\n",
       "          0.0023977 , -0.00427377, -0.00434145, -0.00829907,  0.01664159,\n",
       "         -0.00395029,  0.01022531,  0.00990237, -0.00872123, -0.00885494,\n",
       "          0.01478426, -0.00558476, -0.01077561, -0.0007042 ,  0.00194362,\n",
       "         -0.00793404,  0.00879662, -0.01714475,  0.00217758,  0.00122889,\n",
       "         -0.00477201, -0.0065162 , -0.00040997, -0.04801662, -0.01045263,\n",
       "         -0.04906296, -0.00536092,  0.00722547,  0.01657146,  0.00530316,\n",
       "          0.01483216, -0.01128762, -0.00379048,  0.00215045,  0.01257507,\n",
       "         -0.01889868,  0.00194182, -0.01128105, -0.00697749, -0.00403871,\n",
       "          0.00664912, -0.01789072, -0.00163299,  0.0096326 ,  0.03343866,\n",
       "         -0.01306755,  0.02090225,  0.03926821, -0.0030564 , -0.02172961,\n",
       "         -0.00660755,  0.01139761, -0.00764894,  0.00086532,  0.01742348,\n",
       "          0.0026752 ,  0.0023647 , -0.01338386, -0.00076124, -0.01229224,\n",
       "         -0.01394619, -0.00772771,  0.06364863,  0.01096072, -0.00490674,\n",
       "         -0.0122852 , -0.00388034,  0.01314738, -0.01789087,  0.05869711,\n",
       "          0.00033649, -0.01050628, -0.00400885],\n",
       "        [ 0.00951255,  0.00264095,  0.00581716,  0.00410335,  0.01554073,\n",
       "          0.00155865, -0.00531315, -0.0486992 ,  0.01001061,  0.0275503 ,\n",
       "         -0.01856712,  0.02733872, -0.01220564, -0.00409245, -0.01378252,\n",
       "          0.02417263,  0.0424455 , -0.00400096, -0.03358102, -0.01337754,\n",
       "          0.00209014,  0.0083914 ,  0.01903611, -0.01897969, -0.01052155,\n",
       "         -0.00292968, -0.00483444, -0.00320132,  0.00730183,  0.00939251,\n",
       "         -0.01223756, -0.00878278, -0.03004627,  0.00201226, -0.0283811 ,\n",
       "          0.0882051 , -0.01944843, -0.00362339, -0.00534452,  0.00465307,\n",
       "          0.03759826,  0.02692185, -0.01060383, -0.01799456, -0.00403827,\n",
       "          0.00252116, -0.01058688,  0.00853458, -0.02458911, -0.01007137,\n",
       "         -0.01323183,  0.02515492,  0.0021419 , -0.02518671,  0.01158138,\n",
       "          0.00251912,  0.00623424, -0.00763583, -0.0002046 ,  0.02412642,\n",
       "         -0.01792274, -0.01551224,  0.00407362,  0.00840264,  0.01201278,\n",
       "          0.02824499,  0.01818276, -0.00939074,  0.01009864, -0.00292176,\n",
       "         -0.00900582,  0.02283668, -0.01499937,  0.00130755, -0.00663957,\n",
       "          0.00384603, -0.01983928,  0.00238754, -0.09609843, -0.00700378,\n",
       "         -0.04753551, -0.00761724,  0.00506977,  0.03132378,  0.00143958,\n",
       "          0.02421383, -0.00658738,  0.00736526, -0.00423964, -0.00053276,\n",
       "          0.01396377, -0.01487834, -0.01020603, -0.0028683 ,  0.00351243,\n",
       "         -0.01290814, -0.01116716, -0.00046081, -0.00557255,  0.02389654,\n",
       "         -0.01345766,  0.02728853,  0.0351985 ,  0.00053739,  0.05083071,\n",
       "         -0.00119851,  0.003093  , -0.00084562, -0.01434198, -0.02998802,\n",
       "         -0.02008529,  0.00151968, -0.00806379, -0.00130424, -0.00750031,\n",
       "         -0.02583344, -0.01281969,  0.06313882,  0.00421329,  0.00013549,\n",
       "         -0.01175434, -0.02000248, -0.00588495, -0.00521331,  0.01010471,\n",
       "          0.02438281, -0.02614382,  0.01847476]], dtype=float32)>,\n",
       " <tf.Variable 'embeddings/layer_norm/gamma:0' shape=(128,) dtype=float32, numpy=\n",
       " array([1.784238  , 1.8911074 , 1.8905034 , 1.9057425 , 1.8650321 ,\n",
       "        1.8636343 , 1.7877816 , 1.8195857 , 1.8656446 , 1.2655708 ,\n",
       "        1.9069104 , 1.3946952 , 1.8374404 , 1.8023626 , 1.9435433 ,\n",
       "        1.7843491 , 1.2114031 , 1.8846418 , 1.7030636 , 1.9192235 ,\n",
       "        1.7903665 , 2.0551925 , 1.74248   , 1.8133804 , 1.9753551 ,\n",
       "        1.989415  , 1.8258712 , 1.8483336 , 1.763082  , 1.8771491 ,\n",
       "        1.7475238 , 1.9329997 , 1.7697673 , 1.7356033 , 1.3468678 ,\n",
       "        0.78101933, 1.8969817 , 1.7009562 , 1.5682787 , 1.7086339 ,\n",
       "        1.1830001 , 1.6317121 , 1.913202  , 1.8163817 , 1.6605611 ,\n",
       "        1.8390589 , 1.8502692 , 1.8124421 , 1.9104227 , 1.8153015 ,\n",
       "        1.857881  , 1.8622516 , 1.8423904 , 1.8103615 , 1.9763014 ,\n",
       "        1.7104832 , 1.8234707 , 1.204974  , 1.7138005 , 2.0394995 ,\n",
       "        1.9105498 , 1.7087021 , 1.7348415 , 1.6919541 , 1.8145578 ,\n",
       "        1.816033  , 1.8895462 , 1.8128321 , 1.7113    , 1.8947492 ,\n",
       "        1.7440127 , 1.4704036 , 1.698051  , 1.5989746 , 1.8759258 ,\n",
       "        1.9134552 , 1.750476  , 1.9095942 , 1.0195127 , 1.8230925 ,\n",
       "        1.2226448 , 1.9014497 , 1.8833795 , 1.2149465 , 1.9334767 ,\n",
       "        1.9730097 , 1.7325569 , 1.8963674 , 1.8866736 , 1.4662828 ,\n",
       "        1.677325  , 1.7258406 , 1.7485677 , 1.7495961 , 1.9264982 ,\n",
       "        1.8111265 , 1.9744962 , 1.852702  , 1.470927  , 1.8730377 ,\n",
       "        1.8827767 , 1.346964  , 1.20589   , 1.7344401 , 1.2985107 ,\n",
       "        1.2589256 , 1.9948875 , 1.8720955 , 1.8002546 , 1.3359108 ,\n",
       "        1.7856027 , 1.811178  , 1.8101059 , 1.8091516 , 1.703375  ,\n",
       "        1.8165079 , 1.8748785 , 1.1074941 , 1.9313413 , 1.7621762 ,\n",
       "        1.7592689 , 1.755607  , 1.7710795 , 1.8012387 , 1.1473479 ,\n",
       "        1.1444027 , 1.833286  , 1.7557659 ], dtype=float32)>,\n",
       " <tf.Variable 'embeddings/layer_norm/beta:0' shape=(128,) dtype=float32, numpy=\n",
       " array([-0.58431846,  0.24215072, -0.12441287,  0.07130426, -0.3501481 ,\n",
       "         0.16602772, -0.30258954,  0.6489783 , -0.30628097, -0.36525714,\n",
       "         0.16850038, -0.4376622 ,  0.12390139,  0.33461693,  0.01217765,\n",
       "        -0.29078212, -1.1395822 ,  0.11720613,  0.25817418,  0.28460678,\n",
       "         0.03821912,  0.46570972, -0.35280418,  0.3546364 ,  0.86360157,\n",
       "        -0.1620212 ,  0.11301306,  0.05148577,  0.22025247,  0.07926475,\n",
       "         0.53494644, -0.03765855,  0.548139  ,  0.34827116,  0.28900385,\n",
       "         0.06873418,  0.16723868, -0.21325539,  0.07591271,  0.03080936,\n",
       "        -1.4614547 , -0.4167677 ,  0.28249553,  0.1962387 ,  0.4725739 ,\n",
       "         0.21561034, -0.18333909, -0.50378346,  0.39413366,  0.3555976 ,\n",
       "        -0.14059944,  0.04356432,  0.04219358,  0.04059717,  0.26725903,\n",
       "         0.0665924 ,  0.21405374,  0.3850737 , -0.04258903,  0.04373862,\n",
       "         0.5361473 ,  0.00830258, -0.08908978, -0.13961788, -0.01114026,\n",
       "        -0.15046547,  0.03000396,  0.12461947, -0.2386309 ,  0.26002762,\n",
       "         0.06479163, -0.34243295,  0.12502424, -0.31145695, -0.05658039,\n",
       "         0.32144162,  0.3296594 ,  0.25999135,  1.2609555 , -0.00445405,\n",
       "         0.5341016 , -0.06292585, -0.0542938 , -0.525745  ,  0.10482913,\n",
       "        -0.24676107,  0.17760146, -0.21457657, -0.04889537,  0.19378927,\n",
       "        -0.00373382, -0.03949284,  0.29388705,  0.0850191 , -0.03935042,\n",
       "        -0.12671012,  0.04977922,  0.14742811, -0.05298305, -0.5433303 ,\n",
       "         0.24022229, -0.26321164, -0.8369609 , -0.05908672, -0.03545802,\n",
       "         0.03379943,  0.46842477, -0.31233716,  0.2927743 ,  0.207562  ,\n",
       "         0.02672779, -0.07318676, -0.23444661,  0.365884  ,  0.5936579 ,\n",
       "         0.39844182,  0.11542974, -1.1620063 , -0.13978428, -0.11084779,\n",
       "        -0.07988358,  0.5730211 ,  0.03196151,  0.2450798 , -0.72575706,\n",
       "        -0.31196836,  0.36211893, -0.02703617], dtype=float32)>,\n",
       " <tf.Variable 'embedding_projection/kernel:0' shape=(128, 1024) dtype=float32, numpy=\n",
       " array([[-0.03345976, -0.01151603, -0.01304341, ...,  0.03367984,\n",
       "          0.03176067, -0.07044788],\n",
       "        [-0.01972511, -0.01842294, -0.00096918, ...,  0.00270559,\n",
       "          0.01310278, -0.05456224],\n",
       "        [-0.00570585, -0.00553138,  0.00897587, ..., -0.00020782,\n",
       "          0.00870874, -0.00464339],\n",
       "        ...,\n",
       "        [ 0.03775604,  0.01286366,  0.03730331, ...,  0.0463453 ,\n",
       "         -0.02289711, -0.02597073],\n",
       "        [ 0.05102894,  0.04739675, -0.03011082, ...,  0.0345526 ,\n",
       "         -0.00741202, -0.01038355],\n",
       "        [ 0.01264559, -0.01609442,  0.00261487, ..., -0.02841979,\n",
       "         -0.00545128, -0.00485017]], dtype=float32)>,\n",
       " <tf.Variable 'embedding_projection/bias:0' shape=(1024,) dtype=float32, numpy=\n",
       " array([ 0.01379894,  0.0859147 ,  0.07090439, ..., -0.04455978,\n",
       "         0.01015603,  0.02803007], dtype=float32)>,\n",
       " <tf.Variable 'transformer/self_attention/query/kernel:0' shape=(1024, 16, 64) dtype=float32, numpy=\n",
       " array([[[-0.01305808,  0.07364346, -0.04479966, ..., -0.05771842,\n",
       "          -0.03491592, -0.01497703],\n",
       "         [ 0.07869475, -0.01202113, -0.02025816, ..., -0.02911681,\n",
       "          -0.03477065,  0.09679548],\n",
       "         [-0.06109387,  0.01023928, -0.08890238, ...,  0.04619653,\n",
       "          -0.08239103, -0.02242596],\n",
       "         ...,\n",
       "         [-0.0224205 , -0.07426018, -0.09119964, ..., -0.02319775,\n",
       "          -0.00233577, -0.02608563],\n",
       "         [-0.06488198,  0.05081485,  0.06164851, ..., -0.00844049,\n",
       "           0.01373386, -0.00616033],\n",
       "         [ 0.00748381, -0.00675483,  0.00990584, ..., -0.07239372,\n",
       "          -0.01741917,  0.07919498]],\n",
       " \n",
       "        [[-0.08133779, -0.04441871, -0.0153372 , ..., -0.12136034,\n",
       "           0.04688136, -0.02422284],\n",
       "         [-0.03188413, -0.00149011, -0.01040214, ...,  0.00908756,\n",
       "           0.05555762,  0.00100455],\n",
       "         [-0.02396845,  0.03597059, -0.02242527, ...,  0.06836001,\n",
       "           0.04673126,  0.00394613],\n",
       "         ...,\n",
       "         [-0.04556946, -0.01128064, -0.06345017, ..., -0.00144592,\n",
       "          -0.02358146, -0.01037056],\n",
       "         [-0.03967742,  0.03220677, -0.024954  , ..., -0.00911395,\n",
       "           0.10110556,  0.02565271],\n",
       "         [-0.00912704,  0.02193622, -0.01474701, ...,  0.01886166,\n",
       "          -0.01533103,  0.04230545]],\n",
       " \n",
       "        [[-0.03600834, -0.03587236,  0.07545744, ...,  0.11947948,\n",
       "           0.01678071,  0.0025643 ],\n",
       "         [-0.00891783, -0.03241969,  0.04335005, ...,  0.05258056,\n",
       "           0.02384821,  0.02036062],\n",
       "         [-0.01008803,  0.06062868,  0.01904355, ..., -0.03412808,\n",
       "          -0.12894209,  0.00096603],\n",
       "         ...,\n",
       "         [-0.04350172,  0.0431339 , -0.06519169, ...,  0.0071992 ,\n",
       "           0.0124436 , -0.11264576],\n",
       "         [-0.08143827, -0.04018426, -0.02538418, ...,  0.04103369,\n",
       "           0.02017873, -0.01357513],\n",
       "         [-0.02453474,  0.06444684, -0.02890934, ..., -0.05902385,\n",
       "          -0.05214071,  0.00163646]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[-0.03204974, -0.07303525, -0.05254488, ..., -0.01980535,\n",
       "          -0.09058508, -0.10201614],\n",
       "         [ 0.04988991,  0.02094498, -0.03876654, ..., -0.00483671,\n",
       "          -0.0561576 ,  0.1321026 ],\n",
       "         [-0.03450627, -0.03052652, -0.01419363, ...,  0.02704313,\n",
       "          -0.05218493,  0.03557277],\n",
       "         ...,\n",
       "         [ 0.06933472,  0.00527673, -0.08321609, ..., -0.03634268,\n",
       "          -0.00220063, -0.02179569],\n",
       "         [ 0.02518762,  0.02561204, -0.07307518, ...,  0.04900375,\n",
       "           0.02438294, -0.00780029],\n",
       "         [ 0.02440577, -0.06702448, -0.02715798, ..., -0.00644845,\n",
       "          -0.02366737, -0.00801914]],\n",
       " \n",
       "        [[ 0.02301747, -0.11584006, -0.00555029, ...,  0.06881443,\n",
       "           0.00073429,  0.03573141],\n",
       "         [-0.02142981, -0.01733049, -0.0478464 , ...,  0.04590307,\n",
       "           0.02238515,  0.02093671],\n",
       "         [-0.03307306,  0.03826388,  0.00900178, ...,  0.0026802 ,\n",
       "           0.01865537, -0.08554617],\n",
       "         ...,\n",
       "         [ 0.04458208, -0.0552475 ,  0.18399827, ..., -0.06701726,\n",
       "           0.00316772, -0.00961873],\n",
       "         [ 0.03825528, -0.00516968, -0.00745279, ..., -0.06133492,\n",
       "          -0.02598863,  0.01186869],\n",
       "         [-0.03812407, -0.00996027,  0.02947854, ...,  0.0246892 ,\n",
       "          -0.04885544,  0.01204717]],\n",
       " \n",
       "        [[ 0.01848724,  0.02079326, -0.06911898, ...,  0.06594574,\n",
       "           0.00293093,  0.03374371],\n",
       "         [-0.07371383, -0.01791303,  0.02850021, ..., -0.03742529,\n",
       "          -0.02329437,  0.05924642],\n",
       "         [ 0.01531304,  0.0024405 ,  0.03941339, ...,  0.11953703,\n",
       "          -0.0575078 ,  0.01585094],\n",
       "         ...,\n",
       "         [-0.02529871,  0.03300816,  0.05099135, ..., -0.04773176,\n",
       "           0.05956319,  0.00113708],\n",
       "         [-0.09593632,  0.04523377,  0.0261396 , ..., -0.11270167,\n",
       "          -0.10433777,  0.04245922],\n",
       "         [ 0.03623733, -0.05500538,  0.02787429, ..., -0.01153349,\n",
       "          -0.00538313,  0.00548494]]], dtype=float32)>,\n",
       " <tf.Variable 'transformer/self_attention/query/bias:0' shape=(16, 64) dtype=float32, numpy=\n",
       " array([[ 2.45376539e+00,  4.74478751e-02, -1.27161264e-01, ...,\n",
       "         -3.47480774e-01, -2.13164657e-01,  1.15052134e-01],\n",
       "        [ 9.08090472e-02,  2.12105992e-03,  2.07837269e-01, ...,\n",
       "         -1.40766370e+00,  8.31531733e-03, -8.12099874e-03],\n",
       "        [ 5.74346304e-01,  5.32688737e-01, -1.39216334e-01, ...,\n",
       "          1.49161294e-01, -1.08963691e-01,  5.08350059e-02],\n",
       "        ...,\n",
       "        [ 1.98263854e-01,  1.18225373e-01, -1.37172621e-02, ...,\n",
       "          2.18417093e-01, -1.01096500e-02, -2.01218233e-01],\n",
       "        [ 1.70852900e-01, -4.32008579e-02,  6.08182438e-02, ...,\n",
       "         -2.54631132e-01, -4.62922491e-02, -2.83184707e-01],\n",
       "        [ 2.12350875e-01,  3.77651006e-01,  3.82532001e-01, ...,\n",
       "         -4.46428090e-01, -2.10090563e-01,  3.62432301e-01]], dtype=float32)>,\n",
       " <tf.Variable 'transformer/self_attention/key/kernel:0' shape=(1024, 16, 64) dtype=float32, numpy=\n",
       " array([[[ 2.47821417e-02, -8.39557685e-03,  9.47190672e-02, ...,\n",
       "           7.92201161e-02, -1.29405558e-02,  1.81492865e-02],\n",
       "         [ 7.31229829e-03, -1.96689703e-02,  2.09270008e-02, ...,\n",
       "          -1.55713968e-02, -4.12303582e-03,  1.11330664e-02],\n",
       "         [ 6.99726045e-02,  1.33993023e-03, -5.54048233e-02, ...,\n",
       "          -4.44759205e-02,  1.31418910e-02,  1.18424995e-02],\n",
       "         ...,\n",
       "         [ 3.95523049e-02, -7.87695944e-02,  3.48423459e-02, ...,\n",
       "           6.19233996e-02, -6.31853566e-02, -1.80743355e-02],\n",
       "         [-3.08786556e-02,  3.60678211e-02,  7.02643245e-02, ...,\n",
       "          -1.25492364e-01,  3.51977013e-02,  1.07286125e-02],\n",
       "         [-5.79076295e-04,  7.31656179e-02, -2.37863399e-02, ...,\n",
       "          -3.84584405e-02,  1.63575225e-02,  2.05390956e-02]],\n",
       " \n",
       "        [[ 2.01652870e-02, -9.31952521e-03, -5.92457876e-02, ...,\n",
       "          -4.47020940e-02,  1.01465243e-03, -3.02275959e-02],\n",
       "         [ 1.94437727e-02,  2.34157946e-02,  3.30996662e-02, ...,\n",
       "           3.47309224e-02, -3.10732797e-02,  4.24472569e-03],\n",
       "         [-4.91201319e-03,  7.11822063e-02, -4.04558070e-02, ...,\n",
       "          -9.24846232e-02, -5.51202409e-02, -1.68074239e-02],\n",
       "         ...,\n",
       "         [ 1.46348970e-02,  2.30095107e-02, -1.25155309e-02, ...,\n",
       "           7.64800534e-02, -7.74956495e-02, -9.11033973e-02],\n",
       "         [-8.49685147e-02,  2.60965228e-02, -9.10297036e-02, ...,\n",
       "          -6.28949031e-02,  4.46696058e-02,  3.17028863e-03],\n",
       "         [-1.44494008e-02, -1.91296823e-02, -6.99478551e-05, ...,\n",
       "           2.87068710e-02, -3.27759646e-02,  1.24044316e-02]],\n",
       " \n",
       "        [[-1.37934191e-02, -6.91205682e-03,  1.02826273e-02, ...,\n",
       "          -2.66607367e-02, -3.18595278e-03,  7.43433312e-02],\n",
       "         [ 3.79966348e-02, -7.05711097e-02,  2.77894847e-02, ...,\n",
       "           6.37022499e-03,  8.50341097e-03, -5.76514611e-03],\n",
       "         [-6.08623885e-02, -1.94990691e-02,  3.56777525e-03, ...,\n",
       "          -7.96539485e-02, -8.84525552e-02, -6.32934496e-02],\n",
       "         ...,\n",
       "         [ 1.76883321e-02, -1.63576119e-02,  6.19401271e-03, ...,\n",
       "          -9.87441614e-02,  1.67870205e-02, -4.89414074e-02],\n",
       "         [-4.68735471e-02, -1.37241585e-02, -3.63074020e-02, ...,\n",
       "          -4.74681938e-03,  1.66878905e-02,  2.73127481e-02],\n",
       "         [ 4.30030599e-02,  5.02029844e-02,  1.46419620e-02, ...,\n",
       "          -5.73862195e-02, -8.13698843e-02,  2.01386977e-02]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 6.12237416e-02, -1.41854770e-02,  1.77366880e-03, ...,\n",
       "           7.52145872e-02,  4.06597778e-02, -6.58695819e-04],\n",
       "         [ 3.87964658e-02,  1.77261177e-02, -3.90456617e-02, ...,\n",
       "           2.43398966e-03,  3.01827639e-02,  4.89492379e-02],\n",
       "         [ 8.87926444e-02, -4.80597951e-02, -1.13565205e-02, ...,\n",
       "           5.07058725e-02,  5.56457937e-02, -6.88442960e-02],\n",
       "         ...,\n",
       "         [ 3.85566652e-02,  2.94552837e-02, -6.09112997e-03, ...,\n",
       "          -4.21835333e-02,  7.74582773e-02, -8.62103179e-02],\n",
       "         [-1.01276068e-03,  7.45131448e-02, -9.14902389e-02, ...,\n",
       "          -1.24823954e-02, -1.76364109e-02, -1.34939551e-02],\n",
       "         [ 6.55856566e-04, -1.89102292e-02, -4.70709093e-02, ...,\n",
       "          -3.00203562e-02, -5.43711102e-03,  5.22059128e-02]],\n",
       " \n",
       "        [[-1.97893288e-02, -3.99369337e-02, -2.01687682e-02, ...,\n",
       "           1.07571013e-01, -5.48112951e-02,  2.45500430e-02],\n",
       "         [-9.48833674e-02,  1.05895102e-02, -6.57436475e-02, ...,\n",
       "           3.57521884e-02,  7.01470003e-02, -2.10398138e-02],\n",
       "         [ 5.08636236e-02, -3.67280394e-02,  5.57496287e-02, ...,\n",
       "          -1.51846111e-02,  4.66063432e-02,  9.84317437e-02],\n",
       "         ...,\n",
       "         [ 2.95173395e-02, -3.78649198e-02,  9.75678768e-03, ...,\n",
       "          -5.82169443e-02,  1.07622975e-02,  2.03110903e-04],\n",
       "         [ 5.44219501e-02, -7.58614717e-03,  3.80840860e-02, ...,\n",
       "          -3.27596702e-02,  4.34688339e-03, -8.52398202e-03],\n",
       "         [ 4.61454177e-03, -5.36691360e-02,  1.67022124e-02, ...,\n",
       "           1.98370852e-02, -6.28364608e-02,  1.39460023e-02]],\n",
       " \n",
       "        [[ 1.70511529e-02,  5.67287095e-02,  1.56099768e-02, ...,\n",
       "           3.61145623e-02, -1.28884623e-02, -3.24840993e-02],\n",
       "         [-7.30892941e-02, -6.75101648e-04,  4.71097007e-02, ...,\n",
       "           8.39213096e-03, -3.28929420e-03,  3.06057539e-02],\n",
       "         [-1.45385601e-02,  4.61573265e-02, -2.82816328e-02, ...,\n",
       "           2.17523631e-02, -2.90550180e-02, -2.44918950e-02],\n",
       "         ...,\n",
       "         [-5.87690026e-02,  1.77998474e-04,  1.37388401e-04, ...,\n",
       "          -6.14088476e-02, -5.57658114e-02, -2.53464598e-02],\n",
       "         [-4.01050448e-02, -4.31350544e-02, -1.02494713e-02, ...,\n",
       "          -1.14357226e-01, -5.82783390e-03,  5.18922694e-02],\n",
       "         [ 3.73215750e-02, -2.74298276e-04,  1.37561783e-02, ...,\n",
       "          -2.49762088e-02, -2.74428278e-02,  9.41002835e-03]]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'transformer/self_attention/key/bias:0' shape=(16, 64) dtype=float32, numpy=\n",
       " array([[-0.1956429 ,  0.11978535, -0.41439384, ...,  0.08712689,\n",
       "         -0.08149096, -0.1953145 ],\n",
       "        [ 0.6122577 , -0.02963949, -0.16226979, ..., -0.47796267,\n",
       "         -0.06287917, -0.06575805],\n",
       "        [ 0.6078491 , -0.40588948,  0.15858388, ..., -0.40763453,\n",
       "         -0.26575118, -0.01651132],\n",
       "        ...,\n",
       "        [-0.09129642, -0.19593433,  0.586245  , ...,  0.6237123 ,\n",
       "         -0.66800666, -0.12655008],\n",
       "        [ 0.07444556, -0.4216305 , -0.71830815, ..., -0.73751533,\n",
       "          0.46753627,  0.2613811 ],\n",
       "        [-0.02629012,  0.48830336, -0.36769304, ..., -0.03383064,\n",
       "         -0.00832796,  0.16880564]], dtype=float32)>,\n",
       " <tf.Variable 'transformer/self_attention/value/kernel:0' shape=(1024, 16, 64) dtype=float32, numpy=\n",
       " array([[[ 8.84016156e-02,  1.97671587e-03, -2.98831891e-02, ...,\n",
       "           4.21544956e-03,  7.51649141e-02,  3.94895189e-02],\n",
       "         [ 1.20483730e-02,  1.60314571e-02,  9.79871489e-03, ...,\n",
       "           4.21774201e-02,  7.49649554e-02,  5.45979924e-02],\n",
       "         [-4.09779772e-02, -1.26496060e-02,  7.50075951e-02, ...,\n",
       "          -1.33863874e-02,  1.65342558e-02, -8.85554589e-03],\n",
       "         ...,\n",
       "         [-3.96020748e-02, -4.24026549e-02,  4.76025045e-02, ...,\n",
       "          -5.58820479e-02,  3.48976366e-02, -7.39881396e-02],\n",
       "         [-1.88900472e-03, -6.56860881e-03,  4.86176722e-02, ...,\n",
       "          -3.81734073e-02, -2.10725814e-02, -3.14065851e-02],\n",
       "         [-6.78430796e-02,  4.24959287e-02, -1.68318152e-02, ...,\n",
       "          -4.87668626e-02, -2.81981640e-02, -5.31906588e-03]],\n",
       " \n",
       "        [[ 7.26035088e-02, -7.34381899e-02, -8.60327333e-02, ...,\n",
       "          -6.25154451e-02,  2.57068733e-03, -8.86605680e-03],\n",
       "         [-3.76568250e-02,  1.51764341e-02, -2.77973432e-03, ...,\n",
       "          -6.43719956e-02, -1.46120945e-02, -3.16182896e-02],\n",
       "         [-2.19388660e-02, -6.49599582e-02, -4.04830463e-02, ...,\n",
       "           2.17028577e-02, -2.07943954e-02,  1.71940271e-02],\n",
       "         ...,\n",
       "         [-2.74257138e-02,  5.18143782e-03,  3.36040254e-03, ...,\n",
       "           8.87714885e-03,  7.25875609e-03, -5.08724824e-02],\n",
       "         [-1.81816984e-02, -4.07996513e-02,  2.60585919e-02, ...,\n",
       "          -9.56979301e-03, -6.14493787e-02,  2.34778784e-02],\n",
       "         [-2.49769166e-02,  4.90200259e-02, -6.78923773e-03, ...,\n",
       "          -3.66155133e-02,  4.59291972e-02,  2.98734847e-02]],\n",
       " \n",
       "        [[ 1.34013258e-02, -3.84190939e-02,  2.61988882e-02, ...,\n",
       "           1.29537610e-02,  8.66691694e-02,  6.07226789e-02],\n",
       "         [ 7.47839659e-02, -8.54707509e-02, -2.54021697e-02, ...,\n",
       "          -5.08090481e-03,  3.37091461e-03,  6.46888241e-02],\n",
       "         [ 2.11670864e-02, -4.59643081e-04,  4.88058059e-03, ...,\n",
       "          -1.21005429e-02, -1.59156253e-03,  1.80753041e-03],\n",
       "         ...,\n",
       "         [ 2.91894935e-02, -6.00010566e-02,  4.01255749e-02, ...,\n",
       "           6.38656095e-02,  1.38774952e-02, -7.45983198e-02],\n",
       "         [ 3.10542602e-02,  4.46506105e-02,  1.22498265e-02, ...,\n",
       "          -4.85783769e-03,  3.76220979e-02, -2.93234829e-02],\n",
       "         [-6.62807748e-02,  6.11091219e-03,  1.90069210e-02, ...,\n",
       "          -2.86841691e-02,  1.94906630e-02,  2.45454023e-03]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 3.69904153e-02, -7.06183910e-02, -4.65893093e-03, ...,\n",
       "          -5.61791426e-03, -7.15775490e-02,  7.92394020e-03],\n",
       "         [ 5.34623861e-04, -3.02964561e-02, -6.22149324e-03, ...,\n",
       "          -5.18760756e-02,  7.97435939e-02,  9.95759014e-03],\n",
       "         [-4.31443490e-02, -5.05503230e-02,  1.70822460e-02, ...,\n",
       "           6.02459116e-03,  1.83783378e-02,  6.40390022e-03],\n",
       "         ...,\n",
       "         [-1.07716806e-01,  5.19813783e-02,  1.77120473e-02, ...,\n",
       "           5.07399924e-02,  1.25860693e-02,  4.01882045e-02],\n",
       "         [-5.04538044e-02, -1.33127812e-02, -1.23923020e-02, ...,\n",
       "          -4.27794978e-02,  4.38978598e-02,  1.88292358e-02],\n",
       "         [-5.43645360e-02, -3.22994329e-02, -1.73987611e-03, ...,\n",
       "          -3.51011008e-02,  3.07101905e-02,  3.56406495e-02]],\n",
       " \n",
       "        [[ 4.81707864e-02, -3.68030630e-02,  1.46032451e-02, ...,\n",
       "           1.60659328e-02,  1.15535766e-01,  1.92982331e-02],\n",
       "         [ 1.15479773e-03, -4.88907509e-02, -4.35790606e-02, ...,\n",
       "           7.09841400e-02,  1.58313349e-01, -6.31248578e-02],\n",
       "         [ 8.56134444e-02, -3.79269905e-02,  4.17474955e-02, ...,\n",
       "          -2.95574050e-02, -3.95833558e-05,  3.82292569e-02],\n",
       "         ...,\n",
       "         [-1.45074120e-02, -1.39518753e-02, -1.41746439e-02, ...,\n",
       "           1.46158342e-03,  8.66528898e-02,  2.69849673e-02],\n",
       "         [ 3.82338464e-02,  5.97449280e-02,  1.82331726e-02, ...,\n",
       "           4.76913042e-02,  1.03722932e-03, -9.19897109e-02],\n",
       "         [-5.66496849e-02,  2.47478466e-02, -4.93762307e-02, ...,\n",
       "           6.64070100e-02,  1.17479771e-01,  4.56505120e-02]],\n",
       " \n",
       "        [[ 3.77553352e-03, -1.95612367e-02, -2.04074923e-02, ...,\n",
       "          -2.07989626e-02,  4.82789017e-02,  1.80915054e-02],\n",
       "         [ 5.95083907e-02,  4.03393479e-03,  2.71806028e-02, ...,\n",
       "          -4.57914807e-02,  2.46583689e-02, -1.20288208e-02],\n",
       "         [ 6.50061965e-02, -2.31799074e-02,  1.94374390e-03, ...,\n",
       "          -2.64258254e-02,  8.31988268e-03, -1.98145807e-02],\n",
       "         ...,\n",
       "         [ 1.85260400e-02, -1.20331924e-02, -7.38807470e-02, ...,\n",
       "           5.67058250e-02,  3.08949791e-04, -1.94578953e-02],\n",
       "         [-7.53983855e-03,  2.12213304e-02, -1.29044717e-02, ...,\n",
       "           7.53038377e-02, -5.91378324e-02, -3.34478468e-02],\n",
       "         [ 2.52955295e-02,  5.13008423e-03,  1.55525180e-02, ...,\n",
       "          -2.15471396e-03, -1.40539967e-02,  5.33178113e-02]]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'transformer/self_attention/value/bias:0' shape=(16, 64) dtype=float32, numpy=\n",
       " array([[-0.00612804,  0.01936247, -0.07843664, ...,  0.01466416,\n",
       "         -0.04609896,  0.05995765],\n",
       "        [ 0.0112722 , -0.0165302 ,  0.02295177, ...,  0.05125294,\n",
       "          0.04878471, -0.01401286],\n",
       "        [-0.03410047,  0.02393925,  0.04466789, ...,  0.02743834,\n",
       "          0.00653202, -0.027255  ],\n",
       "        ...,\n",
       "        [-0.00806766, -0.00755278,  0.01693884, ...,  0.03747057,\n",
       "         -0.028055  ,  0.02464439],\n",
       "        [-0.10971466, -0.05622318, -0.03724105, ..., -0.02046465,\n",
       "         -0.02270768, -0.06391806],\n",
       "        [-0.05784258, -0.06984774,  0.03340493, ...,  0.05560609,\n",
       "          0.04972344,  0.10994302]], dtype=float32)>,\n",
       " <tf.Variable 'transformer/self_attention_output/kernel:0' shape=(16, 64, 1024) dtype=float32, numpy=\n",
       " array([[[-0.12076962, -0.00124359,  0.00607461, ..., -0.01294489,\n",
       "          -0.01668625, -0.10402326],\n",
       "         [-0.11004813,  0.03989868,  0.0893674 , ..., -0.0095838 ,\n",
       "          -0.03904977, -0.04312504],\n",
       "         [-0.08441711,  0.05304788, -0.05313711, ...,  0.04464523,\n",
       "          -0.02261757,  0.03814788],\n",
       "         ...,\n",
       "         [-0.01430654,  0.00074047, -0.03959184, ..., -0.00637076,\n",
       "           0.02778773,  0.04182913],\n",
       "         [ 0.02673653, -0.04558013, -0.01118895, ...,  0.0786153 ,\n",
       "          -0.10277542, -0.08148177],\n",
       "         [-0.03437106,  0.04556883, -0.07144477, ..., -0.0735973 ,\n",
       "          -0.01271444,  0.04527805]],\n",
       " \n",
       "        [[ 0.04176908,  0.00657758,  0.07336061, ..., -0.02303617,\n",
       "           0.01757016,  0.04795979],\n",
       "         [-0.07754111, -0.02371346,  0.04678926, ..., -0.16256103,\n",
       "          -0.10659497,  0.00786955],\n",
       "         [-0.05925811,  0.02528149,  0.04305986, ..., -0.08639605,\n",
       "          -0.04194742, -0.03341696],\n",
       "         ...,\n",
       "         [ 0.04289128,  0.03874972,  0.0044007 , ...,  0.05060798,\n",
       "          -0.08470653,  0.05530193],\n",
       "         [-0.02249091,  0.00067448,  0.08657753, ..., -0.09440999,\n",
       "          -0.01789555, -0.00100244],\n",
       "         [-0.04187757,  0.02363127,  0.08031391, ...,  0.12403613,\n",
       "           0.03936626, -0.04828065]],\n",
       " \n",
       "        [[-0.09846748, -0.00457191,  0.03751268, ..., -0.03899229,\n",
       "           0.02219013,  0.05998238],\n",
       "         [-0.0095049 , -0.0219955 ,  0.05206475, ..., -0.0404796 ,\n",
       "           0.01705968,  0.06544822],\n",
       "         [ 0.08183285,  0.06762052,  0.05677623, ..., -0.01210072,\n",
       "           0.03179766,  0.00856059],\n",
       "         ...,\n",
       "         [-0.02323305,  0.01557738,  0.05678723, ...,  0.02721384,\n",
       "          -0.01852079, -0.01899264],\n",
       "         [ 0.06470155, -0.0330917 , -0.07008295, ..., -0.05411379,\n",
       "           0.07947845,  0.07558351],\n",
       "         [-0.00261759, -0.04478495, -0.05660342, ...,  0.00734516,\n",
       "           0.00162167, -0.00401215]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 0.04153054, -0.03763792, -0.04070792, ..., -0.0598935 ,\n",
       "          -0.03227507, -0.06377398],\n",
       "         [ 0.01416222, -0.01012977, -0.01811562, ...,  0.03910504,\n",
       "           0.10524264, -0.01084595],\n",
       "         [ 0.01043527, -0.03096985, -0.08023923, ..., -0.00698309,\n",
       "          -0.03181529, -0.03601998],\n",
       "         ...,\n",
       "         [ 0.05502656,  0.00743641, -0.03467162, ...,  0.02610944,\n",
       "          -0.01002952,  0.00538922],\n",
       "         [-0.00382371,  0.10242363,  0.04912334, ..., -0.01872185,\n",
       "          -0.00876849, -0.05615966],\n",
       "         [-0.0432677 , -0.00377473,  0.0486761 , ..., -0.03262281,\n",
       "           0.05927964, -0.0048315 ]],\n",
       " \n",
       "        [[ 0.02465898, -0.01823416, -0.0472507 , ...,  0.0429794 ,\n",
       "          -0.02873516,  0.02178667],\n",
       "         [-0.05688816,  0.02903259, -0.0457236 , ..., -0.00974455,\n",
       "           0.02306955,  0.07989483],\n",
       "         [ 0.02432488, -0.02294524,  0.06199696, ..., -0.03046644,\n",
       "          -0.04192131,  0.04654254],\n",
       "         ...,\n",
       "         [-0.07837856, -0.03841452, -0.00648896, ...,  0.00424615,\n",
       "           0.03522164,  0.00203856],\n",
       "         [-0.06949102, -0.00987331, -0.00963828, ...,  0.04554335,\n",
       "           0.04357611,  0.03793874],\n",
       "         [ 0.01787398,  0.0534835 ,  0.07396908, ...,  0.00537893,\n",
       "          -0.02983464,  0.02539661]],\n",
       " \n",
       "        [[ 0.0326856 , -0.02441664,  0.01665724, ...,  0.0593833 ,\n",
       "           0.03044354, -0.03734902],\n",
       "         [-0.08340491, -0.02600884,  0.04670591, ...,  0.05623489,\n",
       "          -0.04839526, -0.00978218],\n",
       "         [ 0.05021174, -0.00096614, -0.02343779, ...,  0.01671461,\n",
       "           0.0234131 ,  0.0042096 ],\n",
       "         ...,\n",
       "         [-0.01196689,  0.03425705,  0.00565995, ...,  0.04516388,\n",
       "          -0.03617317,  0.0251976 ],\n",
       "         [ 0.0102492 , -0.02441114,  0.00513568, ..., -0.01253297,\n",
       "          -0.02913623,  0.01367229],\n",
       "         [-0.00350012,  0.03103626,  0.02383353, ...,  0.05469441,\n",
       "          -0.02100387, -0.00407371]]], dtype=float32)>,\n",
       " <tf.Variable 'transformer/self_attention_output/bias:0' shape=(1024,) dtype=float32, numpy=\n",
       " array([ 0.09864695, -0.0951082 ,  0.06430744, ...,  0.03073977,\n",
       "        -0.06190206,  0.08906726], dtype=float32)>,\n",
       " <tf.Variable 'transformer/self_attention_layer_norm/gamma:0' shape=(1024,) dtype=float32, numpy=\n",
       " array([0.47836667, 0.5672604 , 0.47873163, ..., 0.4815665 , 0.478954  ,\n",
       "        0.5079666 ], dtype=float32)>,\n",
       " <tf.Variable 'transformer/self_attention_layer_norm/beta:0' shape=(1024,) dtype=float32, numpy=\n",
       " array([-0.11103752,  0.0920947 , -0.04503051, ...,  0.00631373,\n",
       "         0.02131797, -0.02561848], dtype=float32)>,\n",
       " <tf.Variable 'transformer/intermediate/kernel:0' shape=(1024, 4096) dtype=float32, numpy=\n",
       " array([[ 0.02648608,  0.01527689,  0.02645291, ..., -0.01140412,\n",
       "          0.01198651, -0.01803507],\n",
       "        [ 0.01017435,  0.02895561,  0.0084113 , ...,  0.00270681,\n",
       "         -0.07450064, -0.05762589],\n",
       "        [ 0.07269916, -0.11212754, -0.04652131, ..., -0.01061522,\n",
       "          0.0189837 , -0.03882485],\n",
       "        ...,\n",
       "        [-0.03010517, -0.01153286, -0.01826166, ...,  0.11262808,\n",
       "          0.06751062, -0.01863285],\n",
       "        [-0.01862432,  0.04226874,  0.02996612, ..., -0.09953069,\n",
       "         -0.00376862, -0.02929319],\n",
       "        [ 0.07815616, -0.00221558,  0.05969049, ...,  0.03344903,\n",
       "          0.0931612 , -0.00501654]], dtype=float32)>,\n",
       " <tf.Variable 'transformer/intermediate/bias:0' shape=(4096,) dtype=float32, numpy=\n",
       " array([-0.6675501 , -0.23963705, -0.8007771 , ..., -0.6696589 ,\n",
       "        -0.23901248, -0.38618547], dtype=float32)>,\n",
       " <tf.Variable 'transformer/output/kernel:0' shape=(4096, 1024) dtype=float32, numpy=\n",
       " array([[ 0.06252542, -0.09937467, -0.00612367, ..., -0.05158816,\n",
       "         -0.04077099,  0.11504003],\n",
       "        [ 0.06606197, -0.05987186, -0.00526986, ...,  0.00518518,\n",
       "          0.04754982,  0.08218212],\n",
       "        [ 0.00038137,  0.03951951, -0.00363255, ..., -0.05936174,\n",
       "          0.06247735,  0.05207566],\n",
       "        ...,\n",
       "        [-0.01949616, -0.02872715,  0.01997065, ...,  0.05796802,\n",
       "         -0.01691946,  0.0773401 ],\n",
       "        [-0.07415913,  0.01402765, -0.05682854, ..., -0.00647738,\n",
       "          0.04324425, -0.09231628],\n",
       "        [-0.02523838, -0.05340097, -0.05370631, ...,  0.07269071,\n",
       "         -0.07388975,  0.08264983]], dtype=float32)>,\n",
       " <tf.Variable 'transformer/output/bias:0' shape=(1024,) dtype=float32, numpy=\n",
       " array([-0.24631073, -0.0074086 , -0.04412123, ..., -0.0831793 ,\n",
       "         0.09848394, -0.04435193], dtype=float32)>,\n",
       " <tf.Variable 'transformer/output_layer_norm/gamma:0' shape=(1024,) dtype=float32, numpy=\n",
       " array([0.8543308 , 0.8785833 , 0.87600917, ..., 0.8645065 , 0.8235395 ,\n",
       "        0.8551662 ], dtype=float32)>,\n",
       " <tf.Variable 'transformer/output_layer_norm/beta:0' shape=(1024,) dtype=float32, numpy=\n",
       " array([-0.07936585, -0.05046983, -0.00038895, ...,  0.01893941,\n",
       "         0.03680166,  0.1274723 ], dtype=float32)>,\n",
       " <tf.Variable 'pooler_transform/kernel:0' shape=(1024, 1024) dtype=float32, numpy=\n",
       " array([[ 0.00209013, -0.02613378, -0.01575232, ...,  0.01112407,\n",
       "         -0.0258625 , -0.04328314],\n",
       "        [ 0.01167197,  0.01222261,  0.05649068, ..., -0.02364089,\n",
       "          0.03388514,  0.04284851],\n",
       "        [ 0.05607547,  0.01637948, -0.01923612, ..., -0.0089825 ,\n",
       "         -0.00936118,  0.00104921],\n",
       "        ...,\n",
       "        [-0.0143528 , -0.03299445, -0.0298039 , ...,  0.02447739,\n",
       "         -0.01639321, -0.01822768],\n",
       "        [ 0.01995399,  0.02741775,  0.01170857, ..., -0.0106745 ,\n",
       "         -0.00529902,  0.02148839],\n",
       "        [-0.023977  ,  0.01085562, -0.02579332, ..., -0.03129175,\n",
       "          0.00513519,  0.00111219]], dtype=float32)>,\n",
       " <tf.Variable 'pooler_transform/bias:0' shape=(1024,) dtype=float32, numpy=\n",
       " array([-0.04458358, -0.26339373, -0.06983171, ...,  0.26788852,\n",
       "        -0.30023223, -0.05624715], dtype=float32)>,\n",
       " <tf.Variable 'output_1/kernel:0' shape=(1024, 142) dtype=float32, numpy=\n",
       " array([[-0.04154005, -0.01395558, -0.01081378, ...,  0.034966  ,\n",
       "         -0.01656962, -0.0125685 ],\n",
       "        [ 0.01597771, -0.01747953,  0.01603039, ...,  0.01733884,\n",
       "          0.00539459, -0.02287245],\n",
       "        [ 0.02353249,  0.01709798,  0.02557017, ..., -0.00467271,\n",
       "         -0.00704464,  0.00849333],\n",
       "        ...,\n",
       "        [ 0.00343479, -0.0289441 ,  0.02984244, ..., -0.02753221,\n",
       "          0.02406337,  0.01163566],\n",
       "        [ 0.00715165,  0.01472412, -0.00213667, ..., -0.01382537,\n",
       "          0.00591946,  0.01200054],\n",
       "        [-0.00901164, -0.02624142,  0.00892869, ..., -0.03103399,\n",
       "          0.01520123,  0.01467923]], dtype=float32)>,\n",
       " <tf.Variable 'output_1/bias:0' shape=(142,) dtype=float32, numpy=\n",
       " array([ 1.15411286e-03,  1.00988289e-03, -2.02028477e-03, -2.36895634e-03,\n",
       "         9.86960833e-04, -3.53188167e-04, -1.03993563e-03, -1.23772514e-03,\n",
       "         8.77986953e-04, -4.63747536e-04, -9.50771326e-04, -1.65553778e-04,\n",
       "        -1.45022728e-04, -4.11459652e-04, -4.19684839e-05,  3.69637506e-04,\n",
       "        -1.22052967e-03,  5.67146752e-04, -1.31180638e-03, -1.48641632e-03,\n",
       "         5.93700912e-04,  6.64406747e-04,  1.24930486e-03,  4.36759059e-04,\n",
       "        -1.16532254e-04,  1.07290526e-03,  1.28139299e-03, -1.85472472e-03,\n",
       "        -1.53105136e-03,  7.78272748e-04,  1.24870019e-03,  7.47176469e-04,\n",
       "         1.22851261e-03,  9.48715373e-04,  1.08130695e-03, -3.50646937e-04,\n",
       "         8.54528102e-04,  1.05059112e-03, -1.33806583e-03, -5.85215923e-04,\n",
       "         9.64036677e-04,  3.23660170e-05, -9.32435563e-04,  1.12205940e-04,\n",
       "        -1.97153888e-03, -1.34353735e-03, -1.53929566e-03, -5.37992048e-04,\n",
       "        -6.95010182e-04, -1.67278794e-03, -6.73229864e-04, -8.38833395e-04,\n",
       "        -1.30324741e-03,  7.73543783e-04, -1.75130228e-03, -2.72636622e-04,\n",
       "        -1.64115080e-03, -2.31814454e-03, -1.23613805e-03, -2.31664348e-03,\n",
       "        -1.67315986e-04, -1.49014452e-03, -3.03220283e-03, -1.60472831e-04,\n",
       "        -2.01193569e-03, -9.89404623e-04, -1.18305150e-03, -3.52128882e-05,\n",
       "        -1.18662708e-03,  4.29928798e-04, -1.21293962e-03,  2.10818092e-04,\n",
       "        -2.40145344e-03, -1.39733078e-03, -1.10921566e-03, -1.15759659e-03,\n",
       "        -7.94397027e-04, -1.34095107e-03, -1.55800802e-03, -1.46076549e-03,\n",
       "        -5.03152143e-04, -7.89465441e-04, -1.52212870e-03,  3.56022414e-04,\n",
       "        -1.03032007e-03, -2.01638229e-03, -2.62917718e-03, -1.26671593e-03,\n",
       "         1.38290797e-03,  7.91280007e-04,  9.05798108e-04,  1.06606446e-03,\n",
       "         3.73121700e-04,  1.10919925e-03, -1.79663545e-03, -2.40735448e-04,\n",
       "        -1.03468970e-04,  1.01420411e-03, -1.36346649e-03, -2.38528429e-03,\n",
       "        -1.74725812e-03, -1.99159305e-03, -9.99693060e-04,  1.00575306e-03,\n",
       "        -1.82213017e-03, -1.24600146e-03,  9.14572796e-04,  5.52543206e-04,\n",
       "        -1.63823483e-03, -1.46184873e-03, -1.50582427e-03, -1.51146785e-03,\n",
       "         7.88013684e-04, -1.12305104e-03,  4.61524061e-04, -3.08745098e-03,\n",
       "        -7.28351006e-04, -1.23109762e-03, -2.13422463e-03, -1.56620145e-03,\n",
       "        -2.36995285e-03, -2.37237010e-03, -1.41386408e-03, -9.33337084e-04,\n",
       "        -2.53914716e-03, -1.34607908e-04, -8.81027372e-04,  4.18374111e-04,\n",
       "         1.13911857e-03, -1.96607341e-03, -1.65735930e-03,  3.72426817e-04,\n",
       "         3.81238533e-05, -6.01773092e-04, -1.66498451e-03, -1.57664192e-03,\n",
       "        -1.57577218e-04, -1.21519226e-03, -1.13052060e-03,  7.41305121e-04,\n",
       "        -3.04384157e-03, -5.65998140e-04], dtype=float32)>]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "squeezed_values  [930 20 2267]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([b'representative-request'], dtype=object)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.predict(['talk to agent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {i:label for i,label in enumerate(intent_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'representative-request'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map.get(131)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_text (InputLayer)         [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sentencepiece_tokenization (Sen ((None, None), (None 0           input_text[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        [(None, 1024), (None 17683968    sentencepiece_tokenization[0][0] \n",
      "                                                                 sentencepiece_tokenization[0][2] \n",
      "                                                                 sentencepiece_tokenization[0][1] \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 142)          145550      keras_layer[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 17,829,518\n",
      "Trainable params: 17,829,518\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classifier_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%tensorboard --logdir /var/extra/users/jgeorge/tf2.0/input/dish/models/albert_en_large/checkpoint/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /var/extra/users/jgeorge/tf2.0/input/dish/models/albert_en_large/checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "squeezed_values  [10975 4431]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.27895358, -0.8509937 , -0.7052376 , -1.34409   , -0.19968668,\n",
       "        -0.51657605, -0.59884816, -0.6178915 , -0.03377295,  0.03148424,\n",
       "        -0.43111902, -0.44971946,  0.218729  ,  0.2059249 ,  0.05345267,\n",
       "         0.22005515, -0.44621506,  0.65703523, -0.65717566, -0.98199546,\n",
       "         0.7022888 ,  0.1838875 ,  0.97640514, -0.30011883,  0.7176235 ,\n",
       "         0.02592234,  1.5837554 , -0.49451452, -0.86549574,  0.87994015,\n",
       "         0.72771543,  1.0558922 ,  1.836086  ,  2.031024  ,  1.1247745 ,\n",
       "         0.5711162 ,  0.49203905,  1.744866  , -0.97309726,  0.18413556,\n",
       "         1.0031044 , -0.27900767, -0.42430964, -0.07937201, -0.10364547,\n",
       "        -0.08193875, -0.49301875,  0.03154013,  0.1131788 , -0.5524068 ,\n",
       "         0.20787393, -0.09940173, -0.7745844 ,  0.96934766, -0.4284465 ,\n",
       "        -0.22585136, -0.5841641 , -0.6326774 ,  0.19772112, -0.2580366 ,\n",
       "        -0.42142487, -0.32527712, -1.0396574 ,  0.31172433, -0.29836518,\n",
       "        -0.59675443, -0.4839771 ,  0.5275687 ,  0.29021633,  0.6262975 ,\n",
       "        -0.505385  ,  0.66964185, -0.8717921 , -0.7380658 , -0.27209514,\n",
       "        -0.31848228, -0.08867155, -0.7018207 , -0.51054084, -0.7963186 ,\n",
       "        -0.45908177, -0.6399879 , -0.4734236 , -0.40171975, -0.18184662,\n",
       "        -0.65058815, -0.9425552 , -0.15943623,  2.0243478 ,  0.33309934,\n",
       "         0.6515679 ,  0.18172693, -0.06997672,  0.36525014, -0.79005176,\n",
       "        -0.26967007, -0.21508855,  1.0891409 , -0.6483409 , -0.8086802 ,\n",
       "        -0.68494684, -0.36094   , -0.26980934,  1.0651966 , -1.0922254 ,\n",
       "        -0.08303424,  1.0754303 , -0.5327997 , -0.40472534, -0.44601828,\n",
       "        -0.40890202,  0.0596805 , -0.35929966, -0.36072898,  1.0050242 ,\n",
       "        -1.0619447 , -0.10664995, -0.36157984, -1.158436  , -0.5141908 ,\n",
       "        -0.525503  , -0.5045419 , -0.33613428, -0.7068503 , -0.7980415 ,\n",
       "        -0.3763429 , -0.23205972, -0.16781063,  1.1677498 , -0.63779396,\n",
       "        -0.31014085,  1.4180926 ,  0.2528841 , -0.13604827, -0.16281323,\n",
       "        -0.7908212 , -0.0158345 , -0.00257447, -0.01256813,  0.62604445,\n",
       "        -0.6271161 ,  0.3649883 ]], dtype=float32)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_model.predict(['hello testing'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=int32, numpy=array([2, 3], dtype=int32)>"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfs.piece_to_id(['[CLS]','[SEP]'],model_proto=sp_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([636,   1, 500, 636,   1, 500], dtype=int32)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tfs.encode(['[CLS] [SEP]'],model_proto=sp_model)\n",
    "t.values.numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_predict(row):\n",
    "    text  = row['text']\n",
    "    d = {}\n",
    "    t = []\n",
    "    t.append(2)\n",
    "    temp= tfs.encode([text],model_proto=sp_model).values.numpy()[0]\n",
    "    t.extend(temp)\n",
    "    t.append(3)\n",
    "    \n",
    "#     d['input_ids'] = [t]\n",
    "#     d['input_mask'] = [[1]*len(t)]\n",
    "#     d['segment_ids'] = [[0]*len(t)]\n",
    "    \n",
    "    d['input_ids'] = tf.convert_to_tensor([t],dtype=tf.int32)\n",
    "    d['input_mask'] = tf.convert_to_tensor([[1]*len(t)],dtype=tf.int32)\n",
    "    d['segment_ids'] = tf.convert_to_tensor([[0]*len(t)],dtype=tf.int32)\n",
    "#     return pd.Series(d)\n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    {'input_ids': ((tf.Tensor(2, shape=(), dtype=i...\n",
       "1    {'input_ids': ((tf.Tensor(2, shape=(), dtype=i...\n",
       "2    {'input_ids': ((tf.Tensor(2, shape=(), dtype=i...\n",
       "3    {'input_ids': ((tf.Tensor(2, shape=(), dtype=i...\n",
       "4    {'input_ids': ((tf.Tensor(2, shape=(), dtype=i...\n",
       "dtype: object"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_test = df.head()\n",
    "temp_test = temp_test.apply(lambda x: get_input_predict(x),axis=1)\n",
    "temp_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(1, 11), dtype=int32, numpy=\n",
       " array([[   2,   92,   42, 2660,   51, 1071,   20,   51, 4216,   60,    3]],\n",
       "       dtype=int32)>,\n",
       " 'input_mask': <tf.Tensor: shape=(1, 11), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)>,\n",
       " 'segment_ids': <tf.Tensor: shape=(1, 11), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>}"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type dict).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36mnormalize_element\u001b[0;34m(element)\u001b[0m\n\u001b[1;32m     92\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_spec_from_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_fallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36mtype_spec_from_value\u001b[0;34m(element, use_fallback)\u001b[0m\n\u001b[1;32m    465\u001b[0m   raise TypeError(\"Could not build a TypeSpec for %r with type %s\" %\n\u001b[0;32m--> 466\u001b[0;31m                   (element, type(element).__name__))\n\u001b[0m\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Could not build a TypeSpec for 0    {'input_ids': ((tf.Tensor(2, shape=(), dtype=i...\n1    {'input_ids': ((tf.Tensor(2, shape=(), dtype=i...\n2    {'input_ids': ((tf.Tensor(2, shape=(), dtype=i...\n3    {'input_ids': ((tf.Tensor(2, shape=(), dtype=i...\n4    {'input_ids': ((tf.Tensor(2, shape=(), dtype=i...\ndtype: object with type Series",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-221-476b973fbae9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrained_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[1;32m     87\u001b[0m           method.__name__))\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m   return tf_decorator.make_decorator(\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1247\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1248\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1249\u001b[0;31m           model=self)\n\u001b[0m\u001b[1;32m   1250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1251\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model)\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1112\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0mindices_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice_batch_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslice_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"batch\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mslice_inputs\u001b[0;34m(self, indices_dataset, inputs)\u001b[0m\n\u001b[1;32m    388\u001b[0m     dataset = dataset_ops.DatasetV2.zip((\n\u001b[1;32m    389\u001b[0m         \u001b[0mindices_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m         \u001b[0mdataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDatasetV2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m     ))\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mfrom_tensors\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    560\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m     \"\"\"\n\u001b[0;32m--> 562\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, element)\u001b[0m\n\u001b[1;32m   2837\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2838\u001b[0m     \u001b[0;34m\"\"\"See `Dataset.from_tensors()` for details.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2839\u001b[0;31m     \u001b[0melement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2840\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_structure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_spec_from_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2841\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36mnormalize_element\u001b[0;34m(element)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;31m# the value. As a fallback try converting the value to a tensor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         normalized_components.append(\n\u001b[0;32m---> 98\u001b[0;31m             ops.convert_to_tensor(t, name=\"component_%d\" % i))\n\u001b[0m\u001b[1;32m     99\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseTensorSpec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1341\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    319\u001b[0m                                          as_ref=False):\n\u001b[1;32m    320\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    260\u001b[0m   \"\"\"\n\u001b[1;32m    261\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 262\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    268\u001b[0m   \u001b[0mctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     94\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type dict)."
     ]
    }
   ],
   "source": [
    "trained_model.predict(temp_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([2, 92, 42, 2660, 51, 1071, 20, 51, 4216, 60, 3]),\n",
       "       list([2, 13, 1, 93, 13, 1, 1326, 6264, 63, 529, 13, 1, 58, 4213, 2119, 2800, 19, 51, 634, 337, 9, 13, 1, 438, 3320, 6264, 63, 2800, 9, 3]),\n",
       "       list([2, 13, 1, 376, 21, 7582, 3896, 86, 31, 221, 22, 38, 164, 51, 365, 2830, 576, 1427, 3]),\n",
       "       list([2, 184, 92, 31, 477, 51, 375, 6328, 9, 32, 2206, 31, 1049, 57, 186, 3]),\n",
       "       list([2, 13, 1, 1830, 69, 20, 1960, 29, 737, 88, 51, 1071, 3])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_test['input_ids'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_input = [temp_test['input_ids'].values,temp_test['input_mask'].values,temp_test['segment_ids'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[2, 92, 42, 2660, 51, 1071, 20, 51, 4216, 60, 3],\n",
       "  [2,\n",
       "   13,\n",
       "   1,\n",
       "   93,\n",
       "   13,\n",
       "   1,\n",
       "   1326,\n",
       "   6264,\n",
       "   63,\n",
       "   529,\n",
       "   13,\n",
       "   1,\n",
       "   58,\n",
       "   4213,\n",
       "   2119,\n",
       "   2800,\n",
       "   19,\n",
       "   51,\n",
       "   634,\n",
       "   337,\n",
       "   9,\n",
       "   13,\n",
       "   1,\n",
       "   438,\n",
       "   3320,\n",
       "   6264,\n",
       "   63,\n",
       "   2800,\n",
       "   9,\n",
       "   3],\n",
       "  [2,\n",
       "   13,\n",
       "   1,\n",
       "   376,\n",
       "   21,\n",
       "   7582,\n",
       "   3896,\n",
       "   86,\n",
       "   31,\n",
       "   221,\n",
       "   22,\n",
       "   38,\n",
       "   164,\n",
       "   51,\n",
       "   365,\n",
       "   2830,\n",
       "   576,\n",
       "   1427,\n",
       "   3],\n",
       "  [2, 184, 92, 31, 477, 51, 375, 6328, 9, 32, 2206, 31, 1049, 57, 186, 3],\n",
       "  [2, 13, 1, 1830, 69, 20, 1960, 29, 737, 88, 51, 1071, 3]],\n",
       " 'input_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1],\n",
       "  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       " 'segment_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_test.to_dict(orient='list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_predict_dataset(input_file,seq_length,batch_size,is_training=True):\n",
    "# this is a simplified version & slightly less optimized version of what is used in official bert training\n",
    "# refer to function create_classifier_dataset in models/official/nlp/data/create_finetuning_data.py\n",
    "\n",
    "\n",
    "    # create a tf_data set out of the tfrecords file\n",
    "    dataset = tf.data.TFRecordDataset(input_file)\n",
    "    name_to_features = {\n",
    "        \"input_ids\":tf.io.FixedLenFeature([max_seq_length],tf.int64),\n",
    "        \"input_mask\":tf.io.FixedLenFeature([max_seq_length],tf.int64),\n",
    "        \"segment_ids\":tf.io.FixedLenFeature([max_seq_length],tf.int64),\n",
    "        \"label_id\":tf.io.FixedLenFeature([],tf.int64)\n",
    "        }\n",
    "    ## map function processes line by line, similar to spark,scala map or pandas apply fucntion\n",
    "    dataset = dataset.map(lambda record: tf.io.parse_single_example(record, name_to_features))\n",
    "#     now separating out the features & label\n",
    "    def _select_data_from_record(record):\n",
    "#         x contains the features\n",
    "#         y is your prediction\n",
    "#This dataset will be passed to keras's model.fit refer to it's documentation for further details\n",
    "# a short snippet from that documentation\n",
    "\n",
    "# A `tf.data` dataset. Should return a tuple\n",
    "#         of either `(inputs, targets)` or\n",
    "#         `(inputs, targets, sample_weights)`.\n",
    "\n",
    "\n",
    "        x = {\n",
    "            'input_ids': record['input_ids'],\n",
    "            'input_mask': record['input_mask'],\n",
    "            'segment_ids': record['segment_ids']\n",
    "        }\n",
    "        y = record['label_id']\n",
    "        return x\n",
    "    \n",
    "    dataset = dataset.map(_select_data_from_record)\n",
    "    if is_training:\n",
    "        dataset = dataset.shuffle(100)\n",
    "        dataset = dataset.repeat()\n",
    "    \n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(1024)\n",
    "    return dataset\n",
    "prediction_dataset = get_predict_dataset(eval_data_path,max_seq_length,eval_batch_size,is_training=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': <tf.Tensor: shape=(16, 128), dtype=int64, numpy=\n",
      "array([[   2,  184,  107, ...,    0,    0,    0],\n",
      "       [   2,   13,    1, ...,    0,    0,    0],\n",
      "       [   2,  184,  107, ...,    0,    0,    0],\n",
      "       ...,\n",
      "       [   2, 5388, 2697, ...,    0,    0,    0],\n",
      "       [   2,   13,    1, ...,    0,    0,    0],\n",
      "       [   2,   31,   41, ...,    0,    0,    0]])>, 'input_mask': <tf.Tensor: shape=(16, 128), dtype=int64, numpy=\n",
      "array([[1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0]])>, 'segment_ids': <tf.Tensor: shape=(16, 128), dtype=int64, numpy=\n",
      "array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]])>}\n",
      "{'input_ids': <tf.Tensor: shape=(16, 128), dtype=int64, numpy=\n",
      "array([[ 2, 92, 13, ...,  0,  0,  0],\n",
      "       [ 2, 13,  1, ...,  0,  0,  0],\n",
      "       [ 2, 13,  1, ...,  0,  0,  0],\n",
      "       ...,\n",
      "       [ 2, 13,  1, ...,  0,  0,  0],\n",
      "       [ 2, 13,  1, ...,  0,  0,  0],\n",
      "       [ 2, 13,  1, ...,  0,  0,  0]])>, 'input_mask': <tf.Tensor: shape=(16, 128), dtype=int64, numpy=\n",
      "array([[1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0]])>, 'segment_ids': <tf.Tensor: shape=(16, 128), dtype=int64, numpy=\n",
      "array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]])>}\n",
      "{'input_ids': <tf.Tensor: shape=(16, 128), dtype=int64, numpy=\n",
      "array([[  2,  13,   1, ...,   0,   0,   0],\n",
      "       [  2,  51, 236, ...,   0,   0,   0],\n",
      "       [  2,  13,   1, ...,   0,   0,   0],\n",
      "       ...,\n",
      "       [  2,  13,   1, ...,   0,   0,   0],\n",
      "       [  2,  13,   1, ...,   0,   0,   0],\n",
      "       [  2,  13,   1, ...,   0,   0,   0]])>, 'input_mask': <tf.Tensor: shape=(16, 128), dtype=int64, numpy=\n",
      "array([[1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0]])>, 'segment_ids': <tf.Tensor: shape=(16, 128), dtype=int64, numpy=\n",
      "array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]])>}\n",
      "{'input_ids': <tf.Tensor: shape=(16, 128), dtype=int64, numpy=\n",
      "array([[   2, 8827, 2196, ...,    0,    0,    0],\n",
      "       [   2,   13,    1, ...,    0,    0,    0],\n",
      "       [   2,   13,    1, ...,    0,    0,    0],\n",
      "       ...,\n",
      "       [   2,  376,  448, ...,    0,    0,    0],\n",
      "       [   2,   13,    1, ...,    0,    0,    0],\n",
      "       [   2,   13,    1, ...,    0,    0,    0]])>, 'input_mask': <tf.Tensor: shape=(16, 128), dtype=int64, numpy=\n",
      "array([[1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0]])>, 'segment_ids': <tf.Tensor: shape=(16, 128), dtype=int64, numpy=\n",
      "array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]])>}\n",
      "{'input_ids': <tf.Tensor: shape=(16, 128), dtype=int64, numpy=\n",
      "array([[    2,    13,     1, ...,     0,     0,     0],\n",
      "       [    2,    21,    57, ...,     0,     0,     0],\n",
      "       [    2,    13,     1, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [    2,    13,     1, ...,     0,     0,     0],\n",
      "       [    2, 24339,     3, ...,     0,     0,     0],\n",
      "       [    2,    13,     1, ...,     0,     0,     0]])>, 'input_mask': <tf.Tensor: shape=(16, 128), dtype=int64, numpy=\n",
      "array([[1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0]])>, 'segment_ids': <tf.Tensor: shape=(16, 128), dtype=int64, numpy=\n",
      "array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]])>}\n"
     ]
    }
   ],
   "source": [
    "for line in prediction_dataset.take(5):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.9895891 ,  0.38930255, -0.2897632 , ...,  1.347748  ,\n",
       "        -0.7115326 ,  0.43504658],\n",
       "       [-0.5799464 , -0.69323003,  0.49325743, ...,  1.1278716 ,\n",
       "        -0.97258717, -0.09923831],\n",
       "       [-0.9445586 , -1.2251675 , -0.37769505, ..., -0.8239982 ,\n",
       "        -0.5968072 , -0.9161253 ],\n",
       "       ...,\n",
       "       [-1.640143  , -0.58176893, -1.2308036 , ..., -0.83895874,\n",
       "        -0.7102679 , -1.271832  ],\n",
       "       [ 0.96426886, -0.1527807 , -0.3959958 , ...,  2.149941  ,\n",
       "        -0.50963145,  0.30589893],\n",
       "       [ 1.1327688 ,  1.637278  , -1.304856  , ...,  3.4651942 ,\n",
       "        -0.15032795,  0.1539227 ]], dtype=float32)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model.predict(prediction_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model,core_model = get_albert_model(bert_config=bert_config,\n",
    "                                                               float_type=tf.float32,\n",
    "                                                              num_labels=num_classes,\n",
    "                                                              max_seq_length=max_seq_length,\n",
    "                                                               hub_module_url=bert_hub_url)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# @tf.function\n",
    "def get_model_part1(bert_config,float_type,num_labels,max_seq_length,final_layer_initializer=None,hub_module_url=None):\n",
    "#     if final_layer_initializer is not None:  \n",
    "#         initializer = final_layer_initializer\n",
    "#     else:\n",
    "#         initializer = tf.keras.initializers.TruncatedNormal(stddev=bert_config.initializer_range)\n",
    "\n",
    "    if not hub_module_url:\n",
    "        #TODO\n",
    "        print(\"create the model\")\n",
    "        return None\n",
    "#     input_word_ids = tf.keras.Input(shape=(max_seq_length),dtype=tf.int32,name='input_word_ids')\n",
    "    input_ids = tf.keras.Input(shape=(max_seq_length),dtype=tf.int32,name='input_ids')\n",
    "    input_mask = tf.keras.Input(shape=(max_seq_length),dtype=tf.int32,name='input_mask')\n",
    "#     input_type_ids = tf.keras.Input(shape=(max_seq_length),dtype=tf.int32,name='input_type_ids')\n",
    "    segment_ids = tf.keras.Input(shape=(max_seq_length),dtype=tf.int32,name='segment_ids')\n",
    "    return input_ids,input_mask,segment_ids"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "def get_model_bert(bert_config,float_type,num_labels,max_seq_length,final_layer_initializer,hub_module_url,\n",
    "                   input_ids,input_mask,segment_ids):\n",
    "    tags = set()\n",
    "    tags.add('train')\n",
    "\n",
    "    ### pooled_output will give the representation for [CLS]\n",
    "    ### sequence_output will give representations for all tokens\n",
    "#     bert_model = hub.KerasLayer(albert_hub_url,trainable=True,tags=tags,signature='tokens',output_key='pooled_output')\n",
    "    bert_model = hub.KerasLayer(albert_hub_url,trainable=True,tags=None)\n",
    "    print(' bert_model ',bert_model)\n",
    "#     pooled_output,_ = bert_model([input_word_ids,input_mask,input_type_ids]) \n",
    "#     pooled_output,_ = bert_model(input_ids=input_ids,input_mask=input_mask,segment_ids=segment_ids) \n",
    "#     pooled_output,_ = bert_model(inputs = [input_ids,input_mask,segment_ids]) \n",
    "#     pooled_output,temp = bert_model(inputs = {'input_ids':input_ids,'input_mask':input_mask,'segment_ids':segment_ids})\n",
    "#     pooled_output, _ = bert_model([input_word_ids, input_mask, input_type_ids])\n",
    "    pooled_output, _ = bert_model([input_ids, input_mask, segment_ids])\n",
    "    output = tf.keras.layers.Dropout(rate=bert_config.hidden_dropout_prob)(pooled_output)                                                       \n",
    "#     pooled_output = bert_model(inputs = {'input_ids':input_ids,'input_mask':input_mask,'segment_ids':segment_ids})\n",
    "#     output = tf.keras.layers.Dropout(rate = bert_config.hidden_dropout_prob)(pooled_output)\n",
    "#     pooled_output = bert_outputs['pooled_output']\n",
    "#     output = pooled_output\n",
    "#     print('pooled_output ',pooled_output,' temp '+temp)\n",
    "    print('pooled_output ',pooled_output)\n",
    "#     output = bert_outputs['pooled_output']\n",
    "    return output,bert_model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def decode_record(record, name_to_features):\n",
    "    \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n",
    "    example = tf.io.parse_single_example(record, name_to_features)\n",
    "\n",
    "    # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n",
    "    # So cast all int64 to int32.\n",
    "    for name in list(example.keys()):\n",
    "        t = example[name]\n",
    "        if t.dtype == tf.int64:\n",
    "            t = tf.cast(t, tf.int32)\n",
    "        example[name] = t\n",
    "\n",
    "    return example\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def single_file_dataset(input_file, name_to_features):\n",
    "    \"\"\"Creates a single-file dataset to be passed for BERT custom training.\"\"\"\n",
    "    # For training, we want a lot of parallel reading and shuffling.\n",
    "    # For eval, we want no shuffling and parallel reading doesn't matter.\n",
    "    d = tf.data.TFRecordDataset(input_file)\n",
    "    d = d.map(lambda record: decode_record(record, name_to_features))\n",
    "\n",
    "    # When `input_file` is a path to a single file or a list\n",
    "    # containing a single path, disable auto sharding so that\n",
    "    # same input file is sent to all workers.\n",
    "    if isinstance(input_file, str) or len(input_file) == 1:\n",
    "        options = tf.data.Options()\n",
    "        options.experimental_distribute.auto_shard_policy = (\n",
    "                tf.data.experimental.AutoShardPolicy.OFF)\n",
    "        d = d.with_options(options)\n",
    "    return d\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def create_classifier_dataset(file_path,\n",
    "                              seq_length,\n",
    "                              batch_size,\n",
    "                              is_training=True,\n",
    "                              input_pipeline_context=None):\n",
    "  \"\"\"Creates input dataset from (tf)records files for train/eval.\"\"\"\n",
    "  name_to_features = {\n",
    "      'input_ids': tf.io.FixedLenFeature([seq_length], tf.int64),\n",
    "      'input_mask': tf.io.FixedLenFeature([seq_length], tf.int64),\n",
    "      'segment_ids': tf.io.FixedLenFeature([seq_length], tf.int64),\n",
    "      'label_id': tf.io.FixedLenFeature([], tf.int64)\n",
    "#       'is_real_example': tf.io.FixedLenFeature([], tf.int64),\n",
    "  }\n",
    "  dataset = single_file_dataset(file_path, name_to_features)\n",
    "\n",
    "  # The dataset is always sharded by number of hosts.\n",
    "  # num_input_pipelines is the number of hosts rather than number of cores.\n",
    "  if input_pipeline_context and input_pipeline_context.num_input_pipelines > 1:\n",
    "    dataset = dataset.shard(input_pipeline_context.num_input_pipelines,\n",
    "                            input_pipeline_context.input_pipeline_id)\n",
    "\n",
    "  def _select_data_from_record(record):\n",
    "#     x = {\n",
    "#         'input_word_ids': record['input_ids'],\n",
    "#         'input_mask': record['input_mask'],\n",
    "#         'input_type_ids': record['segment_ids']\n",
    "#     }\n",
    "    x = {\n",
    "        'input_ids': record['input_ids'],\n",
    "        'input_mask': record['input_mask'],\n",
    "        'segment_ids': record['segment_ids']\n",
    "    }\n",
    "    y = record['label_id']\n",
    "    return (x, y)\n",
    "\n",
    "  dataset = dataset.map(_select_data_from_record)\n",
    "\n",
    "  if is_training:\n",
    "    dataset = dataset.shuffle(100)\n",
    "    dataset = dataset.repeat()\n",
    "\n",
    "  dataset = dataset.batch(batch_size, drop_remainder=is_training)\n",
    "  dataset = dataset.prefetch(1024)\n",
    "  return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_fn(input_file_pattern, max_seq_length, global_batch_size,\n",
    "                   is_training):\n",
    "  \"\"\"Gets a closure to create a dataset.\"\"\"\n",
    "  def _dataset_fn(ctx=None):\n",
    "    \"\"\"Returns tf.data.Dataset for distributed BERT pretraining.\"\"\"\n",
    "    batch_size = ctx.get_per_replica_batch_size(\n",
    "        global_batch_size) if ctx else global_batch_size\n",
    "#     dataset = input_pipeline.create_classifier_dataset(\n",
    "    dataset = create_classifier_dataset(\n",
    "        input_file_pattern,\n",
    "        max_seq_length,\n",
    "        batch_size,\n",
    "        is_training=is_training,\n",
    "        input_pipeline_context=ctx)\n",
    "    return dataset\n",
    "  return _dataset_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_fn = get_dataset_fn(\n",
    "#       FLAGS.train_data_path,\n",
    "      train_data_path,\n",
    "      max_seq_length,\n",
    "#       FLAGS.train_batch_size,\n",
    "      train_batch_size,\n",
    "      is_training=True)\n",
    "eval_input_fn = get_dataset_fn(\n",
    "#       FLAGS.eval_data_path,\n",
    "      eval_data_path,\n",
    "      max_seq_length,\n",
    "#       FLAGS.eval_batch_size,\n",
    "      eval_batch_size,\n",
    "      is_training=False)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# @tf.function\n",
    "def get_model_part3(bert_output,num_labels,final_layer_initializer):\n",
    "    if final_layer_initializer is not None:  \n",
    "        initializer = final_layer_initializer\n",
    "    else:\n",
    "        initializer = tf.keras.initializers.TruncatedNormal(stddev=bert_config.initializer_range)\n",
    "    #     output = tf.keras.layers.Dropout(rate = bert_config.hidden_dropout_prob)(pooled_output)\n",
    "    output = tf.keras.layers.Dense(num_labels,kernel_initializer=initializer,name='output',dtype=tf.float32)(bert_output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "def combine_model(bert_config,float_type,num_labels,max_seq_length,final_layer_initializer=None,hub_module_url=None):\n",
    "    input_ids,input_mask,segment_ids = get_model_part1(bert_config,float_type,num_labels,max_seq_length,final_layer_initializer,hub_module_url)\n",
    "    bert_output,bert_model = get_model_bert(bert_config,float_type,num_labels,max_seq_length,final_layer_initializer,hub_module_url,input_ids,input_mask,segment_ids)\n",
    "    print('bert_output',bert_output)\n",
    "    output = get_model_part3(bert_output,num_labels,final_layer_initializer)\n",
    "    return tf.keras.Model(inputs={'input_ids':input_ids,\n",
    "                                 'input_mask':input_mask,\n",
    "                                 'segment_ids':segment_ids},\n",
    "                         outputs=output),bert_model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# @tf.function\n",
    "def get_classifier_model(bert_config,float_type,num_labels,max_seq_length,final_layer_initializer=None,hub_module_url=None):\n",
    "    if final_layer_initializer is not None:  \n",
    "        initializer = final_layer_initializer\n",
    "    else:\n",
    "        initializer = tf.keras.initializers.TruncatedNormal(stddev=bert_config.initializer_range)\n",
    "\n",
    "    if not hub_module_url:\n",
    "        #TODO\n",
    "        print(\"create the model\")\n",
    "        return None\n",
    "#     input_word_ids = tf.keras.Input(shape=(max_seq_length),dtype=tf.int32,name='input_word_ids')\n",
    "    input_ids = tf.keras.Input(shape=(max_seq_length),dtype=tf.int32,name='input_ids')\n",
    "    input_mask = tf.keras.Input(shape=(max_seq_length),dtype=tf.int32,name='input_mask')\n",
    "#     input_type_ids = tf.keras.Input(shape=(max_seq_length),dtype=tf.int32,name='input_type_ids')\n",
    "    segment_ids = tf.keras.Input(shape=(max_seq_length),dtype=tf.int32,name='segment_ids')\n",
    "    tags = set()\n",
    "    tags.add('train')\n",
    "#     bert_module = hub.Module(hub_module_url,trainable=True)\n",
    "    bert_module = hub.load(hub_module_url,tags=tags)\n",
    "    bert_inputs = {'input_ids':input_ids,\n",
    "                   'input_mask':input_mask,\n",
    "                   'segment_ids':segment_ids}\n",
    "    # https://www.tensorflow.org/hub/common_issues\n",
    "#     bert_outputs = bert_module.signatures['tokens'](\n",
    "#       inputs=bert_inputs,\n",
    "#       signature=\"tokens\",\n",
    "#       as_dict=True)\n",
    "\n",
    "#     bert_outputs = bert_module.signatures['tokens'](\n",
    "#       bert_inputs)\n",
    "    bert_outputs = bert_module.signatures['tokens'](input_ids=input_ids,input_mask=input_mask,segment_ids=segment_ids)\n",
    "    print(bert_outputs)\n",
    "    \n",
    "#     bert_model = hub.KerasLayer(albert_hub_url,trainable=True,tags=tags,signature='tokens',signature_outputs_as_dict=True)\n",
    "#     pooled_output,_ = bert_model([input_word_ids,input_mask,input_type_ids]) \n",
    "#     pooled_output,_ = bert_model([segment_ids,input_ids,input_mask]) \n",
    "#     output = tf.keras.layers.Dropout(rate = bert_config.hidden_dropout_prob)(pooled_output)\n",
    "#     pooled_output = bert_outputs['pooled_output']\n",
    "    output = bert_outputs['pooled_output']\n",
    "#     output = tf.keras.layers.Dropout(rate = bert_config.hidden_dropout_prob)(pooled_output)\n",
    "    output = tf.keras.layers.Dense(num_labels,kernel_initializer=initializer,name='output',dtype=tf.float32)(output)\n",
    "    return tf.keras.Model(inputs={'input_ids':input_ids,\n",
    "                                 'input_mask':input_mask,\n",
    "                                 'segment_ids':segment_ids},\n",
    "                         outputs=output),bert_model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://tfhub.dev/google/albert_base/2'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "albert_hub_url"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "def get_model_bert_old(bert_config,float_type,num_labels,max_seq_length,final_layer_initializer,hub_module_url,\n",
    "                   input_ids,input_mask,segment_ids):\n",
    "    tags = set()\n",
    "    tags.add('train')\n",
    "    print(hub_module_url)\n",
    "#     bert_module = hub.Module(hub_module_url,trainable=True)\n",
    "\n",
    "#     bert_module = hub.load(hub_module_url,tags=tags)\n",
    "#     bert_inputs = {'input_ids':input_ids,\n",
    "#                    'input_mask':input_mask,\n",
    "#                    'segment_ids':segment_ids}\n",
    "\n",
    "    # https://www.tensorflow.org/hub/common_issues\n",
    "#     bert_outputs = bert_module.signatures['tokens'](\n",
    "#       inputs=bert_inputs,\n",
    "#       signature=\"tokens\",\n",
    "#       as_dict=True)\n",
    "\n",
    "#     bert_outputs = bert_module.signatures['tokens'](bert_inputs) #bert_outputs\n",
    "#     bert_outputs = bert_module.signatures['tokens'](input_ids=input_ids,input_mask=input_mask,segment_ids=segment_ids)\n",
    "#     print(bert_outputs)\n",
    "    \n",
    "#     bert_model = hub.KerasLayer(albert_hub_url,trainable=True,tags=tags,signature='tokens',signature_outputs_as_dict=True)\n",
    "    \n",
    "    ### pooled_output will give the representation for [CLS]\n",
    "    ### sequence_output will give representations for all tokens\n",
    "#     bert_model = hub.KerasLayer(albert_hub_url,trainable=True,tags=tags,signature='tokens',output_key='pooled_output')\n",
    "    bert_model = hub.KerasLayer(albert_hub_url,trainable=True,tags=None)\n",
    "    print(' bert_model ',bert_model)\n",
    "#     pooled_output,_ = bert_model([input_word_ids,input_mask,input_type_ids]) \n",
    "#     pooled_output,_ = bert_model(input_ids=input_ids,input_mask=input_mask,segment_ids=segment_ids) \n",
    "#     pooled_output,_ = bert_model(inputs = [input_ids,input_mask,segment_ids]) \n",
    "#     pooled_output,temp = bert_model(inputs = {'input_ids':input_ids,'input_mask':input_mask,'segment_ids':segment_ids})\n",
    "    pooled_output = bert_model(inputs = {'input_ids':input_ids,'input_mask':input_mask,'segment_ids':segment_ids})\n",
    "#     output = tf.keras.layers.Dropout(rate = bert_config.hidden_dropout_prob)(pooled_output)\n",
    "#     pooled_output = bert_outputs['pooled_output']\n",
    "    output = pooled_output\n",
    "#     print('pooled_output ',pooled_output,' temp '+temp)\n",
    "    print('pooled_output ',pooled_output)\n",
    "#     output = bert_outputs['pooled_output']\n",
    "    return output,bert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/var/extra/users/jgeorge/tf2.0/input/dish/models/albert_en_large'"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init_checkpoint=output_folder\n",
    "init_checkpoint=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = hub.KerasLayer(albert_hub_url,trainable=True,tags=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_config=albert_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " bert_model  <tensorflow_hub.keras_layer.KerasLayer object at 0x7f70b5d017d0>\n",
      "pooled_output  Tensor(\"keras_layer_15/Identity:0\", shape=(None, 1024), dtype=float32)\n",
      "bert_output Tensor(\"dropout_4/Identity:0\", shape=(None, 1024), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "classifier_model,core_model = combine_model(bert_config=albert_config,\n",
    "                                                               float_type=tf.float32,\n",
    "                                                              num_labels=num_classes,\n",
    "                                                              max_seq_length=max_seq_length,\n",
    "                                                               hub_module_url=albert_hub_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://tfhub.dev/tensorflow/albert_en_large/1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The SavedModel at /tmp/tfhub_modules/d0ceaf43f67b8744561ebeeaea4c7c188a6e6f78 has one MetaGraph with tags ['serve'], but got an incompatible argument tags={'train'} to tf.saved_model.load. You may omit it, pass 'None', or pass matching tags.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-135-6425cb3f7ba7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m       \u001b[0meval_input_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m       \u001b[0mrun_eagerly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m       use_keras_compile_fit=False)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-114-71fcfd2258d0>\u001b[0m in \u001b[0;36mrun_bert_classifier\u001b[0;34m(strategy, bert_config, input_meta_data, model_dir, epochs, steps_per_epoch, steps_per_loop, eval_steps, warmup_steps, initial_lr, init_checkpoint, train_input_fn, eval_input_fn, custom_callbacks, run_eagerly, use_keras_compile_fit)\u001b[0m\n\u001b[1;32m     53\u001b[0m       \u001b[0mmetric_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m       \u001b[0mcustom_callbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_callbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m       run_eagerly=run_eagerly)\n\u001b[0m",
      "\u001b[0;32m/var/extra/users/jgeorge/tf2.0/git/models/official/modeling/model_training_utils.py\u001b[0m in \u001b[0;36mrun_customized_training_loop\u001b[0;34m(_sentinel, strategy, model_fn, loss_fn, scale_loss, model_dir, train_input_fn, steps_per_epoch, steps_per_loop, epochs, eval_input_fn, eval_steps, metric_fn, init_checkpoint, custom_callbacks, run_eagerly, sub_model_export_name, explicit_allreduce, pre_allreduce_callbacks, post_allreduce_callbacks)\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0;31m# To correctly place the model weights on accelerators,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;31m# model and optimizer should be created in scope.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'optimizer'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m       raise ValueError('User should set optimizer attribute to model '\n",
      "\u001b[0;32m<ipython-input-114-71fcfd2258d0>\u001b[0m in \u001b[0;36m_get_classifier_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m                                                                   \u001b[0mnum_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                                                                   \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                                                                    hub_module_url=albert_hub_url)\n\u001b[0m\u001b[1;32m     32\u001b[0m         classifier_model.optimizer = optimization.create_optimizer(init_lr=initial_lr,\n\u001b[1;32m     33\u001b[0m                                                                    \u001b[0mnum_train_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-84-2662d067a25b>\u001b[0m in \u001b[0;36mcombine_model\u001b[0;34m(bert_config, float_type, num_labels, max_seq_length, final_layer_initializer, hub_module_url)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcombine_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfloat_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfinal_layer_initializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhub_module_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msegment_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model_part1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfloat_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfinal_layer_initializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhub_module_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mbert_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbert_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model_bert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfloat_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfinal_layer_initializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhub_module_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msegment_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bert_output'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbert_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model_part3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfinal_layer_initializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-129-e276c6602062>\u001b[0m in \u001b[0;36mget_model_bert\u001b[0;34m(bert_config, float_type, num_labels, max_seq_length, final_layer_initializer, hub_module_url, input_ids, input_mask, segment_ids)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m### pooled_output will give the representation for [CLS]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m### sequence_output will give representations for all tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mbert_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKerasLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malbert_hub_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tokens'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pooled_output'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' bert_model '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbert_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m#     pooled_output,_ = bert_model([input_word_ids,input_mask,input_type_ids])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow_hub/keras_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, handle, trainable, arguments, _sentinel, tags, signature, signature_outputs_as_dict, output_key, output_shape, **kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m           _convert_nest_to_shapes(output_shape))\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_training_argument\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_has_training_argument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_hub_module_v1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_is_hub_module_v1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow_hub/keras_layer.py\u001b[0m in \u001b[0;36mload_module\u001b[0;34m(handle, tags)\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodule_v2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow_hub/module_v2.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(handle, tags)\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtags\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_hub_module_v1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m       \u001b[0mtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m   \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_v1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m   \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_hub_module_v1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_hub_module_v1\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(export_dir, tags)\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mdon\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0mt\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0ma\u001b[0m \u001b[0mMetaGraph\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mSavedModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m   \"\"\"\n\u001b[0;32m--> 578\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mload_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36mload_internal\u001b[0;34m(export_dir, tags, loader_cls)\u001b[0m\n\u001b[1;32m    597\u001b[0m            \u001b[0;34m\"incompatible argument tags={} to tf.saved_model.load. You may omit \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m            \"it, pass 'None', or pass matching tags.\")\n\u001b[0;32m--> 599\u001b[0;31m           .format(export_dir, meta_graph_def.meta_info_def.tags, tags))\n\u001b[0m\u001b[1;32m    600\u001b[0m     \u001b[0mobject_graph_proto\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta_graph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobject_graph_def\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The SavedModel at /tmp/tfhub_modules/d0ceaf43f67b8744561ebeeaea4c7c188a6e6f78 has one MetaGraph with tags ['serve'], but got an incompatible argument tags={'train'} to tf.saved_model.load. You may omit it, pass 'None', or pass matching tags."
     ]
    }
   ],
   "source": [
    "trained_model = run_bert_classifier(\n",
    "      strategy,\n",
    "      albert_config,\n",
    "      input_meta_data,\n",
    "      albert_model_dir,\n",
    "      epochs,\n",
    "      steps_per_epoch,\n",
    "      steps_per_loop,\n",
    "      eval_steps,\n",
    "      warmup_steps,\n",
    "      learning_rate,\n",
    "      init_checkpoint,\n",
    "      train_input_fn,\n",
    "      eval_input_fn,\n",
    "      run_eagerly=False,\n",
    "      use_keras_compile_fit=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clear the existing tensorflow graph\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_bert_classifier(strategy,\n",
    "                        bert_config,\n",
    "                        input_meta_data,\n",
    "                        model_dir,\n",
    "                        epochs,\n",
    "                        steps_per_epoch,\n",
    "                        steps_per_loop,\n",
    "                        eval_steps,\n",
    "                        warmup_steps,\n",
    "                        initial_lr,\n",
    "                        init_checkpoint,\n",
    "                        train_input_fn,\n",
    "                        eval_input_fn,\n",
    "                        custom_callbacks=None,\n",
    "                        run_eagerly=False,\n",
    "                        use_keras_compile_fit=False):\n",
    "    \"\"\"Run BERT classifier training using low-level API.\"\"\"\n",
    "    max_seq_length = input_meta_data['max_seq_length']\n",
    "    num_classes = input_meta_data['num_labels']\n",
    "    def _get_classifier_model():\n",
    "#     bert_models.classifier_model\n",
    "#     classifier_model,core_model = get_classifier_model(bert_config=bert_config,\n",
    "#                                                                float_type=tf.float32,\n",
    "#                                                               num_labels=num_classes,\n",
    "#                                                               max_seq_length=max_seq_length,\n",
    "#                                                                hub_module_url=albert_hub_url)\n",
    "        classifier_model,core_model = combine_model(bert_config=bert_config,\n",
    "                                                                   float_type=tf.float32,\n",
    "                                                                  num_labels=num_classes,\n",
    "                                                                  max_seq_length=max_seq_length,\n",
    "                                                                   hub_module_url=albert_hub_url)\n",
    "        classifier_model.optimizer = optimization.create_optimizer(init_lr=initial_lr,\n",
    "                                                                   num_train_steps=steps_per_epoch*epochs,\n",
    "                                                                   num_warmup_steps=warmup_steps)\n",
    "        return classifier_model,core_model\n",
    "    loss_multiplier = 1\n",
    "    loss_fn = get_loss_fn(num_classes,loss_multiplier)\n",
    "    \n",
    "    def metric_fn():\n",
    "        return tf.keras.metrics.SparseCategoricalAccuracy('test_accuracy',dtype=tf.float32)\n",
    "    return model_training_utils.run_customized_training_loop(\n",
    "      strategy=strategy,\n",
    "      model_fn=_get_classifier_model,\n",
    "      loss_fn=loss_fn,\n",
    "      model_dir=model_dir,\n",
    "      steps_per_epoch=steps_per_epoch,\n",
    "      steps_per_loop=steps_per_loop,\n",
    "      epochs=epochs,\n",
    "      train_input_fn=train_input_fn,\n",
    "      eval_input_fn=eval_input_fn,\n",
    "      eval_steps=eval_steps,\n",
    "      init_checkpoint=init_checkpoint,\n",
    "      metric_fn=metric_fn,\n",
    "      custom_callbacks=custom_callbacks,\n",
    "      run_eagerly=run_eagerly)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/var/extra/users/jgeorge/tf2.0/input/albert_base'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "albert_model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy.experimental_run_v2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir_dish=os.path.join(main_exp_folder,'models')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/var/extra/users/jgeorge/tf2.0/input/dish/data/jan17_2020/models'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dir_dish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_checkpoint=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<official.nlp.albert.configs.AlbertConfig at 0x7f74029d7ad0>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "albert_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " bert_model  <tensorflow_hub.keras_layer.KerasLayer object at 0x7f7402ebb150>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    /opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow_hub/keras_layer.py:206 call  *\n        self._check_trainability()\n    /opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow_hub/keras_layer.py:265 _check_trainability  *\n        raise ValueError(\n\n    ValueError: Setting hub.KerasLayer.trainable = True is unsupported when loading from the hub.Module format of TensorFlow 1.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-115-6425cb3f7ba7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m       \u001b[0meval_input_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m       \u001b[0mrun_eagerly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m       use_keras_compile_fit=False)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-114-71fcfd2258d0>\u001b[0m in \u001b[0;36mrun_bert_classifier\u001b[0;34m(strategy, bert_config, input_meta_data, model_dir, epochs, steps_per_epoch, steps_per_loop, eval_steps, warmup_steps, initial_lr, init_checkpoint, train_input_fn, eval_input_fn, custom_callbacks, run_eagerly, use_keras_compile_fit)\u001b[0m\n\u001b[1;32m     53\u001b[0m       \u001b[0mmetric_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m       \u001b[0mcustom_callbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_callbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m       run_eagerly=run_eagerly)\n\u001b[0m",
      "\u001b[0;32m/var/extra/users/jgeorge/tf2.0/git/models/official/modeling/model_training_utils.py\u001b[0m in \u001b[0;36mrun_customized_training_loop\u001b[0;34m(_sentinel, strategy, model_fn, loss_fn, scale_loss, model_dir, train_input_fn, steps_per_epoch, steps_per_loop, epochs, eval_input_fn, eval_steps, metric_fn, init_checkpoint, custom_callbacks, run_eagerly, sub_model_export_name, explicit_allreduce, pre_allreduce_callbacks, post_allreduce_callbacks)\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0;31m# To correctly place the model weights on accelerators,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;31m# model and optimizer should be created in scope.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'optimizer'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m       raise ValueError('User should set optimizer attribute to model '\n",
      "\u001b[0;32m<ipython-input-114-71fcfd2258d0>\u001b[0m in \u001b[0;36m_get_classifier_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m                                                                   \u001b[0mnum_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                                                                   \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                                                                    hub_module_url=albert_hub_url)\n\u001b[0m\u001b[1;32m     32\u001b[0m         classifier_model.optimizer = optimization.create_optimizer(init_lr=initial_lr,\n\u001b[1;32m     33\u001b[0m                                                                    \u001b[0mnum_train_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-84-2662d067a25b>\u001b[0m in \u001b[0;36mcombine_model\u001b[0;34m(bert_config, float_type, num_labels, max_seq_length, final_layer_initializer, hub_module_url)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcombine_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfloat_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfinal_layer_initializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhub_module_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msegment_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model_part1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfloat_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfinal_layer_initializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhub_module_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mbert_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbert_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model_bert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfloat_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfinal_layer_initializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhub_module_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msegment_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bert_output'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbert_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model_part3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfinal_layer_initializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-82-0556aeefba5b>\u001b[0m in \u001b[0;36mget_model_bert\u001b[0;34m(bert_config, float_type, num_labels, max_seq_length, final_layer_initializer, hub_module_url, input_ids, input_mask, segment_ids)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m#     pooled_output,_ = bert_model(inputs = [input_ids,input_mask,segment_ids])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m#     pooled_output,temp = bert_model(inputs = {'input_ids':input_ids,'input_mask':input_mask,'segment_ids':segment_ids})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mpooled_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'input_mask'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0minput_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'segment_ids'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msegment_ids\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;31m#     output = tf.keras.layers.Dropout(rate = bert_config.hidden_dropout_prob)(pooled_output)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m#     pooled_output = bert_outputs['pooled_output']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    920\u001b[0m                     not base_layer_utils.is_in_eager_or_tf_function()):\n\u001b[1;32m    921\u001b[0m                   \u001b[0;32mwith\u001b[0m \u001b[0mauto_control_deps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAutomaticControlDependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0macd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 922\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    923\u001b[0m                     \u001b[0;31m# Wrap Tensors in `outputs` in `tf.identity` to avoid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m                     \u001b[0;31m# circular dependencies.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow_hub/keras_layer.py:206 call  *\n        self._check_trainability()\n    /opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow_hub/keras_layer.py:265 _check_trainability  *\n        raise ValueError(\n\n    ValueError: Setting hub.KerasLayer.trainable = True is unsupported when loading from the hub.Module format of TensorFlow 1.\n"
     ]
    }
   ],
   "source": [
    "trained_model = run_bert_classifier(\n",
    "      strategy,\n",
    "      albert_config,\n",
    "      input_meta_data,\n",
    "      albert_model_dir,\n",
    "      epochs,\n",
    "      steps_per_epoch,\n",
    "      steps_per_loop,\n",
    "      eval_steps,\n",
    "      warmup_steps,\n",
    "      learning_rate,\n",
    "      init_checkpoint,\n",
    "      train_input_fn,\n",
    "      eval_input_fn,\n",
    "      run_eagerly=False,\n",
    "      use_keras_compile_fit=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to find data adapter that can handle input: <class 'function'>, <class 'NoneType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-81e796e74545>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrained_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_input_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.1/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    928\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    931\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m   def predict(self,\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.1/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, model, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m         use_multiprocessing=use_multiprocessing, **kwargs)\n\u001b[0m\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m   def predict(self, model, x, batch_size=None, verbose=0, steps=None,\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.1/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_model_iteration\u001b[0;34m(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    424\u001b[0m           \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m           use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    427\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m       \u001b[0muse_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_samples\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.1/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[0;34m(model, mode, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    634\u001b[0m                     use_multiprocessing=False):\n\u001b[1;32m    635\u001b[0m   \u001b[0;34m\"\"\"Process the inputs for fit/eval/predict().\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 636\u001b[0;31m   \u001b[0madapter_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_adapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_data_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    637\u001b[0m   standardize = functools.partial(\n\u001b[1;32m    638\u001b[0m       \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.1/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mselect_data_adapter\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    996\u001b[0m         \u001b[0;34m\"Failed to find data adapter that can handle \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    997\u001b[0m         \"input: {}, {}\".format(\n\u001b[0;32m--> 998\u001b[0;31m             _type_name(x), _type_name(y)))\n\u001b[0m\u001b[1;32m    999\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter_cls\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1000\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to find data adapter that can handle input: <class 'function'>, <class 'NoneType'>"
     ]
    }
   ],
   "source": [
    "trained_model.evaluate(eval_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_fn():\n",
    "    return tf.keras.metrics.SparseCategoricalAccuracy('test_accuracy',dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = metric_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(eval_input_fn,model):\n",
    "    eval_iter = iter(strategy.experimental_distribute_datasets_from_function(eval_input_fn))\n",
    "    \n",
    "    def _test_step_fn(inputs,label):\n",
    "#         inputs,label = inputs\n",
    "        model_outputs = model(inputs,training=False)\n",
    "        metric.update_state(label,model_outputs)\n",
    "    strategy.experimental_run_v2(_test_step_fn,args=(next(eval_iter)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(eval_input_fn,trained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.125>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_export_dir = '/var/extra/users/jgeorge/tf2.0/input/albert_base_custom/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spm_model = os.path.join(model_dir, \"assets\", \"30k-clean.model\")\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(spm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model.sp_model_file = tf.saved_model.Asset(spm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.tracking.Asset at 0x7f54e0bc8310>"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model.sp_model_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/extra/users/jgeorge/tf2.0/input/albert_base_custom/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/extra/users/jgeorge/tf2.0/input/albert_base_custom/assets\n"
     ]
    }
   ],
   "source": [
    "trained_model.save(model_export_dir,include_optimizer=False,save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.125>"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown signature default in https://tfhub.dev/google/albert_base/2 (available signatures: _SignatureMap({'tokenization_info': <tensorflow.python.eager.wrap_function.WrappedFunction object at 0x7f550be41450>, 'mlm': <tensorflow.python.eager.wrap_function.WrappedFunction object at 0x7f57ec038a50>, 'tokens': <tensorflow.python.eager.wrap_function.WrappedFunction object at 0x7f55086cf990>})).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-345-a847214142b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbert_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKerasLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malbert_hub_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.1/lib/python3.7/site-packages/tensorflow_hub/keras_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, handle, trainable, arguments, _sentinel, tags, signature, signature_outputs_as_dict, output_key, output_shape, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m                        \"a signature (or using a legacy Hub.Module).\")\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.1/lib/python3.7/site-packages/tensorflow_hub/keras_layer.py\u001b[0m in \u001b[0;36m_get_callable\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_signature\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignatures\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m       raise ValueError(\"Unknown signature %s in %s (available signatures: %s).\"\n\u001b[0;32m--> 250\u001b[0;31m                        % (self._signature, self._handle, self._func.signatures))\n\u001b[0m\u001b[1;32m    251\u001b[0m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_signature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown signature default in https://tfhub.dev/google/albert_base/2 (available signatures: _SignatureMap({'tokenization_info': <tensorflow.python.eager.wrap_function.WrappedFunction object at 0x7f550be41450>, 'mlm': <tensorflow.python.eager.wrap_function.WrappedFunction object at 0x7f57ec038a50>, 'tokens': <tensorflow.python.eager.wrap_function.WrappedFunction object at 0x7f55086cf990>}))."
     ]
    }
   ],
   "source": [
    "bert_model = hub.KerasLayer(albert_hub_url,trainable=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = hub.KerasLayer(albert_hub_url,trainable=True,signature='tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "in converted code:\n\n    /opt/custom/python/anaconda3/envs/tf2.1/lib/python3.7/site-packages/tensorflow_hub/keras_layer.py:209 call  *\n        result = f()\n    /opt/custom/python/anaconda3/envs/tf2.1/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py:1551 __call__  *\n        return self._call_impl(args, kwargs)\n    /opt/custom/python/anaconda3/envs/tf2.1/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py:1570 _call_impl\n        ).format(self._num_positional_args, self._arg_keywords, args))\n\n    TypeError: Expected at most 0 positional arguments (and the rest keywords, of ['segment_ids', 'input_ids', 'input_mask']), got ([<tf.Tensor 'segment_ids:0' shape=(None, 128) dtype=int32>, <tf.Tensor 'input_ids:0' shape=(None, 128) dtype=int32>, <tf.Tensor 'input_mask:0' shape=(None, 128) dtype=int32>],). When calling a concrete function, positional arguments may not be bound to Tensors within nested structures.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-126-49c96daf56c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m       \u001b[0meval_input_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m       \u001b[0mrun_eagerly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m       use_keras_compile_fit=False)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-123-6373bbe5532a>\u001b[0m in \u001b[0;36mrun_bert_classifier\u001b[0;34m(strategy, bert_config, input_meta_data, model_dir, epochs, steps_per_epoch, steps_per_loop, eval_steps, warmup_steps, initial_lr, init_checkpoint, train_input_fn, eval_input_fn, custom_callbacks, run_eagerly, use_keras_compile_fit)\u001b[0m\n\u001b[1;32m     23\u001b[0m                                                               \u001b[0mnum_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                                                               \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                                                                hub_module_url=albert_hub_url)\n\u001b[0m\u001b[1;32m     26\u001b[0m     classifier_model.optimizer = optimization.create_optimizer(init_lr=initial_lr,\n\u001b[1;32m     27\u001b[0m                                                                \u001b[0mnum_train_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-121-fef5ea442f65>\u001b[0m in \u001b[0;36mget_classifier_model\u001b[0;34m(bert_config, float_type, num_labels, max_seq_length, final_layer_initializer, hub_module_url)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mbert_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKerasLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malbert_hub_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tokens'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msignature_outputs_as_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m#     pooled_output,_ = bert_model([input_word_ids,input_mask,input_type_ids])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mpooled_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msegment_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_dropout_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpooled_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkernel_initializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'output'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.1/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    771\u001b[0m                     not base_layer_utils.is_in_eager_or_tf_function()):\n\u001b[1;32m    772\u001b[0m                   \u001b[0;32mwith\u001b[0m \u001b[0mauto_control_deps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAutomaticControlDependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0macd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 773\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    774\u001b[0m                     \u001b[0;31m# Wrap Tensors in `outputs` in `tf.identity` to avoid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m                     \u001b[0;31m# circular dependencies.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.1/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    235\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: in converted code:\n\n    /opt/custom/python/anaconda3/envs/tf2.1/lib/python3.7/site-packages/tensorflow_hub/keras_layer.py:209 call  *\n        result = f()\n    /opt/custom/python/anaconda3/envs/tf2.1/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py:1551 __call__  *\n        return self._call_impl(args, kwargs)\n    /opt/custom/python/anaconda3/envs/tf2.1/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py:1570 _call_impl\n        ).format(self._num_positional_args, self._arg_keywords, args))\n\n    TypeError: Expected at most 0 positional arguments (and the rest keywords, of ['segment_ids', 'input_ids', 'input_mask']), got ([<tf.Tensor 'segment_ids:0' shape=(None, 128) dtype=int32>, <tf.Tensor 'input_ids:0' shape=(None, 128) dtype=int32>, <tf.Tensor 'input_mask:0' shape=(None, 128) dtype=int32>],). When calling a concrete function, positional arguments may not be bound to Tensors within nested structures.\n"
     ]
    }
   ],
   "source": [
    "trained_model = run_bert_classifier(\n",
    "      strategy,\n",
    "      bert_config,\n",
    "      input_meta_data,\n",
    "      albert_model_dir,\n",
    "      epochs,\n",
    "      steps_per_epoch,\n",
    "      steps_per_loop,\n",
    "      eval_steps,\n",
    "      warmup_steps,\n",
    "      learning_rate,\n",
    "      init_checkpoint,\n",
    "      train_input_fn,\n",
    "      eval_input_fn,\n",
    "      run_eagerly=False,\n",
    "      use_keras_compile_fit=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = set()\n",
    "tags.add('train')\n",
    "\n",
    "loaded_albert_model = tf.saved_model.load(albert_model_dir,tags=tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_SignatureMap({'mlm': <tensorflow.python.eager.wrap_function.WrappedFunction object at 0x7f551de1d9d0>, 'tokens': <tensorflow.python.eager.wrap_function.WrappedFunction object at 0x7f551d98e090>, 'tokenization_info': <tensorflow.python.eager.wrap_function.WrappedFunction object at 0x7f551d5d5050>})"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_albert_model.signatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.eager.wrap_function.WrappedFunction at 0x7f551de1d9d0>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_albert_model.signatures['mlm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 - tensorflow 2.2 (tf2.2)",
   "language": "python",
   "name": "tf2.2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
