{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "# import bert\n",
    "# from bert import BertModelLayer\n",
    "import functools\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import preprocessing\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import tensorflow_text as tftext\n",
    "import sentencepiece as spm\n",
    "import sys\n",
    "\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import csv\n",
    "import collections\n",
    "import pandas as pd\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "# from absl import logging\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "sys.path.extend([\"/space/users/jgeorge/git/tensorflow_experiments/tf2.3/models\"])\n",
    "\n",
    "# pylint: disable=g-import-not-at-top,redefined-outer-name,reimported\n",
    "# from official.modeling import model_training_utils\n",
    "\n",
    "from official.nlp.modeling.models import bert_classifier, bert_pretrainer\n",
    "# from official.nlp.modeling.models.bert_classifier import BertClassifier\n",
    "# from official.nlp.modeling.models.bert_pretrainer import BertPretrainer\n",
    "# from official.nlp import bert_modeling as modeling\n",
    "# from official.nlp import bert_models\n",
    "from official.nlp import optimization\n",
    "from official.nlp.bert import common_flags\n",
    "from official.nlp.bert import input_pipeline\n",
    "from official.nlp.bert import model_saving_utils\n",
    "from official.utils.misc import distribution_utils\n",
    "from official.utils.misc import keras_utils\n",
    "from official.nlp.bert import tokenization\n",
    "\n",
    "from official.nlp.albert import configs as albert_configs\n",
    "from official.nlp.bert import run_classifier as run_classifier_bert\n",
    "\n",
    "# from tensorflow.python.keras.engine import network\n",
    "\n",
    "from tensorflow.python.keras.utils import tf_utils\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logging info [<StreamHandler stdout (NOTSET)>]\n"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/13733552/logger-configuration-to-log-to-file-and-print-to-stdout\n",
    "log_filepath = \"/var/extra/users/jgeorge/tf2.0/input/dish/multi_logs.txt\"\n",
    "file_handler = logging.handlers.RotatingFileHandler(log_filepath, maxBytes=(1048576*5), backupCount=7)\n",
    "# file_handler = logging.FileHandler()\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
    "# logging.get_absl_handler().use_absl_log_file('absl_logging', os.path.join(main_data_folder,'Smithfield/AlwaysOn/logs'))\n",
    "file_handler.setFormatter(formatter)\n",
    "\n",
    "rootLogger = logging.getLogger()\n",
    "# customLogger = logging.getLogger('TestControl')\n",
    "rootLogger.setLevel(logging.INFO)\n",
    "# removing existing loggers\n",
    "while(len(rootLogger.handlers)>0):\n",
    "    rootLogger.removeHandler(rootLogger.handlers[0])\n",
    "\n",
    "# uncomment to add logs to a file    \n",
    "# rootLogger.addHandler(file_handler)\n",
    "\n",
    "stream_handler = logging.StreamHandler(sys.stdout)\n",
    "rootLogger.addHandler(stream_handler)\n",
    "print('logging info',rootLogger.handlers )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAGS = flags.FLAGS    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:4', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:5', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:6', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:7', device_type='GPU')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "##restricting no of gpus\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "device_to_use = gpus[3]\n",
    "tf.config.experimental.set_memory_growth(device_to_use,True)\n",
    "tf.config.experimental.set_visible_devices(device_to_use, 'GPU')\n",
    "print(tf.config.get_visible_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##setting direcory for downloading tfhub modules\n",
    "os.environ[\"TFHUB_CACHE_DIR\"] = '/space/engineering/tfhub_modules'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dish_data_path='/var/extra/users/jgeorge/tf2.0/input/dish/data/jan17_2020/dishDataJan17.txt'\n",
    "df = pd.read_csv(dish_data_path,sep='\\t',header=None,names=['filename','text','granular_intent','ru_intent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>text</th>\n",
       "      <th>granular_intent</th>\n",
       "      <th>ru_intent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INT-sv1appis14-1504137880316-305603_4567</td>\n",
       "      <td>can you send my bill to my mail?</td>\n",
       "      <td>billing-preferences</td>\n",
       "      <td>billing-preferences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>INT-va1appis15-1504373018548-41332</td>\n",
       "      <td>My Wally receiver has lost Satellite signal in...</td>\n",
       "      <td>comp_part_signal_loss-issue</td>\n",
       "      <td>comp_part_signal_loss-issue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INT-sv1appis12-1503954587819-263368</td>\n",
       "      <td>I need a payment extension so i don't get my s...</td>\n",
       "      <td>payment_extension-request</td>\n",
       "      <td>payment_extension-request</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8ed8e9b1-10f5-438d-c409-e616c3ff9ede</td>\n",
       "      <td>how can i find my local channels. it seems i d...</td>\n",
       "      <td>channel_package-issue</td>\n",
       "      <td>channel_package-issue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>INT-sv1appis13-1504099917638-293735</td>\n",
       "      <td>Wanted to speak with someone about my bill</td>\n",
       "      <td>representative-request</td>\n",
       "      <td>representative-request</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   filename  \\\n",
       "0  INT-sv1appis14-1504137880316-305603_4567   \n",
       "1        INT-va1appis15-1504373018548-41332   \n",
       "2       INT-sv1appis12-1503954587819-263368   \n",
       "3      8ed8e9b1-10f5-438d-c409-e616c3ff9ede   \n",
       "4       INT-sv1appis13-1504099917638-293735   \n",
       "\n",
       "                                                text  \\\n",
       "0                   can you send my bill to my mail?   \n",
       "1  My Wally receiver has lost Satellite signal in...   \n",
       "2  I need a payment extension so i don't get my s...   \n",
       "3  how can i find my local channels. it seems i d...   \n",
       "4         Wanted to speak with someone about my bill   \n",
       "\n",
       "               granular_intent                    ru_intent  \n",
       "0          billing-preferences          billing-preferences  \n",
       "1  comp_part_signal_loss-issue  comp_part_signal_loss-issue  \n",
       "2    payment_extension-request    payment_extension-request  \n",
       "3        channel_package-issue        channel_package-issue  \n",
       "4       representative-request       representative-request  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train,df_val,y_train,y_val = train_test_split(df,df['granular_intent'],train_size=0.9,random_state=42,stratify=df['granular_intent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_exp_folder = '/space/users/jgeorge/projects/k/tensorflow2-question-answering/input/dish/data/jan17_2020/'\n",
    "main_exp_folder = '/var/extra/users/jgeorge/tf2.0/input/dish/data/jan17_2020/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = '/var/extra/users/jgeorge/tf2.0/output/dish/models/bert_multilingual'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_csv_file='/var/extra/users/jgeorge/tf2.0/input/dish/data/jan17_2020/train.txt'\n",
    "eval_csv_file='/var/extra/users/jgeorge/tf2.0/input/dish/data/jan17_2020/val.txt'\n",
    "\n",
    "df_train.to_csv(train_csv_file,sep='\\t',header=False,index=False)\n",
    "df_val.to_csv(eval_csv_file,sep='\\t',header=False,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing a file with list of intents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_file = '/var/extra/users/jgeorge/tf2.0/input/dish/data/jan17_2020/intentlist.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(intent_file,'w',encoding='utf-8') as out_f:\n",
    "    for intent in sorted(df_train['granular_intent'].unique()):\n",
    "        out_f.write(intent+'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_list = []\n",
    "with open(intent_file,'r') as inp_f:\n",
    "    for intent in inp_f:\n",
    "        intent_list.append(intent.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23146"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2572"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_meta_data = {\n",
    "    'max_seq_length':128,\n",
    "    'num_labels':len(intent_list),\n",
    "    'train_data_size':len(df_train),\n",
    "    'eval_data_size':len(df_val),\n",
    "    'default_label':'other-other'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_seq_length': 128,\n",
       " 'num_labels': 142,\n",
       " 'train_data_size': 23146,\n",
       " 'eval_data_size': 2572,\n",
       " 'default_label': 'other-other'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_meta_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_meta_path = os.path.join(main_exp_folder,'input_metadata.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writing metadata file\n",
    "with open(input_meta_path,'w',encoding='utf-8') as jf:\n",
    "    jf.write(json.dumps(input_meta_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading metadata file\n",
    "with open(input_meta_path,'r',encoding='utf-8') as jf:\n",
    "#     input_meta_data = json.loads(jf.read())\n",
    "    input_meta_data = json.load(jf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing & feeding into graph\n",
    "We will be reading the regular text file as input & encoding the text \n",
    "using sentence piece encoder provided along with albert & then\n",
    "writing it as tfrecord. Other than the sentencepiece encoder these models does not require any preprocessing like regex-replacement, lemmatization etc \n",
    "\n",
    "Writing as tfrecord is not necessary, but it is a more optimized file format for reading into \n",
    "tf.data api (you can even base your tf dataset on a textfile or even a python iterator)\n",
    "Also it's not necessary to use tf.data apis but this is much more optimized, like preloading data into gpu memory & optimization required if you are running across systems & all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence piece encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multilingual bert hub url\n",
    "bert_hub_url = \"https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# download from link given in https://github.com/tensorflow/models/tree/master/official/nlp/albert\n",
    "# specifically https://storage.googleapis.com/cloud-tpu-checkpoints/albert/checkpoints/albert_v2_large.tar.gz\n",
    "# this is not required if we are just using tensorflow hub, since the hub module didn't have a detailed config, \n",
    "# will load the config from this folder for reference  \n",
    "bert_model_dir='/space/engineering/pretrained_models/albert/albert_large'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to load the used sentence piece model from the hub module, \n",
    "it will be present in the assests folder of the downloaded albert model\n",
    "Since we are using hub here & not directly downloading it, the path can be fetched from hub layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_path /space/engineering/tfhub_modules/67c5e34716a3dd645ca3507fac096a07b2c8607d/assets/vocab.txt\n"
     ]
    }
   ],
   "source": [
    "bert_hub_loaded = hub.load(bert_hub_url)\n",
    "vocab_path = bert_hub_loaded.vocab_file.asset_path.numpy().decode('utf-8')\n",
    "print(f'vocab_path {vocab_path}')\n",
    "# or if you are using hub.KeraLayer\n",
    "# bert_model = hub.KerasLayer(bert_hub_url,trainable=True)\n",
    "# sentencepiece_path = bert_model.resolved_object.sp_model_file.asset_path.numpy()\n",
    "\n",
    "##if you have directly downloaded the model you could do this\n",
    "# sentencepiece_path = os.path.join(model_dir, \"assets\", \"30k-clean.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading TF-Hub Module 'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/1'.\n",
      "Downloaded https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/1, Total size: 3.95MB\n",
      "Downloaded TF-Hub Module 'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/1'.\n"
     ]
    }
   ],
   "source": [
    "bert_preprocessor_url = \"https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/1\"\n",
    "bert_preprocessor_loaded = hub.load(bert_preprocessor_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length=128\n",
    "bert_config = {}\n",
    "bert_config[\"initializer_range\"] = 0.02\n",
    "# we will use this for the Dropout probability between bert layer & final Dense layer\n",
    "bert_config[\"hidden_dropout_prob\"]=0.2\n",
    "num_labels = input_meta_data['num_labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bert_preprocessor_loaded.tokenize(tf.constant(\"talk to an \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = tf.keras.layers.Input(shape=(), dtype=tf.string,name='input_text')\n",
    "tokenize = hub.KerasLayer(bert_preprocessor_loaded.tokenize)\n",
    "tokenized_inputs = [tokenize(input_text)] # bert can handle 2 input_texts (segments)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_pack_inputs = hub.KerasLayer(bert_preprocessor_loaded.bert_pack_inputs,arguments=dict(seq_length=max_seq_length))\n",
    "encoder_inputs = bert_pack_inputs(tokenized_inputs)\n",
    "encoder = hub.KerasLayer(bert_hub_url,trainable=True)\n",
    "outputs = encoder(encoder_inputs)\n",
    "pooled_output = outputs[\"pooled_output\"]      # [batch_size, 768].\n",
    "sequence_output = outputs[\"sequence_output\"]  # [batch_size, seq_length, 768].\n",
    "bert_output = tf.keras.layers.Dropout(rate=bert_config[\"hidden_dropout_prob\"])(pooled_output) \n",
    "initializer = tf.keras.initializers.TruncatedNormal(stddev=bert_config[\"initializer_range\"])\n",
    "output = tf.keras.layers.Dense(num_labels,kernel_initializer=initializer,name='output',dtype=tf.float32)(bert_output)\n",
    "    \n",
    "# model = tf.keras.Model(inputs={'input_ids':input_ids,\n",
    "#                                  'input_mask':input_mask,\n",
    "#                                  'segment_ids':segment_ids},\n",
    "#                          outputs=output),bert_model\n",
    "\n",
    "model = tf.keras.Model(inputs=input_text,\n",
    "                         outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_type_ids': <tf.Tensor 'keras_layer_3/StatefulPartitionedCall:1' shape=(None, None) dtype=int32>,\n",
       " 'input_mask': <tf.Tensor 'keras_layer_3/StatefulPartitionedCall:0' shape=(None, None) dtype=int32>,\n",
       " 'input_word_ids': <tf.Tensor 'keras_layer_3/StatefulPartitionedCall:2' shape=(None, None) dtype=int32>}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_multilingual_bert(bert_config,num_labels,max_seq_length,model_hub_url,preprocessor_hub_url):\n",
    "    bert_preprocessor_loaded = hub.load(preprocessor_hub_url)\n",
    "    input_text = tf.keras.layers.Input(shape=(), dtype=tf.string,name='input_text')\n",
    "    tokenize = hub.KerasLayer(bert_preprocessor_loaded.tokenize)\n",
    "    tokenized_inputs = [tokenize(input_text)] # bert can handle 2 input_texts (segments)\n",
    "    \n",
    "    bert_pack_inputs = hub.KerasLayer(bert_preprocessor_loaded.bert_pack_inputs,arguments=dict(seq_length=max_seq_length))\n",
    "    encoder_inputs = bert_pack_inputs(tokenized_inputs)\n",
    "    encoder = hub.KerasLayer(model_hub_url,trainable=True)\n",
    "    outputs = encoder(encoder_inputs)\n",
    "    pooled_output = outputs[\"pooled_output\"]      # [batch_size, 768].\n",
    "    sequence_output = outputs[\"sequence_output\"]  # [batch_size, seq_length, 768].\n",
    "    bert_output = tf.keras.layers.Dropout(rate=bert_config[\"hidden_dropout_prob\"])(pooled_output) \n",
    "    initializer = tf.keras.initializers.TruncatedNormal(stddev=bert_config[\"initializer_range\"])\n",
    "    output_dense = tf.keras.layers.Dense(num_labels,kernel_initializer=initializer,name='output',dtype=tf.float32)(bert_output)\n",
    "\n",
    "#     return tf.keras.Model(inputs={'input_ids':input_ids,\n",
    "#                                      'input_mask':input_mask,\n",
    "#                                      'segment_ids':segment_ids},\n",
    "#                              outputs=output),bert_model\n",
    "    return tf.keras.Model(inputs=input_text,outputs=output_dense),encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the data into TFRecords\n",
    "For following functionalities refer the file  - models/official/nlp/data/classifier_data_lib.py       \n",
    "This includes even the sentence piece tokenization part\n",
    "#### Defining an object for the 1 row of text input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#in our case we are doing just classification so we only need 1 text value = text_a\n",
    "class InputExample(object):\n",
    "    def __init__(self,uuid,text_a,text_b=None,label=None):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "        Args:\n",
    "          uuid: Unique id for the example.\n",
    "          text_a: string. The untokenized text of the first sequence. For single\n",
    "            sequence tasks, only this sequence must be specified.\n",
    "          text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "            Only must be specified for sequence pair tasks.\n",
    "          label: (Optional) string. The label of the example. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "        \"\"\"\n",
    "        self.uuid = uuid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### defining an object for the 1 row of input feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "### in the official one there was something called is_real_example as feature, here i skipped it since it's \n",
    "#only classification\n",
    "# class InputFeatures(object):\n",
    "#     def __init__(self,input_ids,input_mask,\n",
    "#                 segment_ids,label_id):\n",
    "#         self.input_ids = input_ids\n",
    "#         self.input_mask = input_mask\n",
    "#         self.segment_ids = segment_ids\n",
    "#         self.label_id = label_id\n",
    "\n",
    "        \n",
    "## keeping the original text itself as features & can be processed to ids\n",
    "# with the required length during dataset.map function or during the model\n",
    "class InputFeatures(object):\n",
    "    def __init__(self,text_a,text_b,label_id):\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label_id = label_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### functions for converting values to train features so that it can be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/var/extra/users/jgeorge/tf2.0/input/dish/data/jan17_2020/'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_exp_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bytes_feature(value):\n",
    "  \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "  if isinstance(value, type(tf.constant(0))):\n",
    "    print(\"type constant \",type(tf.constant(0)))\n",
    "    value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
    "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _int64_feature(values):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\n",
    "      #Note that the Feature requires a list as input\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path=os.path.join(main_exp_folder,'train.tfrecords')\n",
    "eval_data_path=os.path.join(main_exp_folder,'eval.tfrecords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_single_example(ex_index, example, label_list, max_seq_length,sp_proto):\n",
    "    ## please refer to functionality in file models/official/nlp/data/classifier_data_lib.py \n",
    "    ##this function is based on an older version & slightly different as in it's taking care of only \n",
    "    # the classification part\n",
    "    \n",
    "    # The convention in BERT is:\n",
    "    # (a) For sequence pairs:\n",
    "    #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "    #  type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1\n",
    "    # (b) For single sequences:\n",
    "    #  tokens:   [CLS] the dog is hairy . [SEP]\n",
    "    #  type_ids: 0     0   0   0  0     0 0\n",
    "    #\n",
    "    # Where \"type_ids\" are used to indicate whether this is the first\n",
    "    # sequence or the second sequence. The embedding vectors for `type=0` and\n",
    "    # `type=1` were learned during pre-training and are added to the wordpiece\n",
    "    # embedding vector (and position vector). This is not *strictly* necessary\n",
    "    # since the [SEP] token unambiguously separates the sequences, but it makes\n",
    "    # it easier for the model to learn the concept of sequences.\n",
    "    #\n",
    "    # For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "    # used as the \"sentence vector\". Note that this only makes sense because\n",
    "    # the entire model is fine-tuned.\n",
    "    \n",
    "#     CLS_ID = 2\n",
    "#     SEP_ID = 3\n",
    "    [CLS_ID,SEP_ID]  = sp_external_tokenizer.piece_to_id(['[CLS]','[SEP]'])\n",
    "    # -2 for [CLS], [SEP] \n",
    "    # we need to add these tokens in addition to the sentence words is while keeping the max sequence length limit\n",
    "    # check Bert paper for this [CLS] is added at the beginning & [SEP] after 1st sentence (in our classificatin example \n",
    "    # we don't differentiate the sentences here, so [SEP] is at the end. For challenges like SQUAD 2 sentences is \n",
    "    # where i have seen people use 2 sentences , 1st question & 2nd answer )\n",
    "    \n",
    "    max_word_ids = max_seq_length -2\n",
    "    input_ids = []\n",
    "    segment_ids = []\n",
    "    input_mask = []\n",
    "    input_ids.append(CLS_ID)\n",
    "    \n",
    "    # word_ids = tfs.encode([example.text_a],model_proto=sp_proto).values[0]\n",
    "    word_ids = sp_tokenizer.tokenize(example.text_a).numpy()\n",
    "    \n",
    "    # trimming to the max size\n",
    "    word_ids = word_ids[:max_word_ids]\n",
    "    input_ids.extend(word_ids)\n",
    "    input_ids.append(SEP_ID)\n",
    "    segment_ids = [0]*len(input_ids)\n",
    "    input_mask = [1]*len(input_ids)\n",
    "    if len(input_ids)<max_seq_length:\n",
    "        diff = max_seq_length - len(input_ids)\n",
    "        input_ids.extend([0]*diff)\n",
    "        segment_ids.extend([0]*diff)\n",
    "        input_mask.extend([0]*diff)\n",
    "    assert(len(input_ids)==max_seq_length)\n",
    "    assert(len(segment_ids)==max_seq_length)\n",
    "    assert(len(input_mask)==max_seq_length)\n",
    "    label_map = {label:i for i,label in enumerate(label_list)}\n",
    "    label_id = label_map[example.label]\n",
    "    if ex_index < 5:\n",
    "        # id_to_piece requires a python int type (not numpy int)\n",
    "        input_ids_new = []\n",
    "        for input_id in input_ids:\n",
    "            if type(input_id) == np.int32:\n",
    "                input_ids_new.append(input_id.item())\n",
    "            else:\n",
    "                input_ids_new.append(input_id)\n",
    "        input_ids = input_ids_new\n",
    "                \n",
    "        logging.info(\"*** Example ***\")\n",
    "        logging.info(\"uuid: %s\", (example.uuid))\n",
    "        logging.info(\"input_ids: %s\", \" \".join(map(str,input_ids)))\n",
    "        logging.info(\"tokens: %s\",\n",
    "                     \" \".join(sp_external_tokenizer.id_to_piece(input_ids)))\n",
    "        logging.info(\"input_mask: %s\", \" \".join(map(str,input_mask)))\n",
    "        logging.info(\"segment_ids: %s\", \" \".join(map(str,segment_ids)))\n",
    "        logging.info(\"label: %s (id = %d)\", example.label, label_id)\n",
    "\n",
    "    feature = InputFeatures(input_ids,input_mask,segment_ids,label_id)\n",
    "    return feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#refer file_based_convert_examples_to_features function in models/official/nlp/data/classifier_data_lib.py\n",
    "def write_training_data(input_csv_file,output_file,sp_proto=None,delimiter='\\t',max_seq_length=128):\n",
    "    # actually writing tfrecords is not necessary, but since \n",
    "    with open(input_csv_file,'r',encoding='utf-8') as csv_file, tf.io.TFRecordWriter(output_file) as tf_record_writer:\n",
    "        #tried using binary format for bytes_list, but csv_reader requires text format\n",
    "#     with open(input_csv_file,'rb') as csv_file, tf.io.TFRecordWriter(output_file) as tf_record_writer:\n",
    "        csv_reader = csv.reader(csv_file,delimiter=delimiter,quotechar='\"')\n",
    "        for i,cols in enumerate(csv_reader):\n",
    "            ##skipping filename & granular tag (granular tag & final tag are the same here)\n",
    "#             yield cols[1:-1]\n",
    "            features = collections.OrderedDict()\n",
    "            \n",
    "            uuid = cols[0]\n",
    "            text = cols[1]\n",
    "            text_b = \"\"\n",
    "            if(len(cols)<4):\n",
    "                print('uuid ',uuid)\n",
    "            intent = cols[2]\n",
    "            input_example = InputExample(uuid,text,label=intent)\n",
    "            #this will encode the text,label into ids \n",
    "            feature = convert_single_example(ex_index=i,example=input_example,label_list=intent_list,\n",
    "                                             max_seq_length=max_seq_length,sp_proto=sp_proto)\n",
    "            \n",
    "            features[\"text_a\"] = _bytes_feature(text)\n",
    "            features[\"text_b\"] = _bytes_feature(text_b)\n",
    "            features[\"label_id\"] = _int64_feature(feature.segment_ids)\n",
    "            #Note making it as a list & passing it\n",
    "#             features[\"label_id\"] = _int64_feature([feature.label_id])\n",
    "#             features[\"is_real_example\"]\n",
    "            tf_example = tf.train.Example(features=tf.train.Features(feature=features))\n",
    "            tf_record_writer.write(tf_example.SerializeToString())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_training_data(train_csv_file,train_data_path,sp_proto=sp_model_proto,max_seq_length=max_seq_length)\n",
    "write_training_data(eval_csv_file,eval_data_path,sp_proto=sp_model_proto,max_seq_length=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/var/extra/users/jgeorge/tf2.0/input/dish/data/jan17_2020/train.txt'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_csv_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'INT-sv1appis14-1504137880316-305603_1154\\tPlease freeze account\\tdish_pause-query\\tdish_pause-query', shape=(), dtype=string)\n",
      "tf.Tensor(b'f9074f6c-1831-46ee-75af-227fbab1a6a582\\thow can I watch college football?\\tother-other\\tother-other', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for line in train_dataset.take(2):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_label = input_meta_data['default_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelToId(tf.keras.layers.Layer):\n",
    "#     def __init__(self,model_proto):\n",
    "    def __init__(self,filepath,default_index,*args,**kwargs):\n",
    "        \"\"\" \n",
    "        filepath - path to the text file with 1 label per line\n",
    "        \"\"\"\n",
    "        super(LabelToId, self).__init__(trainable=False,dtype=tf.string,*args, **kwargs)\n",
    "        self.filepath = filepath\n",
    "        self.default_index=default_index\n",
    "        \n",
    "    def build(self,input_shape):\n",
    "        # need to convert label name to label index. So keeping line no as value & whole text line as key\n",
    "        # issue with using table lookup (both file based & key value tensorbased) in tf2.2 (tf2.x) - https://github.com/tensorflow/tensorflow/issues/38305\n",
    "        self.table = tf.lookup.StaticHashTable(\n",
    "            tf.lookup.TextFileInitializer(self.filepath,\n",
    "                                          key_dtype=tf.string,key_index=tf.lookup.TextFileIndex.WHOLE_LINE,\n",
    "                                     value_dtype=tf.int64,value_index=tf.lookup.TextFileIndex.LINE_NUMBER),self.default_index)\n",
    "        self.built=True\n",
    "        \n",
    "    def call(self, input_index):\n",
    "        word_ids = self.table.lookup(input_index)\n",
    "        return word_ids\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(LabelLookup, self).get_config()\n",
    "#         config.update({'filepath': self.filepath})\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {l:i for i,l  in enumerate(intent_list)}\n",
    "default_index = d.get(default_label)\n",
    "label_to_id = LabelToId(intent_file,default_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "delimiter = '\\t'\n",
    "temp_dataset = tf.data.experimental.CsvDataset(train_csv_file,record_defaults=[tf.constant(''),tf.constant('MISSING')],\n",
    "                                               header=False,field_delim=delimiter,use_quote_delim=True,\n",
    "                                               select_cols=[1,2])\n",
    "# temp_dataset = temp_dataset.map(lambda line: )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'Please freeze account'>, <tf.Tensor: shape=(), dtype=string, numpy=b'dish_pause-query'>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'how can I watch college football?'>, <tf.Tensor: shape=(), dtype=string, numpy=b'other-other'>)\n"
     ]
    }
   ],
   "source": [
    "for line in temp_dataset.take(2):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7f4e1587e280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7f4e1587e280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7f4e1587e820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7f4e1587e820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "preprocessor_hub_url = bert_preprocessor_url\n",
    "bert_preprocessor_loaded = hub.load(preprocessor_hub_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " \n",
    "# input_text = tf.keras.layers.Input(shape=(), dtype=tf.string,name='input_text')\n",
    "# tokenize = hub.KerasLayer(bert_preprocessor_loaded.tokenize)\n",
    "# tokenized_inputs = [tokenize(input_text)]\n",
    "# bert_pack_inputs = hub.KerasLayer(bert_preprocessor_loaded.bert_pack_inputs,arguments=dict(seq_length=max_seq_length))\n",
    "# encoder_inputs = bert_pack_inputs(tokenized_inputs)\n",
    "max_seq_length=128\n",
    "@tf.function(input_signature=[tf.TensorSpec(shape=(None),dtype=tf.string)])\n",
    "def encode_text(text):\n",
    "# def encode_text(text, preprocessor_hub_url,max_seq_length=128 ):\n",
    "    print(f'text {text}')\n",
    "#     tokenized_inputs = [tokenize(input_text)]\n",
    "#     bert_preprocessor_loaded = hub.load(preprocessor_hub_url)\n",
    "#     input_text = tf.keras.layers.Input(shape=(), dtype=tf.string,name='input_text')\n",
    "    tokenize = hub.KerasLayer(bert_preprocessor_loaded.tokenize)\n",
    "    tokenized_inputs = [tokenize(text)] # bert can handle 2 input_texts (segments)\n",
    "    \n",
    "    bert_pack_inputs = hub.KerasLayer(bert_preprocessor_loaded.bert_pack_inputs,arguments=dict(seq_length=max_seq_length))\n",
    "    return encoder_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_label(label):\n",
    "    print(f'label {label}')\n",
    "    return label_to_id(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute '_datatype_enum'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-255-6befe63809f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# t = temp_dataset.map(lambda columns: transform_data(columns, bert_preprocessor_url, intent_list))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mencode_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.3/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls, deterministic)\u001b[0m\n\u001b[1;32m   1693\u001b[0m     \"\"\"\n\u001b[1;32m   1694\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1695\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mMapDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreserve_cardinality\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1696\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m       return ParallelMapDataset(\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.3/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\u001b[0m\n\u001b[1;32m   4044\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4045\u001b[0m         use_legacy_function=use_legacy_function)\n\u001b[0;32m-> 4046\u001b[0;31m     variant_tensor = gen_dataset_ops.map_dataset(\n\u001b[0m\u001b[1;32m   4047\u001b[0m         \u001b[0minput_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4048\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.3/lib/python3.8/site-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mmap_dataset\u001b[0;34m(input_dataset, other_arguments, f, output_types, output_shapes, use_inter_op_parallelism, preserve_cardinality, name)\u001b[0m\n\u001b[1;32m   3066\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3067\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3068\u001b[0;31m       return map_dataset_eager_fallback(\n\u001b[0m\u001b[1;32m   3069\u001b[0m           \u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother_arguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3070\u001b[0m           \u001b[0moutput_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.3/lib/python3.8/site-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mmap_dataset_eager_fallback\u001b[0;34m(input_dataset, other_arguments, f, output_types, output_shapes, use_inter_op_parallelism, preserve_cardinality, name, ctx)\u001b[0m\n\u001b[1;32m   3131\u001b[0m     \u001b[0mpreserve_cardinality\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3132\u001b[0m   \u001b[0mpreserve_cardinality\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_bool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreserve_cardinality\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"preserve_cardinality\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3133\u001b[0;31m   \u001b[0m_attr_Targuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother_arguments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_mixed_eager_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother_arguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3134\u001b[0m   \u001b[0minput_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_dtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariant\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3135\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother_arguments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.3/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mconvert_to_mixed_eager_tensors\u001b[0;34m(values, ctx)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mconvert_to_mixed_eager_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m   \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m   \u001b[0mtypes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_datatype_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.3/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mconvert_to_mixed_eager_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m   \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m   \u001b[0mtypes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_datatype_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute '_datatype_enum'"
     ]
    }
   ],
   "source": [
    "# t = temp_dataset.map(lambda columns: transform_data(columns, bert_preprocessor_url, intent_list))\n",
    "t = temp_dataset.batch(2).map(lambda text, label: (encode_text(text), encode_label(label)))\n",
    "for line in t.take(2):\n",
    "    print(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset shapes: (), types: tf.string>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_dataset.map(lambda : v1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing sentence piece model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##loading the sentence piece model into memory\n",
    "sp_model_proto = tf.io.gfile.GFile(sentencepiece_path, 'rb').read()\n",
    "\n",
    "# sp_model = sp_model_proto\n",
    "# this is a tensorflow version of sentence piece tokenizer & can be used in graphs\n",
    "sp_tokenizer = tftext.SentencepieceTokenizer(model=sp_model_proto)\n",
    "# this is the common python package for sentence piece tokenization, \n",
    "# it currently have more functions than the tensorflow text version of sentence piece, hence we are using it for some of the steps\n",
    "# Note - this can't be used in a graph\n",
    "sp_external_tokenizer = spm.SentencePieceProcessor()\n",
    "sp_external_tokenizer.load(model_proto=sp_model_proto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying out using various functions in sentence piece tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For sentence piece we can use either the       \n",
    "1) tf_sentencepiece (import as tfs here)     \n",
    "2) sentencepiece (imported as spm) package        \n",
    "3) sentencepiece from tensorflow_text    \n",
    "4) tokenization.FullSentencePieceTokenizer - a wrapper on top of 2) sentencepiece, code is available in models/official/nlp/bert/tokenization.py      \n",
    "\n",
    "We will eventually just use tf_sentencepiece as that's the one which is currently, available to export as a graph.      \n",
    "But for the 1st step of creating training data (in tf records format) either of them will do\n",
    "but i'll illustrate how to use others as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded_text3  tf.Tensor(\n",
      "[  590    55  7876    20  6557  3284  5477 11969   357     8  3099     8\n",
      "  1323  1433   159 10114], shape=(16,), dtype=int32) , decoded text3  tf.Tensor(b'give me directions to nearest restaurant randomtext 15-08-2019 year 2019', shape=(), dtype=string) \n",
      "\n",
      "encoded_text2  [590, 55, 7876, 20, 6557, 3284, 5477, 11969, 357, 8, 3099, 8, 1323, 1433, 159, 10114] , decoded text2  give me directions to nearest restaurant randomtext 15-08-2019 year 2019 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "actual_sentence = 'give me directions to nearest restaurant randomtext 15-08-2019 year 2019'\n",
    "\n",
    "###3rd methond\n",
    "sp_tokenizer = tftext.SentencepieceTokenizer(model=sp_model_proto)\n",
    "encoded_text3 = sp_tokenizer.tokenize(actual_sentence)\n",
    "print('encoded_text3 ',encoded_text3, ', decoded text3 ',sp_tokenizer.detokenize(encoded_text3.numpy()),'\\n')\n",
    "\n",
    "\n",
    "sp_external_tokenizer = spm.SentencePieceProcessor()\n",
    "sp_external_tokenizer.load(model_proto=sp_model_proto)\n",
    "encoded_text2 = sp_external_tokenizer.encode_as_ids(actual_sentence)\n",
    "print('encoded_text2 ',encoded_text2, ', decoded text2 ',\n",
    "      sp_external_tokenizer.decode_ids(encoded_text2),'\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this returns a ragged tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[4148, 48, 25], [1289, 485]]>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_tokenizer.tokenize(['hi this is','test run'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=string, numpy=array([b'<pad>', b'<unk>', b'[CLS]'], dtype=object)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = sp_tokenizer.id_to_string([0,1,2])\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pieces1  ['▁give', '▁me', '▁directions', '▁to', '▁nearest', '▁restaurant', '▁random', 'text', '▁15', '-', '08', '-', '20', '19', '▁year', '▁2019'] \n",
      "ids1  [590, 55, 7876, 20, 6557, 3284, 5477, 11969, 357, 8, 3099, 8, 1323, 1433, 159, 10114] \n",
      "\n",
      "pieces2  ['▁give', '▁me', '▁directions', '▁to', '▁nearest', 'rest', 'au', 'rant'] \n",
      "ids2  [590, 55, 7876, 20, 6557, 11466, 1346, 7874] \n",
      "pieces2_2  ['▁give', '▁me', '▁directions', '▁to', '▁nearest', 'rest', 'au', 'rant'] \n",
      "decoded_piece  give me directions to nearestrestaurant\n"
     ]
    }
   ],
   "source": [
    "pieces1 = sp_external_tokenizer.encode_as_pieces(actual_sentence)\n",
    "ids1 = sp_external_tokenizer.piece_to_id(pieces1)\n",
    "print('pieces1 ', pieces1 , '\\nids1 ',ids1,'\\n')\n",
    "\n",
    "pieces2 = sp_external_tokenizer.encode_as_pieces('give me directions to nearestrestaurant')\n",
    "ids2 = sp_external_tokenizer.piece_to_id(pieces2)\n",
    "pieces2_2 = sp_external_tokenizer.id_to_piece(ids2)\n",
    "decoded_piece = sp_external_tokenizer.decode_pieces(pieces2)\n",
    "\n",
    "print('pieces2 ', pieces2, '\\nids2 ',ids2,'\\npieces2_2 ',pieces2_2, '\\ndecoded_piece ',decoded_piece )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 <pad>\n",
      "1 <unk>\n",
      "2 [CLS]\n",
      "3 [SEP]\n",
      "4 [MASK]\n",
      "5 (\n",
      "6 )\n",
      "7 \"\n",
      "8 -\n",
      "9 .\n",
      "10 –\n",
      "11 £\n",
      "12 €\n",
      "13 ▁\n",
      "14 ▁the\n",
      "15 ,\n",
      "16 ▁of\n",
      "17 ▁and\n",
      "18 s\n",
      "19 ▁in\n",
      "20 ▁to\n",
      "21 ▁a\n",
      "22 '\n",
      "23 ▁was\n",
      "24 ▁he\n",
      "25 ▁is\n",
      "26 ▁for\n",
      "27 ▁on\n",
      "28 ▁as\n",
      "29 ▁with\n",
      "30 ▁that\n",
      "31 ▁i\n",
      "32 ▁it\n",
      "33 ▁his\n",
      "34 ▁by\n",
      "35 ▁at\n",
      "36 ▁her\n",
      "37 ▁from\n",
      "38 t\n",
      "39 ▁she\n",
      "40 ▁an\n",
      "41 ▁had\n",
      "42 ▁you\n",
      "43 d\n",
      "44 ▁be\n",
      "45 :\n",
      "46 ▁were\n",
      "47 ▁but\n",
      "48 ▁this\n",
      "49 i\n",
      "50 ▁are\n",
      "51 ▁my\n",
      "52 ▁not\n",
      "53 ▁one\n",
      "54 ▁or\n",
      "55 ▁me\n",
      "56 ▁which\n",
      "57 ▁have\n",
      "58 a\n",
      "59 ▁they\n",
      "60 ?\n",
      "61 ▁him\n",
      "62 e\n",
      "63 ▁has\n",
      "64 ▁first\n",
      "65 ▁all\n",
      "66 ▁their\n",
      "67 ▁also\n",
      "68 ing\n",
      "69 ed\n",
      "70 ▁out\n",
      "71 ▁up\n",
      "72 ▁who\n",
      "73 ;\n",
      "74 ▁been\n",
      "75 ▁after\n",
      "76 ▁when\n",
      "77 ▁into\n",
      "78 ▁new\n",
      "79 m\n",
      "80 ▁there\n",
      "81 ▁two\n",
      "82 ▁its\n",
      "83 ▁would\n",
      "84 ▁over\n",
      "85 ▁time\n",
      "86 ▁so\n",
      "87 ▁said\n",
      "88 ▁about\n",
      "89 ▁other\n",
      "90 ▁no\n",
      "91 ▁more\n",
      "92 ▁can\n",
      "93 y\n",
      "94 ▁then\n",
      "95 ▁we\n",
      "96 th\n",
      "97 ▁back\n",
      "98 ▁what\n",
      "99 re\n"
     ]
    }
   ],
   "source": [
    "#printing out 1st 100 tokens, just to get a feel of what all entries are present\n",
    "for index,token in enumerate (sp_tokenizer.id_to_string([*range(100)])):\n",
    "    print(index,token.numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the data into TFRecords\n",
    "For following functionalities refer the file  - models/official/nlp/data/classifier_data_lib.py       \n",
    "This includes even the sentence piece tokenization part\n",
    "#### Defining an object for the 1 row of text input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#in our case we are doing just classification so we only need 1 text value = text_a\n",
    "class InputExample(object):\n",
    "    def __init__(self,uuid,text_a,text_b=None,label=None):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "        Args:\n",
    "          uuid: Unique id for the example.\n",
    "          text_a: string. The untokenized text of the first sequence. For single\n",
    "            sequence tasks, only this sequence must be specified.\n",
    "          text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "            Only must be specified for sequence pair tasks.\n",
    "          label: (Optional) string. The label of the example. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "        \"\"\"\n",
    "        self.uuid = uuid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### defining an object for the 1 row of input feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "### in the official one there was something called is_real_example as feature, here i skipped it since it's \n",
    "#only classification\n",
    "class InputFeatures(object):\n",
    "    def __init__(self,input_ids,input_mask,\n",
    "                segment_ids,label_id):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### functions for converting values to train features so that it can be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bytes_feature(value):\n",
    "  \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "  if isinstance(value, type(tf.constant(0))):\n",
    "    print(\"type constant \",type(tf.constant(0)))\n",
    "    value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
    "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _int64_feature(values):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\n",
    "      #Note that the Feature requires a list as input\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path=os.path.join(main_exp_folder,'train.tfrecords')\n",
    "eval_data_path=os.path.join(main_exp_folder,'eval.tfrecords')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function which converts a single input to Features, one of the feature being the sentencepiece encoded array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([26660,  5123,   259], dtype=int32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_ids = sp_tokenizer.tokenize(\"dummy sentence want to see if this works\").numpy()\n",
    "word_ids = word_ids[:3]\n",
    "word_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[26660, 5123, 259]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_ids.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>', '[CLS]', '[SEP]']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_external_tokenizer.id_to_piece([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁dummy', '▁sentence', '▁want']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_external_tokenizer.id_to_piece(word_ids.tolist())\n",
    "# sp_external_tokenizer.id_to_piece(map(lambda x: x.item(),input_ids))\n",
    "# sp_external_tokenizer.id_to_piece([x.item() for x in input_ids])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "[CLS_ID,SEP_ID]  = sp_external_tokenizer.piece_to_id(['[CLS]','[SEP]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_single_example(ex_index, example, label_list, max_seq_length,sp_proto):\n",
    "    ## please refer to functionality in file models/official/nlp/data/classifier_data_lib.py \n",
    "    ##this function is based on an older version & slightly different as in it's taking care of only \n",
    "    # the classification part\n",
    "    \n",
    "    # The convention in BERT is:\n",
    "    # (a) For sequence pairs:\n",
    "    #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "    #  type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1\n",
    "    # (b) For single sequences:\n",
    "    #  tokens:   [CLS] the dog is hairy . [SEP]\n",
    "    #  type_ids: 0     0   0   0  0     0 0\n",
    "    #\n",
    "    # Where \"type_ids\" are used to indicate whether this is the first\n",
    "    # sequence or the second sequence. The embedding vectors for `type=0` and\n",
    "    # `type=1` were learned during pre-training and are added to the wordpiece\n",
    "    # embedding vector (and position vector). This is not *strictly* necessary\n",
    "    # since the [SEP] token unambiguously separates the sequences, but it makes\n",
    "    # it easier for the model to learn the concept of sequences.\n",
    "    #\n",
    "    # For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "    # used as the \"sentence vector\". Note that this only makes sense because\n",
    "    # the entire model is fine-tuned.\n",
    "    \n",
    "#     CLS_ID = 2\n",
    "#     SEP_ID = 3\n",
    "    [CLS_ID,SEP_ID]  = sp_external_tokenizer.piece_to_id(['[CLS]','[SEP]'])\n",
    "    # -2 for [CLS], [SEP] \n",
    "    # we need to add these tokens in addition to the sentence words is while keeping the max sequence length limit\n",
    "    # check Bert paper for this [CLS] is added at the beginning & [SEP] after 1st sentence (in our classificatin example \n",
    "    # we don't differentiate the sentences here, so [SEP] is at the end. For challenges like SQUAD 2 sentences is \n",
    "    # where i have seen people use 2 sentences , 1st question & 2nd answer )\n",
    "    \n",
    "    max_word_ids = max_seq_length -2\n",
    "    input_ids = []\n",
    "    segment_ids = []\n",
    "    input_mask = []\n",
    "    input_ids.append(CLS_ID)\n",
    "    \n",
    "    # word_ids = tfs.encode([example.text_a],model_proto=sp_proto).values[0]\n",
    "    word_ids = sp_tokenizer.tokenize(example.text_a).numpy()\n",
    "    \n",
    "    # trimming to the max size\n",
    "    word_ids = word_ids[:max_word_ids]\n",
    "    input_ids.extend(word_ids)\n",
    "    input_ids.append(SEP_ID)\n",
    "    segment_ids = [0]*len(input_ids)\n",
    "    input_mask = [1]*len(input_ids)\n",
    "    if len(input_ids)<max_seq_length:\n",
    "        diff = max_seq_length - len(input_ids)\n",
    "        input_ids.extend([0]*diff)\n",
    "        segment_ids.extend([0]*diff)\n",
    "        input_mask.extend([0]*diff)\n",
    "    assert(len(input_ids)==max_seq_length)\n",
    "    assert(len(segment_ids)==max_seq_length)\n",
    "    assert(len(input_mask)==max_seq_length)\n",
    "    label_map = {label:i for i,label in enumerate(label_list)}\n",
    "    label_id = label_map[example.label]\n",
    "    if ex_index < 5:\n",
    "        # id_to_piece requires a python int type (not numpy int)\n",
    "        input_ids_new = []\n",
    "        for input_id in input_ids:\n",
    "            if type(input_id) == np.int32:\n",
    "                input_ids_new.append(input_id.item())\n",
    "            else:\n",
    "                input_ids_new.append(input_id)\n",
    "        input_ids = input_ids_new\n",
    "                \n",
    "        logging.info(\"*** Example ***\")\n",
    "        logging.info(\"uuid: %s\", (example.uuid))\n",
    "        logging.info(\"input_ids: %s\", \" \".join(map(str,input_ids)))\n",
    "        logging.info(\"tokens: %s\",\n",
    "                     \" \".join(sp_external_tokenizer.id_to_piece(input_ids)))\n",
    "        logging.info(\"input_mask: %s\", \" \".join(map(str,input_mask)))\n",
    "        logging.info(\"segment_ids: %s\", \" \".join(map(str,segment_ids)))\n",
    "        logging.info(\"label: %s (id = %d)\", example.label, label_id)\n",
    "\n",
    "    feature = InputFeatures(input_ids,input_mask,segment_ids,label_id)\n",
    "    return feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_config = {}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# bert_config_dict = {\n",
    "#     \"hidden_dropout_prob\":0,\n",
    "#     'initializer_range': 0.02\n",
    "# }\n",
    "# bert_config = albert_configs.AlbertConfig.from_dict(bert_config_dict)\n",
    "\n",
    "# in some cases it might be inside assets folder\n",
    "bert_config_file = os.path.join(bert_model_dir,'albert_config.json')\n",
    "\n",
    "# bert_config = modeling.AlbertConfig.from_json_file(FLAGS.bert_config_file)\n",
    "bert_config = albert_configs.AlbertConfig.from_json_file(bert_config_file)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "bert_config.to_dict()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "bert_config = {'vocab_size': 30000,\n",
    " 'hidden_size': 1024,\n",
    " 'num_hidden_layers': 24,\n",
    " 'num_attention_heads': 16,\n",
    " 'hidden_act': 'gelu',\n",
    " 'intermediate_size': 4096,\n",
    " 'hidden_dropout_prob': 0,\n",
    " 'attention_probs_dropout_prob': 0,\n",
    " 'max_position_embeddings': 512,\n",
    " 'type_vocab_size': 2,\n",
    " 'initializer_range': 0.02,\n",
    " 'backward_compatible': True,\n",
    " 'embedding_size': 128,\n",
    " 'num_hidden_groups': 1,\n",
    " 'net_structure_type': 0,\n",
    " 'gap_size': 0,\n",
    " 'num_memory_blocks': 0,\n",
    " 'inner_group_num': 1,\n",
    " 'down_scale_factor': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_config[\"initializer_range\"] = 0.02\n",
    "# we will use this for the Dropout probability between bert layer & final Dense layer\n",
    "bert_config[\"hidden_dropout_prob\"]=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_seq_length = bert_config.max_position_embeddings\n",
    "# could even keep max_seq_length as 512, but limiting to 128 for this expt\n",
    "max_seq_length = 128\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is the code which reads data line by line converts & writes the data in tfrecord format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#refer file_based_convert_examples_to_features function in models/official/nlp/data/classifier_data_lib.py\n",
    "def write_training_data(input_csv_file,output_file,sentencepiece_path=None,sp_proto=None,delimiter='\\t',max_seq_length=128):\n",
    "    ##loading the sentence piece model into memory\n",
    "    if sp_proto ==None:\n",
    "        sp_proto = tf.io.gfile.GFile(sentencepiece_path, 'rb').read()\n",
    "   \n",
    "    with open(input_csv_file,'r',encoding='utf-8') as csv_file, tf.io.TFRecordWriter(output_file) as tf_record_writer:\n",
    "        #tried using binary format for bytes_list, but csv_reader requires text format\n",
    "#     with open(input_csv_file,'rb') as csv_file, tf.io.TFRecordWriter(output_file) as tf_record_writer:\n",
    "        csv_reader = csv.reader(csv_file,delimiter=delimiter,quotechar='\"')\n",
    "        for i,cols in enumerate(csv_reader):\n",
    "            ##skipping filename & granular tag (granular tag & final tag are the same here)\n",
    "#             yield cols[1:-1]\n",
    "            features = collections.OrderedDict()\n",
    "            \n",
    "            uuid = cols[0]\n",
    "            text = cols[1]\n",
    "            if(len(cols)<4):\n",
    "                print('uuid ',uuid)\n",
    "            intent = cols[2]\n",
    "            input_example = InputExample(uuid,text,label=intent)\n",
    "            #this will encode the text,label into ids \n",
    "            feature = convert_single_example(ex_index=i,example=input_example,label_list=intent_list,\n",
    "                                             max_seq_length=max_seq_length,sp_proto=sp_proto)\n",
    "            features[\"input_ids\"] = _int64_feature(feature.input_ids)\n",
    "            features[\"input_mask\"] = _int64_feature(feature.input_mask)\n",
    "            features[\"segment_ids\"] = _int64_feature(feature.segment_ids)\n",
    "            #Note making it as a list & passing it\n",
    "            features[\"label_id\"] = _int64_feature([feature.label_id])\n",
    "#             features[\"is_real_example\"]\n",
    "            tf_example = tf.train.Example(features=tf.train.Features(feature=features))\n",
    "            tf_record_writer.write(tf_example.SerializeToString())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Example ***\n",
      "uuid: INT-sv1appis14-1504137880316-305603_1154\n",
      "input_ids: 2 13 1 413 6105 11551 2176 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "tokens: [CLS] ▁ <unk> le ase ▁freeze ▁account [SEP] <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "input_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "label: dish_pause-query (id = 43)\n",
      "*** Example ***\n",
      "uuid: f9074f6c-1831-46ee-75af-227fbab1a6a582\n",
      "input_ids: 2 184 92 13 1 1455 314 435 60 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "tokens: [CLS] ▁how ▁can ▁ <unk> ▁watch ▁college ▁football ? [SEP] <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "label: other-other (id = 88)\n",
      "*** Example ***\n",
      "uuid: INT-sv1appis14-1504137880316-305603_1103\n",
      "input_ids: 2 13 1 83 101 20 57 14 13 1 13 1 11699 11557 13 1 947 2304 13 1 3448 111 1906 1974 37 51 2176 9 13 1 221 22 38 1518 462 27156 48 20 44 905 20 51 2176 9 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "tokens: [CLS] ▁ <unk> ▁would ▁like ▁to ▁have ▁the ▁ <unk> ▁ <unk> rot ect ▁ <unk> il ver ▁ <unk> ide o ▁charge ▁removed ▁from ▁my ▁account . ▁ <unk> ▁don ' t ▁remember ▁ever ▁approving ▁this ▁to ▁be ▁added ▁to ▁my ▁account . [SEP] <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "label: channel_package-cancel (id = 30)\n",
      "*** Example ***\n",
      "uuid: INT-sv1appis13-1504099702813-293669\n",
      "input_ids: 2 13 1 111 8763 52 638 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "tokens: [CLS] ▁ <unk> o pper ▁not ▁working [SEP] <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "input_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "label: hopper-issues (id = 63)\n",
      "*** Example ***\n",
      "uuid: INT-sv1appis14-1504137880316-305603_2067\n",
      "input_ids: 2 13 1 410 31 417 20 1455 2331 801 20 14 13 16700 162 9 9 9 9 630 14 3646 13 5 9725 7709 15 16394 6 376 20 44 2587 20 4807 1707 60 13 1 111 160 14 13 16700 839 590 168 82 258 4807 1707 2800 19 389 26 14 16394 20 6379 20 32 60 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "tokens: [CLS] ▁ <unk> f ▁i ▁wanted ▁to ▁watch ▁content ▁recorded ▁to ▁the ▁ hopper ▁go . . . . ▁does ▁the ▁device ▁ ( cell phone , ▁tablet ) ▁need ▁to ▁be ▁connected ▁to ▁wi fi ? ▁ <unk> o es ▁the ▁ hopper go ▁give ▁off ▁its ▁own ▁wi fi ▁signal ▁in ▁order ▁for ▁the ▁tablet ▁to ▁connect ▁to ▁it ? [SEP] <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "label: hopper-issues (id = 63)\n",
      "*** Example ***\n",
      "uuid: a9cf6bee-3971-4516-459f-2243d9a606fd\n",
      "input_ids: 2 13 1 259 20 10361 42 30 13 1 221 22 38 57 5592 365 9 13 1 5608 42 2660 55 21 1071 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "tokens: [CLS] ▁ <unk> ▁want ▁to ▁inform ▁you ▁that ▁ <unk> ▁don ' t ▁have ▁electricity ▁service . ▁ <unk> hy ▁you ▁send ▁me ▁a ▁bill [SEP] <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "label: billing-issue (id = 22)\n",
      "*** Example ***\n",
      "uuid: 0f050cf9-88d7-44bd-23c8-5f26674f5b6a\n",
      "input_ids: 2 13 1 6775 2384 20 51 1071 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "tokens: [CLS] ▁ <unk> hat ▁happen ▁to ▁my ▁bill [SEP] <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "input_mask: 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "label: billing-query (id = 24)\n",
      "*** Example ***\n",
      "uuid: INT-sv1appis11-1503849377195-243917\n",
      "input_ids: 2 13 1 376 20 3934 51 5388 569 9 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "tokens: [CLS] ▁ <unk> ▁need ▁to ▁replace ▁my ▁remote ▁control . [SEP] <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "input_mask: 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "label: remote-place_order (id = 127)\n",
      "*** Example ***\n",
      "uuid: INT-sv1appis14-1504137880316-305603_1158\n",
      "input_ids: 2 13 1 62 1892 5221 25 1148 52 9401 27 352 1318 60 60 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "tokens: [CLS] ▁ <unk> e ci ever ▁is ▁saying ▁not ▁authorized ▁on ▁every ▁channel ? ? [SEP] <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "label: receiver_issue-get_help (id = 114)\n",
      "*** Example ***\n",
      "uuid: 6e1842f4-63a1-4259-3999-6928033ce7eb\n",
      "input_ids: 2 31 376 20 11100 51 3251 2056 15404 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "tokens: [CLS] ▁i ▁need ▁to ▁update ▁my ▁credit ▁card ▁info [SEP] <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "label: payment-get_help (id = 89)\n"
     ]
    }
   ],
   "source": [
    "write_training_data(train_csv_file,train_data_path,sp_proto=sp_model_proto,max_seq_length=max_seq_length)\n",
    "write_training_data(eval_csv_file,eval_data_path,sp_proto=sp_model_proto,max_seq_length=max_seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the data from tfrecord to provide to training graph\n",
    "Before we write the actual code to read it let's just explore some of the functions in tf.data api & on how to decode the tfrecord data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_ds = tf.data.TFRecordDataset(train_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _decode_data(record):\n",
    "    features = {\n",
    "        \"input_ids\":tf.io.FixedLenFeature([max_seq_length],tf.int64),\n",
    "        \"input_mask\":tf.io.FixedLenFeature([max_seq_length],tf.int64),\n",
    "        \"segment_ids\":tf.io.FixedLenFeature([max_seq_length],tf.int64),\n",
    "        \"label_id\":tf.io.FixedLenFeature([],tf.int64)\n",
    "    }\n",
    "#     features = {\n",
    "#         'uuid':tf.io.FixedLenFeature([],tf.string),\n",
    "#         'text':tf.io.VarLenFeature(tf.int64),\n",
    "#         'intent':tf.io.FixedLenFeature([],tf.string)\n",
    "#     }    \n",
    "    return tf.io.parse_single_example(record,features=features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "## map function processes line by line, similar to spark,scala map or pandas apply fucntion\n",
    "processed_data = raw_ds.map(_decode_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': <tf.Tensor: shape=(128,), dtype=int64, numpy=\n",
      "array([    2,    13,     1,   413,  6105, 11551,  2176,     3,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0])>, 'input_mask': <tf.Tensor: shape=(128,), dtype=int64, numpy=\n",
      "array([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])>, 'label_id': <tf.Tensor: shape=(), dtype=int64, numpy=43>, 'segment_ids': <tf.Tensor: shape=(128,), dtype=int64, numpy=\n",
      "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])>}\n",
      "{'input_ids': <tf.Tensor: shape=(128,), dtype=int64, numpy=\n",
      "array([   2,  184,   92,   13,    1, 1455,  314,  435,   60,    3,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0])>, 'input_mask': <tf.Tensor: shape=(128,), dtype=int64, numpy=\n",
      "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])>, 'label_id': <tf.Tensor: shape=(), dtype=int64, numpy=88>, 'segment_ids': <tf.Tensor: shape=(128,), dtype=int64, numpy=\n",
      "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])>}\n"
     ]
    }
   ],
   "source": [
    "## taking only 2 rows & iterating & viewing output\n",
    "for line in processed_data.take(2):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function for reading & creating a dataset object from tfrecord\n",
    "putting the same code above into a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classifier_dataset(input_file,seq_length,batch_size,is_training=True):\n",
    "# this is a simplified version & slightly less optimized version of what is used in official bert training\n",
    "# refer to function create_classifier_dataset in models/official/nlp/data/create_finetuning_data.py\n",
    "\n",
    "\n",
    "    # create a tf_data set out of the tfrecords file\n",
    "    dataset = tf.data.TFRecordDataset(input_file)\n",
    "    name_to_features = {\n",
    "        \"input_ids\":tf.io.FixedLenFeature([max_seq_length],tf.int64),\n",
    "        \"input_mask\":tf.io.FixedLenFeature([max_seq_length],tf.int64),\n",
    "        \"segment_ids\":tf.io.FixedLenFeature([max_seq_length],tf.int64),\n",
    "        \"label_id\":tf.io.FixedLenFeature([],tf.int64)\n",
    "    }\n",
    "    ## map function processes line by line, similar to spark,scala map or pandas apply fucntion\n",
    "    dataset = dataset.map(lambda record: tf.io.parse_single_example(record, name_to_features),\n",
    "                          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "#     or could even do this\n",
    "#     dataset = dataset.map(_decode_data)\n",
    "    \n",
    "#     now separating out the features & label\n",
    "    def _select_data_from_record(record):\n",
    "#         x contains the features\n",
    "#         y is your prediction\n",
    "# This dataset will be passed to keras's model.fit refer to it's documentation for further details\n",
    "# a short snippet from that documentation\n",
    "\n",
    "# A `tf.data` dataset. Should return a tuple\n",
    "#         of either `(inputs, targets)` or\n",
    "#         `(inputs, targets, sample_weights)`.\n",
    "\n",
    "# Even a tuple of values for x works, but list doesn't work\n",
    "#         x = (\n",
    "#             record['input_ids'],\n",
    "#             record['input_mask'],\n",
    "#             record['segment_ids']\n",
    "#         )\n",
    "\n",
    "        x = {\n",
    "            'input_ids': record['input_ids'],\n",
    "            'input_mask': record['input_mask'],\n",
    "            'segment_ids': record['segment_ids']\n",
    "        }\n",
    "\n",
    "        y = record['label_id']\n",
    "        # our dataset is returning a tuple (x,y) - x are the features\n",
    "        return (x, y)\n",
    "    \n",
    "    dataset = dataset.map(_select_data_from_record,num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    # caching to avoid overhead from reading data\n",
    "    dataset = dataset.cache()\n",
    "    if is_training:\n",
    "        dataset = dataset.shuffle(100)\n",
    "        dataset = dataset.repeat()\n",
    "    \n",
    "    dataset = dataset.batch(batch_size)\n",
    "#     dataset = dataset.prefetch(5)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    return dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model code, graph creation\n",
    "This section is the core logic of the model, here we are using tensorflow hub url for albert model. Using hub simplifies the code a lot      \n",
    "It's a slightly simplied version of official bert code, that code have functionality to load a non hub model. \n",
    "#### refer to models/official/nlp/bert/run_classifier.py\n",
    "Since in this notebook we are only using one gpu, we can even ignore the strategy part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strategy = distribution_utils.get_distribution_strategy('mirrored',num_gpus=1)\n",
    "\n",
    "# strategy = tf.distribute.OneDeviceStrategy(\"device:GPU:2\")\n",
    "#since devices to use is set to 2 already, only 1 device is visible which is 0\n",
    "strategy = tf.distribute.OneDeviceStrategy(\"device:GPU:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporary tryouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_keras_layer = hub.KerasLayer(bert_hub_url,trainable=True,tags=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_keras_layer.non_trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_loaded = hub.load(bert_hub_url,tags=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'/space/engineering/tfhub_modules/d0ceaf43f67b8744561ebeeaea4c7c188a6e6f78/assets/30k-clean.model'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_keras_layer.resolved_object.sp_model_file.asset_path.numpy()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "bert_keras_layer.resolved_object.vocab_file.asset_path.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'keras_layer',\n",
       " 'trainable': True,\n",
       " 'dtype': 'float32',\n",
       " 'handle': 'https://tfhub.dev/tensorflow/albert_en_large/1'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_keras_layer.get_config()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "bert_keras_layer.resolved_object()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ConcreteFunction signature_wrapper(input_mask, input_type_ids, input_word_ids) at 0x7F811C4DA700>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = bert_loaded.signatures['serving_default']\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'/space/engineering/tfhub_modules/d0ceaf43f67b8744561ebeeaea4c7c188a6e6f78/assets/30k-clean.model'>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_hub_loaded.sp_model_file.asset_path"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tf.keras.utils.register_keras_serializable()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "###custom code from saved model folder (not hub url)\n",
    "# How does it differentiate between the bert vs albert - models/official/nlp/bert/bert_models.py\n",
    "  if isinstance(bert_config, albert_configs.AlbertConfig):  \n",
    "        kwargs['embedding_width'] = bert_config.embedding_size\n",
    "    return networks.AlbertTransformerEncoder(**kwargs)                                                                                                                 else:\n",
    "    assert isinstance(bert_config, configs.BertConfig)                                                                                                                   return networks.TransformerEncoder(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the graph\n",
    "It's just this function & your graph definition is done :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_albert_model(bert_config,num_labels,max_seq_length,hub_url):\n",
    "    \"\"\"This function return our classification model based on bert + original bert model\"\"\"\n",
    "#     https://www.tensorflow.org/hub/reusable_saved_models\n",
    "    ## Note that name parameter specified here is the same as the feature name keys in the dictionary (x) in tf.data dataset  \n",
    "    # Apparantely the code even works even if the name of the input specified here & as the key value to inputs dictionary of tf.keras.Model\n",
    "    # are different from the tf.data.Dataset input. But let's not rely on that, that seems like a potential bug\n",
    "    input_ids = tf.keras.Input(shape=(max_seq_length),dtype=tf.int32,name='input_ids')\n",
    "    input_mask = tf.keras.Input(shape=(max_seq_length),dtype=tf.int32,name='input_mask')\n",
    "    segment_ids = tf.keras.Input(shape=(max_seq_length),dtype=tf.int32,name='segment_ids')\n",
    "    \n",
    "    bert_model = hub.KerasLayer(hub_url,trainable=True,tags=None)\n",
    "    ### pooled_output will give the representation for [CLS]\n",
    "    ### sequence_output will give representations for all tokens\n",
    "    ##since it's classification task we will just use pooled_output\n",
    "    pooled_output, sequence_output = bert_model([input_ids, input_mask, segment_ids])\n",
    "    bert_output = tf.keras.layers.Dropout(rate=bert_config[\"hidden_dropout_prob\"])(pooled_output) \n",
    "    \n",
    "    initializer = tf.keras.initializers.TruncatedNormal(stddev=bert_config[\"initializer_range\"])\n",
    "    output = tf.keras.layers.Dense(num_labels,kernel_initializer=initializer,name='output',dtype=tf.float32)(bert_output)\n",
    "    \n",
    "    return tf.keras.Model(inputs={'input_ids':input_ids,\n",
    "                                 'input_mask':input_mask,\n",
    "                                 'segment_ids':segment_ids},\n",
    "                         outputs=output),bert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "## clearing the existing graphs\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at the classification model description/summary\n",
    "We are calling get_albert_model here just to print out model summary, otherwise we call it internally\n",
    "in another function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classification_model,core_bert_model = get_albert_model(bert_config=bert_config,\n",
    "                                                              num_labels=input_meta_data['num_labels'],\n",
    "                                                              max_seq_length=max_seq_length,\n",
    "                                                               hub_url=bert_hub_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_mask (InputLayer)         [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        [(None, 1024), (None 17683968    input_ids[0][0]                  \n",
      "                                                                 input_mask[0][0]                 \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 1024)         0           keras_layer[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 142)          145550      dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 17,829,518\n",
      "Trainable params: 17,829,518\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classification_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'functional_1',\n",
       " 'layers': [{'class_name': 'InputLayer',\n",
       "   'config': {'batch_input_shape': (None, 128),\n",
       "    'dtype': 'int32',\n",
       "    'sparse': False,\n",
       "    'ragged': False,\n",
       "    'name': 'input_ids'},\n",
       "   'name': 'input_ids',\n",
       "   'inbound_nodes': []},\n",
       "  {'class_name': 'InputLayer',\n",
       "   'config': {'batch_input_shape': (None, 128),\n",
       "    'dtype': 'int32',\n",
       "    'sparse': False,\n",
       "    'ragged': False,\n",
       "    'name': 'input_mask'},\n",
       "   'name': 'input_mask',\n",
       "   'inbound_nodes': []},\n",
       "  {'class_name': 'InputLayer',\n",
       "   'config': {'batch_input_shape': (None, 128),\n",
       "    'dtype': 'int32',\n",
       "    'sparse': False,\n",
       "    'ragged': False,\n",
       "    'name': 'segment_ids'},\n",
       "   'name': 'segment_ids',\n",
       "   'inbound_nodes': []},\n",
       "  {'class_name': 'KerasLayer',\n",
       "   'config': {'name': 'keras_layer',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'handle': 'https://tfhub.dev/tensorflow/albert_en_large/1'},\n",
       "   'name': 'keras_layer',\n",
       "   'inbound_nodes': [[['input_ids', 0, 0, {}],\n",
       "     ['input_mask', 0, 0, {}],\n",
       "     ['segment_ids', 0, 0, {}]]]},\n",
       "  {'class_name': 'Dropout',\n",
       "   'config': {'name': 'dropout',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'rate': 0.2,\n",
       "    'noise_shape': None,\n",
       "    'seed': None},\n",
       "   'name': 'dropout',\n",
       "   'inbound_nodes': [[['keras_layer', 0, 0, {}]]]},\n",
       "  {'class_name': 'Dense',\n",
       "   'config': {'name': 'output',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'units': 142,\n",
       "    'activation': 'linear',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'TruncatedNormal',\n",
       "     'config': {'mean': 0.0, 'stddev': 0.02, 'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None},\n",
       "   'name': 'output',\n",
       "   'inbound_nodes': [[['dropout', 0, 0, {}]]]}],\n",
       " 'input_layers': {'input_ids': ['input_ids', 0, 0],\n",
       "  'input_mask': ['input_mask', 0, 0],\n",
       "  'segment_ids': ['segment_ids', 0, 0]},\n",
       " 'output_layers': [['output', 0, 0]]}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_model.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'keras_layer',\n",
       " 'trainable': True,\n",
       " 'dtype': 'float32',\n",
       " 'handle': 'https://tfhub.dev/tensorflow/albert_en_large/1'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "core_bert_model.get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Loss function,  metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_fn(num_classes, loss_factor=1.0):\n",
    "    \"\"\"Gets the classification loss function.\"\"\"\n",
    "\n",
    "    def classification_loss_fn(labels, logits):\n",
    "        \"\"\"Classification loss.\"\"\"\n",
    "        labels = tf.squeeze(labels)\n",
    "#         log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "#         one_hot_labels = tf.one_hot(\n",
    "#             tf.cast(labels, dtype=tf.int32), depth=num_classes, dtype=tf.float32)\n",
    "#         per_example_loss = -tf.reduce_sum(\n",
    "#             tf.cast(one_hot_labels, dtype=tf.float32) * probs, axis=-1)\n",
    "        per_example_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = labels, logits=logits)\n",
    "        #batch loss\n",
    "        loss = tf.reduce_mean(per_example_loss)\n",
    "        loss *= loss_factor\n",
    "        return loss\n",
    "    return classification_loss_fn  \n",
    "\n",
    "\n",
    "def metric_fn():\n",
    "    return tf.keras.metrics.SparseCategoricalAccuracy('test_accuracy',dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function for running training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_keras_compile_fit(model_dir,\n",
    "                          strategy,\n",
    "                          model_fn,\n",
    "                          training_dataset,\n",
    "                          evaluation_dataset,\n",
    "                          loss_fn,\n",
    "                          metric_fn,\n",
    "                          init_checkpoint,\n",
    "                          epochs,\n",
    "                          steps_per_epoch,\n",
    "                          steps_per_loop,\n",
    "                          eval_steps,\n",
    "                          custom_callbacks=None):\n",
    "    \"\"\"Runs BERT classifier model using Keras compile/fit API.\"\"\"\n",
    "    ###slightly simplied version of official bert code \n",
    "    # refer to models/official/nlp/bert/run_classifier.py -   function run_keras_compile_fit\n",
    "    # if you running on a single gpu or on just cpu this strategy is not necessary\n",
    "    with strategy.scope():\n",
    "        #sub_model is the original bert\n",
    "        classification_model, sub_model = model_fn()\n",
    "        optimizer = classification_model.optimizer\n",
    "        \n",
    "        # this is not required for the hub version of the model, this restore method is trying to restor values,\n",
    "        # from a saved bert model (since we have only provided the sub_model to checkpoint)\n",
    "        # Let's saying we stopped training at some point & want to continue from that point next time,\n",
    "        # here we can instead load our classification_model & init_checkpoint can be our previous saved checkpoint file\n",
    "        if init_checkpoint:\n",
    "            checkpoint = tf.train.Checkpoint(model=sub_model)\n",
    "#             checkpoint = tf.train.Checkpoint(model=classification_model)\n",
    "            checkpoint.restore(init_checkpoint).assert_existing_objects_matched()\n",
    "#             checkpoint.restore(init_checkpoint).expect_partial()\n",
    "\n",
    "        classification_model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=loss_fn,\n",
    "            metrics=[metric_fn()])\n",
    "    #     ,experimental_steps_per_execution=steps_per_loop)\n",
    "\n",
    "        summary_dir = os.path.join(model_dir, 'summaries')\n",
    "        summary_callback = tf.keras.callbacks.TensorBoard(summary_dir, profile_batch='10,20')\n",
    "        checkpoint_path = os.path.join(model_dir, 'checkpoint')\n",
    "        checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "            checkpoint_path, save_weights_only=True)\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping()\n",
    "\n",
    "        if custom_callbacks is not None:\n",
    "            custom_callbacks += [summary_callback, checkpoint_callback, early_stopping]\n",
    "        else:\n",
    "            custom_callbacks = [summary_callback, checkpoint_callback, early_stopping]\n",
    "#       Note that we are only passing x & not y in fit function\n",
    "#       Refer to keras model.fit documentation for further details\n",
    "\n",
    "#       If `x` is a dataset, generator,\n",
    "#       or `keras.utils.Sequence` instance, `y` should\n",
    "#       not be specified (since targets will be obtained from `x`).\n",
    "        model_history = classification_model.fit(\n",
    "            x=training_dataset,\n",
    "            validation_data=evaluation_dataset,\n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            epochs=epochs,\n",
    "            validation_steps=eval_steps,\n",
    "            callbacks=custom_callbacks)\n",
    "\n",
    "        return classification_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/var/extra/users/jgeorge/tf2.0/input/dish/models/albert_en_large'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = '/var/extra/users/jgeorge/tf2.0/input/dish/models/albert_en_large_softmax'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The training function call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "## clearing the existing graphs\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### defining various parameters for the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1446\n"
     ]
    }
   ],
   "source": [
    "steps_per_loop = 1\n",
    "learning_rate=1e-5\n",
    "epochs=20\n",
    "\n",
    "train_batch_size=16\n",
    "eval_batch_size=16\n",
    "\n",
    "train_data_size = input_meta_data['train_data_size']\n",
    "steps_per_epoch = int(train_data_size / train_batch_size)\n",
    "print(steps_per_epoch)\n",
    "\n",
    "warmup_steps = int(epochs * train_data_size * 0.1 / train_batch_size)\n",
    "eval_steps = int(math.ceil(input_meta_data['eval_data_size'] / eval_batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training & validation dataset object creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = get_classifier_dataset(train_data_path,max_seq_length,train_batch_size,is_training=True)\n",
    "evaluation_dataset = get_classifier_dataset(eval_data_path,max_seq_length,eval_batch_size,is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'input_ids': <tf.Tensor: shape=(16, 128), dtype=int64, numpy=\n",
      "array([[   2,   13,    1, ...,    0,    0,    0],\n",
      "       [   2,   13,    1, ...,    0,    0,    0],\n",
      "       [   2,   63, 3108, ...,    0,    0,    0],\n",
      "       ...,\n",
      "       [   2,   13,    1, ...,    0,    0,    0],\n",
      "       [   2,   13,    1, ...,    0,    0,    0],\n",
      "       [   2,   13,    1, ...,    0,    0,    0]])>, 'input_mask': <tf.Tensor: shape=(16, 128), dtype=int64, numpy=\n",
      "array([[1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0]])>, 'segment_ids': <tf.Tensor: shape=(16, 128), dtype=int64, numpy=\n",
      "array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]])>}, <tf.Tensor: shape=(16,), dtype=int64, numpy=\n",
      "array([ 95,  37,  18,  53,  89,   5,  30,  39,   0,   6, 131,  25,  53,\n",
      "        30,  14,  13])>)\n",
      "({'input_ids': <tf.Tensor: shape=(16, 128), dtype=int64, numpy=\n",
      "array([[  2,  13,   1, ...,   0,   0,   0],\n",
      "       [  2, 483,  25, ...,   0,   0,   0],\n",
      "       [  2,  13,   1, ...,   0,   0,   0],\n",
      "       ...,\n",
      "       [  2,  13,   1, ...,   0,   0,   0],\n",
      "       [  2,  13,   1, ...,   0,   0,   0],\n",
      "       [  2,  13,   1, ...,   0,   0,   0]])>, 'input_mask': <tf.Tensor: shape=(16, 128), dtype=int64, numpy=\n",
      "array([[1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0]])>, 'segment_ids': <tf.Tensor: shape=(16, 128), dtype=int64, numpy=\n",
      "array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]])>}, <tf.Tensor: shape=(16,), dtype=int64, numpy=\n",
      "array([ 24, 126,   9,   1, 103,  91, 136, 128,  91,  15,  15,  58,  97,\n",
      "       103,  97, 127])>)\n"
     ]
    }
   ],
   "source": [
    "## taking only 2 rows & iterating & viewing output\n",
    "for line in training_dataset.take(2):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_multiplier = 1\n",
    "max_seq_length = input_meta_data['max_seq_length']\n",
    "num_classes = input_meta_data['num_labels']\n",
    "loss_fn = get_loss_fn(num_classes,loss_multiplier)\n",
    "\n",
    "def _get_classifier_model():\n",
    "    classifier_model,core_model = get_albert_model(bert_config=bert_config,\n",
    "                                                              num_labels=num_classes,\n",
    "                                                              max_seq_length=max_seq_length,\n",
    "                                                               hub_url=bert_hub_url)\n",
    "    ##This is basically Adam optimizer with weight decay after set no of warm up steps & \n",
    "    # before that an increasing learning rate from 0 to initial_lr\n",
    "    # refer to models/official/nlp/optimization.py for more details\n",
    "    classifier_model.optimizer = optimization.create_optimizer(init_lr=learning_rate,\n",
    "                                                               num_train_steps=steps_per_epoch*epochs,\n",
    "                                                               num_warmup_steps=warmup_steps)\n",
    "    return classifier_model,core_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using Adamw optimizer\n",
      "Epoch 1/20\n",
      "  19/1446 [..............................] - ETA: 15:23 - loss: 5.1123 - test_accuracy: 0.0197WARNING:tensorflow:From /opt/custom/python/anaconda3/envs/tf2.3/lib/python3.8/site-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "From /opt/custom/python/anaconda3/envs/tf2.3/lib/python3.8/site-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "1284/1446 [=========================>....] - ETA: 1:55 - loss: 4.5122 - test_accuracy: 0.0881"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-ab00938ada91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m trained_model = run_keras_compile_fit(model_dir=output_folder,strategy=strategy,model_fn=_get_classifier_model,\n\u001b[0m\u001b[1;32m      2\u001b[0m                                      \u001b[0mtraining_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mevaluation_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevaluation_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                       \u001b[0mloss_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetric_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                      \u001b[0minit_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                       \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-66-03f1d075da07>\u001b[0m in \u001b[0;36mrun_keras_compile_fit\u001b[0;34m(model_dir, strategy, model_fn, training_dataset, evaluation_dataset, loss_fn, metric_fn, init_checkpoint, epochs, steps_per_epoch, steps_per_loop, eval_steps, custom_callbacks)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;31m#       or `keras.utils.Sequence` instance, `y` should\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;31m#       not be specified (since targets will be obtained from `x`).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         model_history = classification_model.fit(\n\u001b[0m\u001b[1;32m     57\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevaluation_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1842\u001b[0m     \"\"\"\n\u001b[0;32m-> 1843\u001b[0;31m     return self._call_flat(\n\u001b[0m\u001b[1;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[1;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.3/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "trained_model = run_keras_compile_fit(model_dir=output_folder,strategy=strategy,model_fn=_get_classifier_model,\n",
    "                                     training_dataset=training_dataset,evaluation_dataset=evaluation_dataset,\n",
    "                                      loss_fn=loss_fn,metric_fn=metric_fn,\n",
    "                                     init_checkpoint=None,\n",
    "                                      epochs=epochs,\n",
    "                                      steps_per_epoch=steps_per_epoch,\n",
    "                                      steps_per_loop=steps_per_loop,\n",
    "                                      eval_steps=eval_steps\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trying training for a few more epochs"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## clearing the existing graphs\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "epochs=5\n",
    "init_checkpoint = tf.train.latest_checkpoint(output_folder)\n",
    "trained_model = run_keras_compile_fit(model_dir=output_folder,strategy=strategy,model_fn=_get_classifier_model,\n",
    "                                     training_dataset=training_dataset,evaluation_dataset=evaluation_dataset,\n",
    "                                      loss_fn=loss_fn,metric_fn=metric_fn,\n",
    "                                     init_checkpoint=init_checkpoint,\n",
    "                                      epochs=epochs,\n",
    "                                      steps_per_epoch=steps_per_epoch,\n",
    "                                      steps_per_loop=steps_per_loop,\n",
    "                                      eval_steps=eval_steps\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model\n",
    "The model that we trained right now takes the sentence piece encoded list as input, along with input_mask list, segment_ids list\n",
    "When we are deploying we want to give a text as input & get the text label as output.   \n",
    "So what we are going to do is to have a graph with sentence piece encoding within the graph itself & create input_mask, segment_ids also as part of the graph for input   \n",
    "For the output have a softmax & a table lookup to on labels to return the label name "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence piece tokenizaton\n",
    "I'll walk through details of this Layer after we are done saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SentencepieceTokenization(tf.keras.layers.Layer):\n",
    "    def __init__(self,model_path,max_seq_length):\n",
    "        super(SentencepieceTokenization, self).__init__(trainable=False,dtype=tf.int64)\n",
    "        self.model_path = model_path\n",
    "        self.max_seq_length = max_seq_length\n",
    "        \n",
    "    def build(self,input_shape):\n",
    "        self.model_proto = tf.io.gfile.GFile(self.model_path, 'rb').read()\n",
    "        self.sp_tokenizer =  tftext.SentencepieceTokenizer(model=self.model_proto)\n",
    "        ## the output from id_to_string can be used to validate CLS_ID, SEP_ID\n",
    "        self.sp_tokenizer.id_to_string([0,1,2,3])\n",
    "        ## pad_id is usually 0 \n",
    "#         [self.CLS_ID,self.SEP_ID,self.PAD_ID]  = self.sp_tokenizer.tokenize(['[CLS]','[SEP]','<pad>'],model_file=self.model_path)\n",
    "        [self.PAD_ID, self.CLS_ID, self.SEP_ID] = [0, 2, 3]\n",
    "        self.built=True\n",
    "    \n",
    "    def call(self, input_text):\n",
    "        ##tensorflow sentence piece works while exporting to graph while, tf_text sentencepiece doesn't\n",
    "        encoded_text = self.get_encoded_text(input_text,max_seq_length=self.max_seq_length)\n",
    "        return encoded_text\n",
    "    \n",
    "    # converting this to tf.function will make all the inputs a tensor & some of the operation require max_seq_length to be\n",
    "    # a regular integer. Rather than passing it as an argument we could also pick up the variable from the object\n",
    "#     @tf.function\n",
    "    def get_encoded_text(self,input_text_batch,max_seq_length):\n",
    "        # sp_encoded is a ragged tensor 2 dimensional tensor with 1st dimension being the batch size\n",
    "        sp_encoded = self.sp_tokenizer.tokenize(input_text_batch)\n",
    "        # tf.shape gives dynamic shape &\n",
    "        # tenorflow_variable.shape gives static shape with dynamic entries = None\n",
    "#             actual_token_length = tf.shape(concat)[-1]\n",
    "        # tf.shape was failing with ragged tensor, hence converting it to regular tensor for getting batch dimension\n",
    "        # TODO find if there is better way to get shape\n",
    "        batch_size_dynamic = tf.shape(sp_encoded.to_tensor())[0]\n",
    "        batch_size = sp_encoded.shape[0]\n",
    "#         print(f'sp_encoded {sp_encoded}')\n",
    "        # casting down to int32 from int64 as subsequent operations required int32\n",
    "        sequence_length = tf.cast(sp_encoded.row_lengths(),tf.int32)\n",
    "#         trimming to max_length-2 (-2 to incorporate [CLS], [SEP])\n",
    "        trimmed_max_length = max_seq_length-2\n",
    "#         print(f'sequence_length {sequence_length} trimmed_max_length {trimmed_max_length}')\n",
    "        # keeping the batch dimension (1st dimension) as is & trimming the 2nd one\n",
    "        values_trimmed = sp_encoded[:,:trimmed_max_length]\n",
    "#         print(f'values_trimmed {values_trimmed}')\n",
    "\n",
    "        # let's say batch_size = 2 then values_to_prepend will have shape=(2, 1), similary values_to_append will have the same shape\n",
    "        value_to_prepend = tf.expand_dims(tf.repeat([self.CLS_ID],repeats=batch_size_dynamic),axis=1)\n",
    "        value_to_append = tf.expand_dims(tf.repeat([self.SEP_ID],repeats=batch_size_dynamic),axis=1)\n",
    "        concat = tf.concat([value_to_prepend,values_trimmed,value_to_append],axis=1,name='concat_out')\n",
    "\n",
    "\n",
    "        # concat is a ragged tensor & to_tensor will by default pad by 0\n",
    "        padded = concat.to_tensor(shape=(None,max_seq_length))\n",
    "#         segment_ids = tf.zeros(shape=tf.shape(padded),dtype=tf.int32)\n",
    "#         or\n",
    "        # This will create zeros with similar shape as padded tensor\n",
    "        # for classification use case we only have 1 segment \n",
    "        # (for tasks which require distinguishing between 1st & 2nd sentence there will be 2 values )\n",
    "        segment_ids = tf.zeros_like(padded,dtype=tf.int32,name='segment_ids')\n",
    "        \n",
    "        # input mask have 1 as the value for all positions with a token & zero for the remaining ones\n",
    "        input_mask = tf.ones_like(concat).to_tensor(shape=(None,max_seq_length))\n",
    "        return (padded,segment_ids,input_mask)\n",
    "    \n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(SentencepieceTokenization, self).get_config()\n",
    "        config.update({'model_path': self.model_path})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_piece_layer_temp = SentencepieceTokenization(sentencepiece_path,5)\n",
    "sentence_piece_layer_temp.build(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[    2, 10975,   930,    20,     3]], dtype=int32)>,\n",
       " <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[0, 0, 0, 0, 0]], dtype=int32)>,\n",
       " <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[1, 1, 1, 1, 1]], dtype=int32)>)"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_piece_layer_temp.call(tf.constant(['hello talk to an agent and test new text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def get_temp_encoded(text):\n",
    "    sp_encoded = sp_tokenizer.tokenize(text)\n",
    "    return sp_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[10975, 930, 20, 40, 2267, 17, 1289, 78, 1854], [10975], [226, 26660, 1854]]>"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_encoded = get_temp_encoded(tf.constant(['hello talk to an agent and test new text',\"hello\",\"another dummy text\"]))\n",
    "sp_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelLookup(tf.keras.layers.Layer):\n",
    "#     def __init__(self,model_proto):\n",
    "    def __init__(self,filepath,default,*args,**kwargs):\n",
    "        \"\"\" \n",
    "        filepath - path to the text file with 1 label per line\n",
    "        \"\"\"\n",
    "        super(LabelLookup, self).__init__(trainable=False,dtype=tf.int64,*args, **kwargs)\n",
    "        self.filepath = filepath\n",
    "        self.default=default\n",
    "        \n",
    "    def build(self,input_shape):\n",
    "        # need to convert label index to label name. So keeping line no as key & whole text line as label\n",
    "        # issue with using table lookup (both file based & key value tensorbased) in tf2.2 (tf2.x) - https://github.com/tensorflow/tensorflow/issues/38305\n",
    "        self.table = tf.lookup.StaticHashTable(\n",
    "            tf.lookup.TextFileInitializer(self.filepath,\n",
    "                                          key_dtype=tf.int64,key_index=tf.lookup.TextFileIndex.LINE_NUMBER,\n",
    "                                     value_dtype=tf.string,value_index=tf.lookup.TextFileIndex.WHOLE_LINE),self.default)\n",
    "        self.built=True\n",
    "        \n",
    "    def call(self, input_index):\n",
    "        word_ids = self.table.lookup(input_index)\n",
    "        return word_ids\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(LabelLookup, self).get_config()\n",
    "#         config.update({'filepath': self.filepath})\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tensorflow_hub.keras_layer.KerasLayer object at 0x7f7d85db3b50> and <tensorflow.python.keras.layers.core.Dense object at 0x7f7d85d36be0>).\n",
      "Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tensorflow_hub.keras_layer.KerasLayer object at 0x7f7d85db3b50> and <tensorflow.python.keras.layers.core.Dense object at 0x7f7d85d36be0>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tensorflow.python.keras.layers.core.Dense object at 0x7f7d85d36be0> and <tensorflow.python.keras.engine.base_layer.TensorFlowOpLayer object at 0x7f7d6be87ac0>).\n",
      "Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tensorflow.python.keras.layers.core.Dense object at 0x7f7d85d36be0> and <tensorflow.python.keras.engine.base_layer.TensorFlowOpLayer object at 0x7f7d6be87ac0>).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f7d6be8e280>"
      ]
     },
     "execution_count": 507,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_length = input_meta_data['max_seq_length']\n",
    "num_labels = input_meta_data['num_labels']\n",
    "initial_lr = learning_rate\n",
    "# default_label_index = 1\n",
    "default_label=input_meta_data['default_label']\n",
    "max_top_intents = 5\n",
    "checkpoint_path=tf.train.latest_checkpoint(output_folder)\n",
    "\n",
    "input_text = tf.keras.Input(shape=(),dtype=tf.string,name='input_text')\n",
    "sentence_piece_layer = SentencepieceTokenization(model_path=sentencepiece_path,max_seq_length=tf.constant(max_seq_length))\n",
    "input_ids_processed,segment_ids_processed,input_mask_processed = sentence_piece_layer(input_text)\n",
    "\n",
    "\n",
    "bert_model = hub.KerasLayer(bert_hub_url,trainable=True,tags=None)\n",
    "### pooled_output will give the representation for [CLS]\n",
    "### sequence_output will give representations for all tokens\n",
    "## since it's classification task we will just use pooled_output\n",
    "pooled_output, sequence_output = bert_model([input_ids_processed, input_mask_processed, segment_ids_processed])\n",
    "#     bert_output = tf.keras.layers.Dropout(rate=bert_config.hidden_dropout_prob)(pooled_output) \n",
    "\n",
    "# This doesn't really matter as we will be loading the weights from saved checkpoint\n",
    "initializer = tf.keras.initializers.TruncatedNormal(stddev=bert_config[\"initializer_range\"])\n",
    "#     output = tf.keras.layers.Dense(num_labels,kernel_initializer=initializer,name='output',dtype=tf.float32)(bert_output)\n",
    "# dense_output = tf.keras.layers.Dense(num_labels,kernel_initializer=initializer,name='output',dtype=tf.float32,activation='softmax')(pooled_output)\n",
    "dense_output = tf.keras.layers.Dense(num_labels,kernel_initializer=initializer,name='output',dtype=tf.float32)(pooled_output)\n",
    "# log_probs = tf.nn.log_softmax(dense_output, axis=-1,name='scores')\n",
    "probs = tf.nn.softmax(dense_output,axis=-1,name='scores')\n",
    "# top_label_id = tf.argmax(log_probs,axis=-1)\n",
    "top_label_id = tf.argmax(probs,axis=-1)\n",
    "# sorted_indexes = tf.slice(tf.cast(tf.argsort(probs,axis=-1,direction='DESCENDING'),tf.int64),0,max_top_intents)\n",
    "sorted_indexes = tf.cast(tf.argsort(probs,axis=-1,direction='DESCENDING'),tf.int64)[:,:max_top_intents]\n",
    "# sorted_scores = tf.gather(log_probs,sorted_indexes, axis=-1)\n",
    "# sorted_scores1 = tf.slice(tf.sort(probs,axis=-1,direction='DESCENDING',name=\"scores1\"),0,max_top_intents)\n",
    "sorted_scores1 = tf.sort(probs,axis=-1,direction='DESCENDING',name=\"scores1\")[:,:max_top_intents]\n",
    "# this identity operation was only required to get the name correctly as in tf2.3 the sort object's name was coming differently\n",
    "# https://github.com/tensorflow/tensorflow/issues/39398\n",
    "# https://stackoverflow.com/questions/38626424/how-to-assign-new-name-or-rename-an-existing-tensor-in-tensorflow\n",
    "sorted_scores = tf.identity(sorted_scores1,name='scores_sorted')\n",
    "\n",
    "top_label = LabelLookup(filepath=intent_file,default=default_label)(top_label_id)\n",
    "labels = LabelLookup(filepath=intent_file,default=default_label,name=\"classes\")(sorted_indexes)\n",
    "\n",
    "loaded_model  = tf.keras.Model(inputs={'input_text':input_text},\n",
    "                     outputs={\"classes\":labels,\"scores\":sorted_scores})\n",
    "\n",
    "\n",
    "# loaded_model.compile()\n",
    "#     checkpoint = tf.train.Checkpoint(model=model)\n",
    "#     checkpoint.restore(checkpoint_path).assert_existing_objects_matched()\n",
    "loaded_model.load_weights(checkpoint_path).assert_existing_objects_matched()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'strided_slice_3:0' shape=(None, None) dtype=float32>"
      ]
     },
     "execution_count": 506,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_scores1[:,:max_top_intents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loaded_model.output_names = ['classes', 'scores']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ListWrapper(['classes', 'scores'])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.output_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classes': array([[b'representative-request', b'other-other', b'get-help',\n",
       "         b'channel_package-vague', b'payment_extension-request',\n",
       "         b'channel_package-cancel', b'account-vague',\n",
       "         b'reactivate-reinstate', b'return-query',\n",
       "         b'comp_part_signal_loss-issue', b'channel_package-issue',\n",
       "         b'spanish-query', b'channel_package-upgrade',\n",
       "         b'billing-preferences', b'account-cancel',\n",
       "         b'account-name_change', b'payment-query', b'channel-list',\n",
       "         b'contract_expiry-query', b'autopay-cancel',\n",
       "         b'blue_black_snowy-issue', b'account-address_change',\n",
       "         b'channel_package-change', b'online_account-unable_login',\n",
       "         b'appointment-tech_get_help', b'payment_arrangement-vague',\n",
       "         b'dish_pause-cancel', b'contract-query',\n",
       "         b'channel_package-query', b'payment-reinstate_service',\n",
       "         b'autopay-setup', b'tech-issue', b'referral-how_to',\n",
       "         b'dish_move-query', b'remote-setup', b'promotion-query',\n",
       "         b'promotion-get_help', b'autopay-query',\n",
       "         b'dish_protection-query', b'payment-get_help',\n",
       "         b'remote-place_order', b'channel-guide', b'return-get_help',\n",
       "         b'account_info-get_help', b'payment-vague',\n",
       "         b'internet-check_availability', b'billing-balance_query',\n",
       "         b'payment-make', b'greet-hello', b'online_account-setup',\n",
       "         b'greet-thank_you', b'greet-goodbye', b'authorization-get_help',\n",
       "         b'online_account-forgot_login',\n",
       "         b'payment_extension-already_done', b'autopay-get_help',\n",
       "         b'referral-code_not_received', b'account_cancel-query',\n",
       "         b'appointment-schedule', b'account_cancel-promotion_seek',\n",
       "         b'yes-give', b'online_account-password_reset', b'billing-issue',\n",
       "         b'dish_pause-query', b'order-cancel', b'channel_price-query',\n",
       "         b'payment-status', b'order-get_help', b'price-increase',\n",
       "         b'receiver-place_order', b'appointment-cancel',\n",
       "         b'security_code-query', b'univision-query', b'equipment-upgrade',\n",
       "         b'recording-issue', b'billing-query', b'receiver_upgrade-query',\n",
       "         b'account-transfer', b'remote-not_working',\n",
       "         b'online_account-login_locked', b'internet-cancel',\n",
       "         b'equipment-vague', b'remote-vague', b'return-status',\n",
       "         b'recording-query', b'joey-issues', b'streaming-issues',\n",
       "         b'hbocinemax-takedown', b'redeem_offer-get_help', b'ppv-order',\n",
       "         b'online_account-forgot_password', b'renewal-query',\n",
       "         b'order-status', b'ppv-query', b'receiver-replacement',\n",
       "         b'billing-vague', b'price-vague', b'redeem_offer-query',\n",
       "         b'on_demand-query', b'appointment-query',\n",
       "         b'online_account-forgot_id_password', b'equipment-place_order',\n",
       "         b'ppv-cancel', b'account_cancel-fee', b'payment_schedule-query',\n",
       "         b'fee-waive_charge', b'receiver-remove',\n",
       "         b'equipment-add_receiver', b'game_popup-issue',\n",
       "         b'equipment_setup-get_help', b'bundle-query', b'internet-query',\n",
       "         b'internet-get_help', b'ppv-issue', b'equipment-issue',\n",
       "         b'app-query', b'receiver-activate', b'remote-order_status',\n",
       "         b'streaming-query', b'app-issue', b'contract-cancel_charge',\n",
       "         b'movie-place_order', b'internet-price_query',\n",
       "         b'receiver-get_help', b'movie_order-get_help', b'receiver-query',\n",
       "         b'refund-get_help', b'order-query', b'equipment-price_query',\n",
       "         b'joey-vague', b'refund-query', b'no-give',\n",
       "         b'receiver_issue-get_help', b'dvr-query', b'refund-vague',\n",
       "         b'appointment-reschedule', b'internet_data-query',\n",
       "         b'equipment-setup', b'equipment-query', b'dvr-get_help',\n",
       "         b'installation-vague', b'hopper-issues'],\n",
       "        [b'remote-setup', b'remote-not_working', b'remote-vague',\n",
       "         b'equipment_setup-get_help', b'comp_part_signal_loss-issue',\n",
       "         b'account-vague', b'other-other', b'channel_package-cancel',\n",
       "         b'blue_black_snowy-issue', b'channel_package-issue',\n",
       "         b'remote-place_order', b'channel_package-change',\n",
       "         b'representative-request', b'payment_extension-request',\n",
       "         b'tech-issue', b'equipment-issue', b'reactivate-reinstate',\n",
       "         b'billing-vague', b'authorization-get_help', b'price-increase',\n",
       "         b'payment-reinstate_service', b'payment-query',\n",
       "         b'payment_arrangement-vague', b'payment-vague', b'autopay-setup',\n",
       "         b'account-cancel', b'receiver_issue-get_help',\n",
       "         b'dish_move-query', b'account-address_change',\n",
       "         b'equipment-upgrade', b'dvr-query', b'account_cancel-query',\n",
       "         b'spanish-query', b'channel-guide', b'channel_package-vague',\n",
       "         b'streaming-issues', b'payment-make', b'payment-get_help',\n",
       "         b'channel_package-upgrade', b'channel-list', b'autopay-cancel',\n",
       "         b'app-issue', b'billing-preferences', b'receiver-get_help',\n",
       "         b'recording-issue', b'dvr-get_help',\n",
       "         b'payment_extension-already_done', b'equipment-setup',\n",
       "         b'autopay-query', b'hopper-issues', b'dish_pause-query',\n",
       "         b'security_code-query', b'remote-order_status',\n",
       "         b'appointment-schedule', b'order-cancel', b'dish_pause-cancel',\n",
       "         b'billing-issue', b'ppv-cancel', b'account_info-get_help',\n",
       "         b'greet-hello', b'bundle-query', b'contract_expiry-query',\n",
       "         b'internet-query', b'recording-query',\n",
       "         b'appointment-tech_get_help', b'online_account-forgot_login',\n",
       "         b'greet-thank_you', b'account-name_change',\n",
       "         b'promotion-get_help', b'installation-vague',\n",
       "         b'receiver_upgrade-query', b'appointment-cancel',\n",
       "         b'appointment-query', b'get-help', b'app-query',\n",
       "         b'streaming-query', b'autopay-get_help',\n",
       "         b'online_account-unable_login', b'internet-cancel',\n",
       "         b'fee-waive_charge', b'no-give', b'joey-issues',\n",
       "         b'payment-status', b'account-transfer', b'order-status',\n",
       "         b'promotion-query', b'game_popup-issue',\n",
       "         b'payment_schedule-query', b'online_account-login_locked',\n",
       "         b'equipment-add_receiver', b'referral-code_not_received',\n",
       "         b'internet-check_availability', b'receiver-query',\n",
       "         b'referral-how_to', b'online_account-password_reset',\n",
       "         b'account_cancel-promotion_seek', b'internet-get_help',\n",
       "         b'equipment-vague', b'return-query', b'receiver-activate',\n",
       "         b'order-query', b'receiver-remove', b'dish_protection-query',\n",
       "         b'yes-give', b'online_account-setup', b'appointment-reschedule',\n",
       "         b'channel_package-query', b'online_account-forgot_password',\n",
       "         b'ppv-issue', b'equipment-query', b'on_demand-query',\n",
       "         b'ppv-order', b'joey-vague', b'billing-query',\n",
       "         b'account_cancel-fee', b'greet-goodbye', b'receiver-replacement',\n",
       "         b'contract-cancel_charge', b'contract-query', b'ppv-query',\n",
       "         b'online_account-forgot_id_password', b'equipment-place_order',\n",
       "         b'hbocinemax-takedown', b'order-get_help', b'univision-query',\n",
       "         b'movie_order-get_help', b'return-get_help', b'refund-vague',\n",
       "         b'redeem_offer-query', b'redeem_offer-get_help',\n",
       "         b'internet_data-query', b'billing-balance_query',\n",
       "         b'receiver-place_order', b'renewal-query', b'refund-get_help',\n",
       "         b'movie-place_order', b'price-vague', b'refund-query',\n",
       "         b'return-status', b'channel_price-query',\n",
       "         b'internet-price_query', b'equipment-price_query']], dtype=object),\n",
       " 'scores': array([[9.86524940e-01, 2.39529693e-03, 9.68928973e-04, 8.03684816e-04,\n",
       "         6.72105292e-04, 6.58492499e-04, 6.12090284e-04, 5.57008723e-04,\n",
       "         3.04841931e-04, 2.85558723e-04, 2.75029044e-04, 2.29599973e-04,\n",
       "         2.18054643e-04, 2.05491597e-04, 1.74108980e-04, 1.66073049e-04,\n",
       "         1.62816665e-04, 1.48402687e-04, 1.46102480e-04, 1.44529840e-04,\n",
       "         1.32905087e-04, 1.29938548e-04, 1.20848388e-04, 1.19216718e-04,\n",
       "         1.10846580e-04, 1.08858752e-04, 9.80130208e-05, 9.64218052e-05,\n",
       "         9.48000888e-05, 9.20310777e-05, 8.38557607e-05, 8.35687606e-05,\n",
       "         8.23763112e-05, 7.81602284e-05, 7.31559994e-05, 7.09966334e-05,\n",
       "         7.07044019e-05, 6.77120115e-05, 6.39927530e-05, 6.27246773e-05,\n",
       "         6.11339201e-05, 6.03539011e-05, 6.02026521e-05, 5.88320690e-05,\n",
       "         5.77951541e-05, 5.52182501e-05, 5.48689022e-05, 5.46575975e-05,\n",
       "         5.28685559e-05, 5.19945825e-05, 5.12743718e-05, 4.79556911e-05,\n",
       "         4.79219052e-05, 4.75924353e-05, 4.73925174e-05, 4.55111331e-05,\n",
       "         4.54609908e-05, 4.42966411e-05, 4.18400232e-05, 4.10590255e-05,\n",
       "         4.03559061e-05, 3.81333630e-05, 3.79943303e-05, 3.73962939e-05,\n",
       "         3.60133963e-05, 3.52353236e-05, 3.41688028e-05, 3.40372790e-05,\n",
       "         3.19244282e-05, 3.13263627e-05, 2.89966683e-05, 2.89707950e-05,\n",
       "         2.84277685e-05, 2.72510206e-05, 2.63497495e-05, 2.59059543e-05,\n",
       "         2.55027135e-05, 2.39418914e-05, 2.31882004e-05, 2.26119791e-05,\n",
       "         2.24609958e-05, 2.22201361e-05, 2.19251233e-05, 2.16876961e-05,\n",
       "         2.16809567e-05, 2.13176845e-05, 2.13052663e-05, 2.09283044e-05,\n",
       "         2.07176872e-05, 2.03564614e-05, 2.03234449e-05, 1.98806174e-05,\n",
       "         1.91773470e-05, 1.86396173e-05, 1.75696641e-05, 1.73526223e-05,\n",
       "         1.70959338e-05, 1.62120268e-05, 1.61415228e-05, 1.59019019e-05,\n",
       "         1.55425114e-05, 1.54457866e-05, 1.53255587e-05, 1.51908425e-05,\n",
       "         1.50269270e-05, 1.44907654e-05, 1.39226022e-05, 1.38394935e-05,\n",
       "         1.37641646e-05, 1.30299477e-05, 1.30018689e-05, 1.25333236e-05,\n",
       "         1.24280468e-05, 1.23140908e-05, 1.22709380e-05, 1.21813573e-05,\n",
       "         1.20729892e-05, 1.19874367e-05, 1.16692700e-05, 1.13961032e-05,\n",
       "         1.13354909e-05, 1.12812768e-05, 1.12670959e-05, 1.10941610e-05,\n",
       "         1.09748589e-05, 1.09334060e-05, 1.07142896e-05, 1.04999335e-05,\n",
       "         1.01575997e-05, 9.77621221e-06, 9.68283166e-06, 8.80243078e-06,\n",
       "         8.51427922e-06, 7.99622194e-06, 7.43614510e-06, 7.40342466e-06,\n",
       "         7.39050847e-06, 7.34531341e-06, 7.07751997e-06, 6.87593274e-06,\n",
       "         6.77770959e-06, 6.07889524e-06],\n",
       "        [9.91001070e-01, 3.08521488e-03, 9.30262147e-04, 2.63479509e-04,\n",
       "         2.28828518e-04, 1.94551816e-04, 1.86918172e-04, 1.62503289e-04,\n",
       "         1.57106027e-04, 1.45371960e-04, 1.38136165e-04, 1.33035559e-04,\n",
       "         1.22968224e-04, 1.07908440e-04, 8.43088128e-05, 7.76231536e-05,\n",
       "         7.24197016e-05, 7.20388489e-05, 6.78838551e-05, 6.39654681e-05,\n",
       "         6.11511059e-05, 6.05727255e-05, 5.87153081e-05, 5.57001731e-05,\n",
       "         5.27551820e-05, 5.19941314e-05, 5.15377760e-05, 4.97913206e-05,\n",
       "         4.47670609e-05, 4.40727199e-05, 4.39785872e-05, 4.29316497e-05,\n",
       "         4.24340651e-05, 4.16559997e-05, 4.16387629e-05, 4.06491163e-05,\n",
       "         3.99714227e-05, 3.92595430e-05, 3.90280220e-05, 3.88411427e-05,\n",
       "         3.84439154e-05, 3.77784781e-05, 3.56026176e-05, 3.53132964e-05,\n",
       "         3.52603965e-05, 3.46550223e-05, 3.39093313e-05, 3.38411664e-05,\n",
       "         3.33733333e-05, 3.32046002e-05, 3.27438174e-05, 3.14324170e-05,\n",
       "         3.14212084e-05, 3.09512689e-05, 2.99640869e-05, 2.88773099e-05,\n",
       "         2.76451774e-05, 2.69977754e-05, 2.52034806e-05, 2.51766942e-05,\n",
       "         2.46028849e-05, 2.45155006e-05, 2.42527021e-05, 2.29908983e-05,\n",
       "         2.29487086e-05, 2.27316104e-05, 2.25372733e-05, 2.23954357e-05,\n",
       "         2.20680413e-05, 2.19809372e-05, 2.19141893e-05, 2.16200024e-05,\n",
       "         2.15763757e-05, 2.15243599e-05, 2.08713373e-05, 2.06726472e-05,\n",
       "         2.00910745e-05, 1.99668593e-05, 1.89262519e-05, 1.85272202e-05,\n",
       "         1.78378905e-05, 1.78087212e-05, 1.75217083e-05, 1.75173645e-05,\n",
       "         1.73900444e-05, 1.73784556e-05, 1.71420033e-05, 1.67858561e-05,\n",
       "         1.66554837e-05, 1.65464171e-05, 1.62164306e-05, 1.60627169e-05,\n",
       "         1.60173295e-05, 1.56942751e-05, 1.54256195e-05, 1.47980181e-05,\n",
       "         1.47289156e-05, 1.43674279e-05, 1.42048102e-05, 1.40564143e-05,\n",
       "         1.38535297e-05, 1.36228909e-05, 1.33924777e-05, 1.33701451e-05,\n",
       "         1.29896480e-05, 1.28241318e-05, 1.27291878e-05, 1.25763900e-05,\n",
       "         1.24193011e-05, 1.19684282e-05, 1.18523549e-05, 1.13958640e-05,\n",
       "         1.13143751e-05, 1.12500120e-05, 1.01270907e-05, 9.24826418e-06,\n",
       "         9.11883762e-06, 8.77395087e-06, 8.52532048e-06, 8.38242158e-06,\n",
       "         7.90328704e-06, 7.80724986e-06, 7.80676510e-06, 7.67105757e-06,\n",
       "         7.67036363e-06, 7.50522395e-06, 6.74067633e-06, 6.65046446e-06,\n",
       "         6.61851300e-06, 6.57990859e-06, 6.53670259e-06, 6.39237805e-06,\n",
       "         6.33371656e-06, 5.77286301e-06, 5.61566048e-06, 4.97911060e-06,\n",
       "         4.64579762e-06, 4.12548616e-06, 3.67683901e-06, 3.20641516e-06,\n",
       "         3.19905394e-06, 2.96326107e-06]], dtype=float32)}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = loaded_model.predict(tf.constant(['talk to agent', 'remote not working']))\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'TopKV2_1:0' shape=(None, None) dtype=float32>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_scores1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'scores_sorted:0' shape=(None, None) dtype=float32>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_text (InputLayer)         [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sentencepiece_tokenization (Sen ((None, None), (None 0           input_text[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        [(None, 1024), (None 17683968    sentencepiece_tokenization[0][0] \n",
      "                                                                 sentencepiece_tokenization[0][2] \n",
      "                                                                 sentencepiece_tokenization[0][1] \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 142)          145550      keras_layer[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_scores (TensorFlowO [(None, 142)]        0           output[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape (TensorFlowOp [(2,)]               0           tf_op_layer_scores[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [()]                 0           tf_op_layer_Shape[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_1 (TensorFlow [(2,)]               0           tf_op_layer_scores[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_TopKV2 (TensorFlowO [(None, None), (None 0           tf_op_layer_scores[0][0]         \n",
      "                                                                 tf_op_layer_strided_slice[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_1 (Te [()]                 0           tf_op_layer_Shape_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Cast (TensorFlowOpL [(None, None)]       0           tf_op_layer_TopKV2[0][1]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_TopKV2_1 (TensorFlo [(None, None), (None 0           tf_op_layer_scores[0][0]         \n",
      "                                                                 tf_op_layer_strided_slice_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "classes (LabelLookup)           (None, None)         0           tf_op_layer_Cast[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_scores_sorted (Tens [(None, None)]       0           tf_op_layer_TopKV2_1[0][0]       \n",
      "==================================================================================================\n",
      "Total params: 17,829,518\n",
      "Trainable params: 17,829,518\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'representative-request', b'payment_extension-request',\n",
       "       b'other-other', b'account-vague', b'billing-vague',\n",
       "       b'appointment-schedule', b'dish_move-query',\n",
       "       b'reactivate-reinstate', b'promotion-query',\n",
       "       b'channel_package-vague', b'tech-issue', b'channel_package-change',\n",
       "       b'account_info-get_help', b'dish_pause-query', b'get-help',\n",
       "       b'appointment-reschedule', b'account-address_change',\n",
       "       b'payment_arrangement-vague', b'internet-query', b'return-query',\n",
       "       b'internet-get_help', b'comp_part_signal_loss-issue',\n",
       "       b'return-get_help', b'equipment-upgrade', b'channel-list',\n",
       "       b'account-transfer', b'blue_black_snowy-issue',\n",
       "       b'online_account-password_reset', b'appointment-tech_get_help',\n",
       "       b'appointment-query', b'equipment-query',\n",
       "       b'channel_package-upgrade', b'remote-setup',\n",
       "       b'payment_schedule-query', b'autopay-setup',\n",
       "       b'channel_price-query', b'promotion-get_help', b'price-increase',\n",
       "       b'equipment-vague', b'online_account-forgot_id_password',\n",
       "       b'renewal-query', b'yes-give', b'account-cancel',\n",
       "       b'redeem_offer-query', b'equipment_setup-get_help',\n",
       "       b'account-name_change', b'payment-get_help',\n",
       "       b'internet_data-query', b'billing-preferences',\n",
       "       b'channel_package-cancel', b'internet-check_availability',\n",
       "       b'channel_package-query', b'contract_expiry-query',\n",
       "       b'referral-how_to', b'internet-price_query', b'dish_pause-cancel',\n",
       "       b'online_account-setup', b'installation-vague', b'receiver-remove',\n",
       "       b'internet-cancel', b'security_code-query',\n",
       "       b'receiver-place_order', b'online_account-forgot_password',\n",
       "       b'receiver_issue-get_help', b'greet-thank_you',\n",
       "       b'appointment-cancel', b'equipment-issue',\n",
       "       b'referral-code_not_received', b'equipment-setup',\n",
       "       b'receiver-replacement', b'billing-query',\n",
       "       b'online_account-login_locked', b'billing-issue',\n",
       "       b'payment-reinstate_service', b'online_account-unable_login',\n",
       "       b'account_cancel-promotion_seek', b'equipment-price_query',\n",
       "       b'joey-vague', b'receiver-get_help', b'remote-place_order',\n",
       "       b'channel_package-issue', b'dvr-get_help', b'app-query',\n",
       "       b'equipment-add_receiver', b'greet-hello', b'payment-vague',\n",
       "       b'bundle-query', b'app-issue', b'remote-vague',\n",
       "       b'autopay-get_help', b'authorization-get_help',\n",
       "       b'equipment-place_order', b'dish_protection-query',\n",
       "       b'payment_extension-already_done', b'autopay-cancel',\n",
       "       b'fee-waive_charge', b'redeem_offer-get_help', b'recording-query',\n",
       "       b'online_account-forgot_login', b'on_demand-query',\n",
       "       b'contract-query', b'order-status', b'channel-guide',\n",
       "       b'receiver-query', b'account_cancel-query', b'price-vague',\n",
       "       b'hopper-issues', b'univision-query', b'receiver-activate',\n",
       "       b'payment-make', b'refund-vague', b'spanish-query',\n",
       "       b'return-status', b'refund-query', b'receiver_upgrade-query',\n",
       "       b'greet-goodbye', b'streaming-issues', b'autopay-query',\n",
       "       b'ppv-query', b'hbocinemax-takedown', b'ppv-issue',\n",
       "       b'movie-place_order', b'recording-issue', b'game_popup-issue',\n",
       "       b'ppv-cancel', b'joey-issues', b'billing-balance_query',\n",
       "       b'refund-get_help', b'order-get_help', b'order-query',\n",
       "       b'dvr-query', b'no-give', b'streaming-query', b'ppv-order',\n",
       "       b'order-cancel', b'contract-cancel_charge', b'remote-not_working',\n",
       "       b'payment-query', b'payment-status', b'account_cancel-fee',\n",
       "       b'remote-order_status', b'movie_order-get_help'], dtype=object)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['classes'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {i:label for i,label in enumerate(intent_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'representative-request'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map.get(131)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's put the loading model part into a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_albert_model_saving(bert_config,input_meta_data,hub_url,checkpoint_path,\n",
    "                            sentencepiece_path,label_file, max_top_intents=2):\n",
    "    \"\"\"This function return our classification model based on bert + updated bert part of model \"\"\"\n",
    "\n",
    "    max_seq_length = input_meta_data['max_seq_length']\n",
    "    num_labels = input_meta_data['num_labels']\n",
    "    default_label = tf.constant(input_meta_data['default_label'])\n",
    "    #alternative way of giving input by specifying the entire text & do the preprocessing in the graph\n",
    "    input_text = tf.keras.Input(shape=(),dtype=tf.string,name='input_text')\n",
    "    sentence_piece_layer = SentencepieceTokenization(model_path=sentencepiece_path,\n",
    "                                                     max_seq_length=tf.constant(max_seq_length))\n",
    "    input_ids_processed,segment_ids_processed,input_mask_processed = sentence_piece_layer(input_text)\n",
    "    \n",
    "    \n",
    "    bert_model = hub.KerasLayer(hub_url,trainable=True,tags=None)\n",
    "    ### pooled_output will give the representation for [CLS]\n",
    "    ### sequence_output will give representations for all tokens\n",
    "    ##since it's classification task we will just use pooled_output\n",
    "    pooled_output, sequence_output = bert_model([input_ids_processed, input_mask_processed, segment_ids_processed])\n",
    "    \n",
    "    # This doesn't really matter as we will be loading the weights from saved checkpoint\n",
    "    initializer = tf.keras.initializers.TruncatedNormal(stddev=bert_config[\"initializer_range\"])\n",
    "    # we don't want dropout for the model we are going to serve so skipping that\n",
    "    dense_output = tf.keras.layers.Dense(num_labels,kernel_initializer=initializer,name='output',dtype=tf.float32)(pooled_output)\n",
    "    # log_probs = tf.nn.softmax()\n",
    "#     log_probs = tf.nn.log_softmax(dense_output, axis=-1)\n",
    "    probs = tf.nn.softmax(dense_output, axis=-1)\n",
    "    top_label_id = tf.argmax(probs,axis=-1)\n",
    "    # sorted_scores = tf.gather(log_probs,sorted_indexes, axis=-1)\n",
    "    sorted_indexes = tf.cast(tf.argsort(probs,axis=-1,direction='DESCENDING'),tf.int64)[:,:max_top_intents]\n",
    "    # sorted_scores = tf.gather(log_probs,sorted_indexes, axis=-1)\n",
    "    sorted_scores1 = tf.sort(probs,axis=-1,direction='DESCENDING',name=\"scores1\")[:,:max_top_intents]\n",
    "    \n",
    "    # this identity operation was only required to get the name correctly as in tf2.3 the sort object's name was coming differently\n",
    "    # https://github.com/tensorflow/tensorflow/issues/39398\n",
    "    # https://stackoverflow.com/questions/38626424/how-to-assign-new-name-or-rename-an-existing-tensor-in-tensorflow\n",
    "    # the identity operation also didn't help\n",
    "    sorted_scores = tf.identity(sorted_scores1,name='scores_sorted')\n",
    "    top_label = LabelLookup(filepath=intent_file,default=default_label)(top_label_id)\n",
    "    labels = LabelLookup(filepath=intent_file,default=default_label,name=\"classes\")(sorted_indexes)\n",
    "    # in the new version the name of the variable seems to be taken rather than the key of the dictionary\n",
    "    model  = tf.keras.Model(inputs={'input_text':input_text},\n",
    "                         outputs={\"classes\":labels,\"scores\":sorted_scores})\n",
    "    # FIXME the following is a hack (not sure if it will create other issues)\n",
    "    model.output_names = ['classes', 'scores']\n",
    "\n",
    "#     checkpoint = tf.train.Checkpoint(model=model)\n",
    "#     checkpoint.restore(checkpoint_path).assert_existing_objects_matched()\n",
    "    model.load_weights(checkpoint_path).assert_existing_objects_matched()\n",
    "    return model, bert_model, input_text, labels, sorted_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tensorflow_hub.keras_layer.KerasLayer object at 0x7f7ecb941910> and <tensorflow.python.keras.layers.core.Dense object at 0x7f7d6be3ed30>).\n",
      "Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tensorflow_hub.keras_layer.KerasLayer object at 0x7f7ecb941910> and <tensorflow.python.keras.layers.core.Dense object at 0x7f7d6be3ed30>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tensorflow.python.keras.layers.core.Dense object at 0x7f7d6be3ed30> and <tensorflow.python.keras.engine.base_layer.TensorFlowOpLayer object at 0x7f7d6a1eba00>).\n",
      "Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tensorflow.python.keras.layers.core.Dense object at 0x7f7d6be3ed30> and <tensorflow.python.keras.engine.base_layer.TensorFlowOpLayer object at 0x7f7d6a1eba00>).\n"
     ]
    }
   ],
   "source": [
    "classification_model_to_save, bert_updated,input_text,labels, sorted_log_probs = get_albert_model_saving(bert_config,\n",
    "                                                                     input_meta_data,\n",
    "                                                                     bert_hub_url,\n",
    "                                                                     checkpoint_path=tf.train.latest_checkpoint(output_folder),\n",
    "                                                                    sentencepiece_path=sentencepiece_path,\n",
    "                                                                     label_file=intent_file\n",
    "                                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_text (InputLayer)         [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sentencepiece_tokenization (Sen ((None, 128), (None, 0           input_text[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        [(None, 1024), (None 17683968    sentencepiece_tokenization[0][0] \n",
      "                                                                 sentencepiece_tokenization[0][2] \n",
      "                                                                 sentencepiece_tokenization[0][1] \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 142)          145550      keras_layer[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softmax (TensorFlow [(None, 142)]        0           output[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape (TensorFlowOp [(2,)]               0           tf_op_layer_Softmax[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [()]                 0           tf_op_layer_Shape[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_1 (TensorFlow [(2,)]               0           tf_op_layer_Softmax[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_TopKV2 (TensorFlowO [(None, None), (None 0           tf_op_layer_Softmax[0][0]        \n",
      "                                                                 tf_op_layer_strided_slice[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_2 (Te [()]                 0           tf_op_layer_Shape_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Cast (TensorFlowOpL [(None, None)]       0           tf_op_layer_TopKV2[0][1]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_TopKV2_1 (TensorFlo [(None, None), (None 0           tf_op_layer_Softmax[0][0]        \n",
      "                                                                 tf_op_layer_strided_slice_2[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_1 (Te [(None, None)]       0           tf_op_layer_Cast[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_3 (Te [(None, None)]       0           tf_op_layer_TopKV2_1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "classes (LabelLookup)           (None, None)         0           tf_op_layer_strided_slice_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_scores_sorted (Tens [(None, None)]       0           tf_op_layer_strided_slice_3[0][0]\n",
      "==================================================================================================\n",
      "Total params: 17,829,518\n",
      "Trainable params: 17,829,518\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classification_model_to_save.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classes': array([[b'other-other', b'payment-status'],\n",
       "        [b'greet-hello', b'yes-give'],\n",
       "        [b'representative-request', b'other-other'],\n",
       "        [b'remote-setup', b'remote-not_working']], dtype=object),\n",
       " 'scores': array([[0.12820116, 0.04802238],\n",
       "        [0.8430833 , 0.02728763],\n",
       "        [0.9865248 , 0.00239532],\n",
       "        [0.99100107, 0.00308522]], dtype=float32)}"
      ]
     },
     "execution_count": 521,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = classification_model_to_save.predict(['','hello','talk to agent', 'remote not working'])\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'other-other', b'greet-hello', b'representative-request',\n",
       "       b'remote-setup'], dtype=object)"
      ]
     },
     "execution_count": 522,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['classes'][:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /space/engineering/tf_serve/models/dish_albert/4/assets\n",
      "Assets written to: /space/engineering/tf_serve/models/dish_albert/4/assets\n"
     ]
    }
   ],
   "source": [
    "# final_model_path=os.path.join(output_folder,'final_model')\n",
    "final_model_path='/space/engineering/tf_serve/models/dish_albert'\n",
    "final_versioned_model_path = os.path.join(final_model_path,'4')\n",
    "classification_model_to_save.save(final_versioned_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading the saved model with preprocessing & trying it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_final_model = tf.saved_model.load(final_versioned_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ConcreteFunction signature_wrapper(input_text) at 0x7F7D6172E1C0>"
      ]
     },
     "execution_count": 531,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serving_default = loaded_final_model.signatures['serving_default']\n",
    "serving_default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'scores': <tf.Tensor: shape=(4, 2), dtype=float32, numpy=\n",
       " array([[0.12820116, 0.04802238],\n",
       "        [0.8430833 , 0.02728763],\n",
       "        [0.9865248 , 0.00239532],\n",
       "        [0.99100107, 0.00308522]], dtype=float32)>,\n",
       " 'classes': <tf.Tensor: shape=(4, 2), dtype=string, numpy=\n",
       " array([[b'other-other', b'payment-status'],\n",
       "        [b'greet-hello', b'yes-give'],\n",
       "        [b'representative-request', b'other-other'],\n",
       "        [b'remote-setup', b'remote-not_working']], dtype=object)>}"
      ]
     },
     "execution_count": 534,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serving_default(tf.constant(['','hello','talk to agent', 'remote not working']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### running against a held out test set\n",
    "ideally we need a held out test set, but using the validation set again just to have the testing code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_val.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>text</th>\n",
       "      <th>granular_intent</th>\n",
       "      <th>ru_intent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20421</th>\n",
       "      <td>a9cf6bee-3971-4516-459f-2243d9a606fd</td>\n",
       "      <td>I want to inform you that I don't have electri...</td>\n",
       "      <td>billing-issue</td>\n",
       "      <td>billing-issue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22946</th>\n",
       "      <td>0f050cf9-88d7-44bd-23c8-5f26674f5b6a</td>\n",
       "      <td>What happen to my bill</td>\n",
       "      <td>billing-query</td>\n",
       "      <td>billing-query</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8207</th>\n",
       "      <td>INT-sv1appis11-1503849377195-243917</td>\n",
       "      <td>I need to replace my remote control.</td>\n",
       "      <td>remote-place_order</td>\n",
       "      <td>remote-place_order</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14144</th>\n",
       "      <td>INT-sv1appis14-1504137880316-305603_1158</td>\n",
       "      <td>Reciever is saying not authorized on every cha...</td>\n",
       "      <td>receiver_issue-get_help</td>\n",
       "      <td>receiver_issue-get_help</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8164</th>\n",
       "      <td>6e1842f4-63a1-4259-3999-6928033ce7eb</td>\n",
       "      <td>i need to update my credit card info</td>\n",
       "      <td>payment-get_help</td>\n",
       "      <td>payment-get_help</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       filename  \\\n",
       "20421      a9cf6bee-3971-4516-459f-2243d9a606fd   \n",
       "22946      0f050cf9-88d7-44bd-23c8-5f26674f5b6a   \n",
       "8207        INT-sv1appis11-1503849377195-243917   \n",
       "14144  INT-sv1appis14-1504137880316-305603_1158   \n",
       "8164       6e1842f4-63a1-4259-3999-6928033ce7eb   \n",
       "\n",
       "                                                    text  \\\n",
       "20421  I want to inform you that I don't have electri...   \n",
       "22946                             What happen to my bill   \n",
       "8207                I need to replace my remote control.   \n",
       "14144  Reciever is saying not authorized on every cha...   \n",
       "8164                i need to update my credit card info   \n",
       "\n",
       "               granular_intent                ru_intent  \n",
       "20421            billing-issue            billing-issue  \n",
       "22946            billing-query            billing-query  \n",
       "8207        remote-place_order       remote-place_order  \n",
       "14144  receiver_issue-get_help  receiver_issue-get_help  \n",
       "8164          payment-get_help         payment-get_help  "
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classes': array([[b'billing-issue', b'price-increase', b'billing-vague', ...,\n",
       "         b'app-query', b'dvr-query', b'receiver_upgrade-query'],\n",
       "        [b'billing-issue', b'billing-vague', b'billing-query', ...,\n",
       "         b'ppv-cancel', b'order-query', b'receiver_upgrade-query'],\n",
       "        [b'remote-place_order', b'remote-order_status',\n",
       "         b'remote-not_working', ..., b'get-help', b'billing-vague',\n",
       "         b'other-other'],\n",
       "        ...,\n",
       "        [b'channel_package-issue', b'streaming-issues',\n",
       "         b'comp_part_signal_loss-issue', ..., b'order-cancel',\n",
       "         b'internet-price_query', b'appointment-cancel'],\n",
       "        [b'price-increase', b'billing-issue',\n",
       "         b'account_cancel-promotion_seek', ..., b'online_account-setup',\n",
       "         b'dish_pause-cancel', b'billing-preferences'],\n",
       "        [b'greet-hello', b'yes-give', b'no-give', ..., b'promotion-query',\n",
       "         b'billing-preferences', b'account-address_change']], dtype=object),\n",
       " 'scores': array([[9.90572035e-01, 1.36903033e-03, 7.22032622e-04, ...,\n",
       "         4.48613173e-06, 3.90728383e-06, 3.59540400e-06],\n",
       "        [9.83889103e-01, 3.92688625e-03, 1.98980793e-03, ...,\n",
       "         5.12303404e-06, 5.08199355e-06, 4.91710898e-06],\n",
       "        [9.39636528e-01, 2.66878530e-02, 5.72897727e-03, ...,\n",
       "         1.40353868e-05, 1.27390758e-05, 1.08726945e-05],\n",
       "        ...,\n",
       "        [9.89189625e-01, 1.56994630e-03, 1.04121200e-03, ...,\n",
       "         3.62604578e-06, 2.82794531e-06, 1.43056025e-06],\n",
       "        [9.91882026e-01, 1.29181775e-03, 6.47934037e-04, ...,\n",
       "         5.17458193e-06, 4.91146602e-06, 4.39468567e-06],\n",
       "        [5.50903201e-01, 1.80327654e-01, 8.84241089e-02, ...,\n",
       "         1.65124256e-05, 1.24546677e-05, 9.99046279e-06]], dtype=float32)}"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = classification_model_to_save.predict(df_val['text'].array[:20].to_numpy())\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicted_intent_per_row(row, classification_model, text_column='text',out_of_domain_intent='none_none'):\n",
    "    text = row['text']\n",
    "    try:\n",
    "        output = classification_model.predict([text])\n",
    "        # 1st index is for batch & batch size is 1, 2nd is the list of intents, taking the top intent\n",
    "        row['predicted_intent'] = output['classes'][0][0]\n",
    "        row['score'] = output['scores'][0][0]\n",
    "    except Exception as e:\n",
    "        print(f'exception occured, text {text} classifying as out of domain intent error {e}')\n",
    "        row['predicted_intent']  = out_of_domain_intent\n",
    "        row['score'] = 1\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of test data 2572\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>text</th>\n",
       "      <th>granular_intent</th>\n",
       "      <th>ru_intent</th>\n",
       "      <th>predicted_intent</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20421</th>\n",
       "      <td>a9cf6bee-3971-4516-459f-2243d9a606fd</td>\n",
       "      <td>I want to inform you that I don't have electri...</td>\n",
       "      <td>billing-issue</td>\n",
       "      <td>billing-issue</td>\n",
       "      <td>b'billing-issue'</td>\n",
       "      <td>0.990572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22946</th>\n",
       "      <td>0f050cf9-88d7-44bd-23c8-5f26674f5b6a</td>\n",
       "      <td>What happen to my bill</td>\n",
       "      <td>billing-query</td>\n",
       "      <td>billing-query</td>\n",
       "      <td>b'billing-issue'</td>\n",
       "      <td>0.983889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8207</th>\n",
       "      <td>INT-sv1appis11-1503849377195-243917</td>\n",
       "      <td>I need to replace my remote control.</td>\n",
       "      <td>remote-place_order</td>\n",
       "      <td>remote-place_order</td>\n",
       "      <td>b'remote-place_order'</td>\n",
       "      <td>0.939638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14144</th>\n",
       "      <td>INT-sv1appis14-1504137880316-305603_1158</td>\n",
       "      <td>Reciever is saying not authorized on every cha...</td>\n",
       "      <td>receiver_issue-get_help</td>\n",
       "      <td>receiver_issue-get_help</td>\n",
       "      <td>b'receiver_issue-get_help'</td>\n",
       "      <td>0.598178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8164</th>\n",
       "      <td>6e1842f4-63a1-4259-3999-6928033ce7eb</td>\n",
       "      <td>i need to update my credit card info</td>\n",
       "      <td>payment-get_help</td>\n",
       "      <td>payment-get_help</td>\n",
       "      <td>b'payment-get_help'</td>\n",
       "      <td>0.947868</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       filename  \\\n",
       "20421      a9cf6bee-3971-4516-459f-2243d9a606fd   \n",
       "22946      0f050cf9-88d7-44bd-23c8-5f26674f5b6a   \n",
       "8207        INT-sv1appis11-1503849377195-243917   \n",
       "14144  INT-sv1appis14-1504137880316-305603_1158   \n",
       "8164       6e1842f4-63a1-4259-3999-6928033ce7eb   \n",
       "\n",
       "                                                    text  \\\n",
       "20421  I want to inform you that I don't have electri...   \n",
       "22946                             What happen to my bill   \n",
       "8207                I need to replace my remote control.   \n",
       "14144  Reciever is saying not authorized on every cha...   \n",
       "8164                i need to update my credit card info   \n",
       "\n",
       "               granular_intent                ru_intent  \\\n",
       "20421            billing-issue            billing-issue   \n",
       "22946            billing-query            billing-query   \n",
       "8207        remote-place_order       remote-place_order   \n",
       "14144  receiver_issue-get_help  receiver_issue-get_help   \n",
       "8164          payment-get_help         payment-get_help   \n",
       "\n",
       "                 predicted_intent     score  \n",
       "20421            b'billing-issue'  0.990572  \n",
       "22946            b'billing-issue'  0.983889  \n",
       "8207        b'remote-place_order'  0.939638  \n",
       "14144  b'receiver_issue-get_help'  0.598178  \n",
       "8164          b'payment-get_help'  0.947868  "
      ]
     },
     "execution_count": 537,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'size of test data {len(df_test)}')\n",
    "df_test = df_test.apply(lambda row:get_predicted_intent_per_row(row,classification_model_to_save,text_column='text'),axis=1)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "load_final_model_keras = tf.keras.models.load_model(final_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((),\n",
       " {'input_text': TensorSpec(shape=(None,), dtype=tf.string, name='input_text')})"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serving_default.structured_input_signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label_lookup': TensorSpec(shape=(None,), dtype=tf.string, name='label_lookup')}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serving_default.structured_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label_lookup': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'other-other'], dtype=object)>}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serving_default(tf.constant(['talk to an agent']))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%tensorboard --logdir /var/extra/users/jgeorge/tf2.0/input/dish/models/albert_en_large/checkpoint/"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def evaluate(eval_input_fn,model):\n",
    "    eval_iter = iter(strategy.experimental_distribute_datasets_from_function(eval_input_fn))\n",
    "    \n",
    "    def _test_step_fn(inputs,label):\n",
    "#         inputs,label = inputs\n",
    "        model_outputs = model(inputs,training=False)\n",
    "        metric.update_state(label,model_outputs)\n",
    "    strategy.experimental_run_v2(_test_step_fn,args=(next(eval_iter)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "evaluate(eval_input_fn,trained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - tensorflow 2.3 (tf2.3)",
   "language": "python",
   "name": "tf2.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
