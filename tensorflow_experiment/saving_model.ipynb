{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-699ec556dbda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_hub\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhub\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_text\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtftext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_text'"
     ]
    }
   ],
   "source": [
    "# import tf_sentencepiece as tfs\n",
    "# import sentencepiece as spm\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as tftext\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file= '/space/engineering/pretrained_models/bert/multi_cased_L-12_H-768_A-12/vocab.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:7', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "##restricting no of gpus\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "device_to_use = gpus[7]\n",
    "tf.config.experimental.set_memory_growth(device_to_use,True)\n",
    "tf.config.experimental.set_visible_devices(device_to_use, 'GPU')\n",
    "print(tf.config.get_visible_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BertTokenizationCustom(tf.keras.layers.Layer):\n",
    "class BertTokenizationCustom(object):\n",
    "    def __init__(self,vocab_path):\n",
    "#         super(BertTokenizationCustom, self).__init__(trainable=False,dtype=tf.int32)\n",
    "        \n",
    "        self.missing_index=-1\n",
    "        #self.vocab_path = vocab_path\n",
    "        self.vocab_table = tf.lookup.StaticHashTable(\n",
    "            tf.lookup.TextFileInitializer(vocab_path,\n",
    "                                          key_dtype=tf.string,key_index=tf.lookup.TextFileIndex.WHOLE_LINE,\n",
    "                                     value_dtype=tf.int64,value_index=tf.lookup.TextFileIndex.LINE_NUMBER),default_value=self.missing_index)\n",
    "        self.max_seq_length = 10\n",
    "        \n",
    "    def build(self,input_shape):\n",
    "        #self.model_proto = tf.io.gfile.GFile(self.model_path, 'rb').read()\n",
    "        ## pad_id is usually 0 \n",
    "#         [self.CLS_ID,self.SEP_ID,self.PAD_ID]  = tfs.piece_to_id(['[CLS]','[SEP]','[PAD]'],model_file=self.model_path)\n",
    "        self.built=True\n",
    "        \n",
    "#     @tf.function    \n",
    "    def call(self, input_text):\n",
    "        ##tensorflow sentence piece works while exporting to graph while, tf_text sentencepiece doesn't\n",
    "        encoded_text = self.process_sentences(input_text)\n",
    "        return encoded_text\n",
    "    \n",
    "    def is_vocab_missing(self,vocab_lookup):\n",
    "        return vocab_lookup==self.missing_index\n",
    "    \n",
    "#     @tf.function\n",
    "    def get_chars_len(self,chars):\n",
    "        chars_len = tf.shape(chars)[0]\n",
    "        # when it is ragged tensor\n",
    "    #     chars_len = chars.nrows()\n",
    "        return chars_len\n",
    "\n",
    "    def inner_cond(self,start,end,word,chars,vocab_lookup):\n",
    "        cond = tf.less(start,end) and self.is_vocab_missing(vocab_lookup)\n",
    "#         tf.print(f'vocab lookup inside inner_cond  start {start} end {end} condition {vocab_lookup} is_vocab_missing {is_vocab_missing(vocab_lookup)} & entire cond {cond}')\n",
    "        return cond\n",
    "    \n",
    "    # def inner_while_body(start,end,word,vocab_lookup):\n",
    "    def inner_while_body(self,start,end,word,chars,vocab_lookup):\n",
    "#         tf.print(f'inner_while_body start {start} end {end} word {word} chars {chars}\\n')\n",
    "    #     chars = tf.strings.bytes_split(word)\n",
    "        substring = tf.strings.join(tf.slice(chars,begin=start,size=end-start))\n",
    "    #     tf.strings.reduce_join()\n",
    "        if start>0:\n",
    "            substring = tf.strings.join([\"##\",substring])\n",
    "        # assigning it rather than creating a new variable so that this is accessible to other\n",
    "        # function referencing this variable\n",
    "        self.vocab_lookup.assign(self.vocab_table.lookup(tf.constant(substring)))\n",
    "#         tf.print(f'substring {substring} vocab lookup {vocab_lookup}')\n",
    "        if self.vocab_lookup==self.missing_index:\n",
    "    #         print(f'missing {substring}')\n",
    "#             tf.print(f'missing {substring}')\n",
    "            # subtracting only if missing\n",
    "            end.assign_sub([1])\n",
    "#         else:\n",
    "#             tf.print(f'present {substring}')\n",
    "\n",
    "        return (start,end,substring,chars,vocab_lookup)\n",
    "    \n",
    "    def outer_cond(self,start,end,word,chars,is_bad,sub_tokens):\n",
    "#         tf.print(f'outer_cond start {start} end {end}  chars {chars} is_bad {is_bad} sub_tokens {sub_tokens}')\n",
    "        return tf.math.logical_and(start < tf.shape(chars)[0],tf.math.logical_not(is_bad))\n",
    "\n",
    "#     @tf.function\n",
    "    def outer_while_loop(self,start,end,word,chars,is_bad,sub_tokens):\n",
    "#         tf.print(f'outer_while_loop start {start} end {end}  chars {chars} is_bad {is_bad} sub_tokens {sub_tokens}')\n",
    "        # vocab_lookup = vocab_table.lookup(tf.constant(substring))\n",
    "#         chars_len = self.get_chars_len(chars)\n",
    "        chars_len = tf.shape(chars)[0]\n",
    "        # end = tf.Variable([chars_len])\n",
    "        end = tf.Variable([0])\n",
    "        end.assign([chars_len])\n",
    "        print(f'end {end} start {start}')\n",
    "        size = end-start\n",
    "        sliced_text_tensor = tf.slice(chars,begin=start,size=size)\n",
    "        num_or_size_splits = tf.shape(sliced_text_tensor)[0]\n",
    "        print(f'sliced_text {sliced_text_tensor} shape is {tf.shape(sliced_text_tensor)} num_or_size_splits {num_or_size_splits}')\n",
    "        \n",
    "        #sliced_text_list = tf.split(sliced_text_tensor, num_or_size_splits = num_or_size_splits, axis = 0)\n",
    "        # TODO remove\n",
    "        sliced_text_list = sliced_text_tensor\n",
    "        substring = tf.strings.join(sliced_text_list)\n",
    "        vocab_lookup = tf.Variable(self.missing_index,shape=(),name='vocab_lookup',dtype=tf.int64)\n",
    "    #     vocab_lookup.assign(vocab_table.lookup(tf.constant(substring)))\n",
    "\n",
    "        (start_1,end_1,substring_1,chars_1,vocab_lookup) = tf.while_loop(self.inner_cond,self.inner_while_body,loop_vars=[start,end,substring,chars,vocab_lookup])\n",
    "#         tf.print(f'****end of inner while start {start_1} end {end_1} substring {substring_1} chars {chars_1} vocab_lookup {vocab_lookup} \\n\\n')\n",
    "        if not is_vocab_missing(vocab_lookup):\n",
    "            sub_tokens_concat = tf.concat([sub_tokens,[substring_1]],axis=0)\n",
    "#             tf.print(\"sub tokens concat \",sub_tokens_concat, 'shape ',tf.shape(sub_tokens_concat))\n",
    "    #         sub_tokens.append(substring)\n",
    "#             tf.print('subtokens ',sub_tokens, ' sub tokens shape ',sub_tokens)\n",
    "            sub_tokens.assign(sub_tokens_concat)\n",
    "#             tf.print(f'subtokens appended ')\n",
    "        else:\n",
    "            # if vocab missing even after the inner loop then this word can't be formed using\n",
    "            # word existing in the dictionary\n",
    "            is_bad.assign(True)\n",
    "#         tf.print(f'start before assigning end {start} & end val {end}')\n",
    "        start.assign(end_1)\n",
    "#         tf.print(f'current subtokens {sub_tokens}')\n",
    "        return (start,end,word,chars,is_bad,sub_tokens)\n",
    "    \n",
    "    @tf.function\n",
    "    def per_word_tokenize(self,word):\n",
    "        print(f'word {word}')\n",
    "#         word = tf.squeeze(word)\n",
    "        chars = tf.strings.bytes_split(word)#.to_tensor()\n",
    "#         tf.print(f'per_word_tokenize word  {word} shape {tf.shape(word)} chars {chars}')\n",
    "        chars_len = tf.shape(chars)[0]\n",
    "#         chars_len = self.get_chars_len(chars)\n",
    "    #     start = tf.zeros((1,),dtype=tf.int32)\n",
    "        start = tf.Variable([0],dtype=tf.int32)\n",
    "#         end = tf.Variable([chars_len])\n",
    "        # directly initializing with a tensor 'chars_len' didn't work, TODO figure out\n",
    "        end = tf.Variable([0])\n",
    "        end.assign([chars_len])\n",
    "\n",
    "        is_bad = tf.Variable(False)\n",
    "        sub_tokens = tf.Variable([],shape=(None,),dtype=tf.string)\n",
    "\n",
    "        (start,end,word,chars,is_bad,sub_tokens_1) =  tf.while_loop(self.outer_cond,self.outer_while_loop,loop_vars=[start,end,word,chars,is_bad,sub_tokens])\n",
    "        sub_tokens.assign(sub_tokens_1)\n",
    "    #     sub_tokens_out = tf.expand_dims(sub_tokens,axis=0)\n",
    "        return sub_tokens\n",
    "    def joined_per_word_tokenized(self,word):\n",
    "        sub_tokens = self.per_word_tokenize(word)\n",
    "        return tf.strings.reduce_join(sub_tokens,separator=' ')\n",
    "    \n",
    "    def process_sentences(self,sentences):\n",
    "        # words is a ragged tensor of words - [batch_size,no_of_words]\n",
    "        words = tf.strings.split(sentences)\n",
    "        print(f'words in process sentence {words}')\n",
    "    #     list_words = tf.expand_dims(words,axis=1)\n",
    "    #     tf.print(f'words in process sentence {list_words} shape {tf.shape(list_words)}')\n",
    "        subword_tokenized_list = tf.map_fn(self.joined_per_word_tokenized,words,parallel_iterations=None,infer_shape=False)\n",
    "        \n",
    "        sentence_tokenized = tf.strings.reduce_join(subword_tokenized_list,separator=' ')\n",
    "        processed_tokens = tf.strings.split(sentence_tokenized)\n",
    "#         tf.print(f'processed tokens {processed_tokens}')\n",
    "        sequence_length = tf.shape(processed_tokens)[-1]\n",
    "        trimmed_max_length = max_sequence_length-2\n",
    "        values_trimmed = tf.cond(tf.greater(sequence_length,trimmed_max_length), \n",
    "                                        lambda : tf.slice(processed_tokens,begin=[0],size=[trimmed_max_length],name='trimmed_out'),lambda : processed_tokens)\n",
    "        concat = tf.concat([['[CLS]'], values_trimmed, ['[SEP]']],axis=0,name='concat_out')\n",
    "        actual_token_length = tf.shape(concat)[-1]\n",
    "        token_ids = self.vocab_table.lookup(concat)\n",
    "        padded = tf.pad(token_ids,paddings=[[0,max_sequence_length-actual_token_length]],name='input_ids')\n",
    "        return padded\n",
    "    \n",
    "#     def process_sentences(self,sentences):\n",
    "# #         sentences_tokenized = tf.map_fn(self.process_sentence,sentences)\n",
    "#         sentences_tokenized = process_sentence(sentences)\n",
    "#         return sentences_tokenized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_text = tf.keras.Input(shape=(),dtype=tf.string,name='input_text')\n",
    "# tokenized_out = BertTokenizationCustom(vocab_path=vocab_file)(input_text)\n",
    "tokenized_out = BertTokenizationCustom(vocab_path=vocab_file)\n",
    "# model_lookup = tf.keras.Model(inputs={'input_text':input_text},outputs=tokenized_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'process_sentence' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-788a352b532e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenized_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'talk to an agent'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'talk to a representative'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-c616b679066a>\u001b[0m in \u001b[0;36mprocess_sentences\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprocess_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;31m#         sentences_tokenized = tf.map_fn(self.process_sentence,sentences)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0msentences_tokenized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msentences_tokenized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'process_sentence' is not defined"
     ]
    }
   ],
   "source": [
    "tokenized_out.process_sentences(tf.constant(['talk to an agent','talk to a representative']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_string = tf.constant([\"hi this is jithin\",\"talk to\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'hi', b'this', b'is', b'jithin'], [b'talk', b'to']]>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.split(temp_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.gather()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tftext.gather_with_default()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.strided_slice()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_string = tf.constant(\"hi this is\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=string, numpy=array([b'hi', b'this', b'is'], dtype=object)>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.split(temp_string,sep=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(tf.random.uniform([5, 30], -1, 1))\n",
    "\n",
    "# Split `x` into 3 tensors along dimension 1\n",
    "s0, s1, s2 = tf.split(x, num_or_size_splits=3, axis=1)\n",
    "splitted_out = tf.split(x, num_or_size_splits=3, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([5, 10])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.training.tracking import util as trackable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.training.tracking import tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracking.AutoTrackable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.python.training.tracking.util' has no attribute 'TrackableResource'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-46d29954a6c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrackable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrackableResource\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow.python.training.tracking.util' has no attribute 'TrackableResource'"
     ]
    }
   ],
   "source": [
    "trackable.TrackableResource()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensorflow.python.keras.engine.training.Model,\n",
       " tensorflow.python.keras.engine.network.Network,\n",
       " tensorflow.python.keras.engine.base_layer.Layer,\n",
       " tensorflow.python.module.module.Module,\n",
       " tensorflow.python.training.tracking.tracking.AutoTrackable,\n",
       " tensorflow.python.training.tracking.base.Trackable,\n",
       " tensorflow.python.keras.utils.version_utils.LayerVersionSelector,\n",
       " tensorflow.python.keras.utils.version_utils.ModelVersionSelector,\n",
       " object]"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.Model.mro()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "multilingual_vocab_path = \"/space/engineering/pretrained_models/bert/multi_cased_L-12_H-768_A-12/vocab.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spm.SentencePieceProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hub.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.training.tracking.base.Trackable"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.python.trackable.Trackable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.training.tracking.base.TrackableReference"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.python.trackable.TrackableReference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.saved_model.Asset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:4', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:5', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:6', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:7', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.experimental.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.experimental.list_physical_devices('CPU')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tf.config.list_logical_devices('CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU')]\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n"
     ]
    }
   ],
   "source": [
    "##restricting no of gpus\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "cpus = tf.config.experimental.list_physical_devices('CPU')\n",
    "device_to_use = gpus[2]\n",
    "# device_to_use = []\n",
    "\n",
    "tf.config.experimental.set_memory_growth(device_to_use,True)\n",
    "tf.config.experimental.set_visible_devices(device_to_use, 'GPU')\n",
    "print(tf.config.experimental.get_visible_devices('GPU'))\n",
    "print(tf.config.experimental.get_visible_devices('CPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TFHUB_CACHE_DIR\"] = '/space/engineering/tfhub_modules'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model_path='/space/engineering/tf_serve/models/tryout_jithin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dir='/var/extra/users/jgeorge/tf2.0/input/albert_base'\n",
    "sp_model_path = os.path.join(model_dir, \"assets\", \"30k-clean.model\")\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(sp_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_hub_url='https://tfhub.dev/tensorflow/albert_en_large/1'\n",
    "bert_hub_loaded = hub.load(bert_hub_url)\n",
    "sentencepiece_path = bert_hub_loaded.sp_model_file.asset_path.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'/space/engineering/tfhub_modules/d0ceaf43f67b8744561ebeeaea4c7c188a6e6f78/assets/30k-clean.model'>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_hub_loaded.sp_model_file.asset_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = hub.KerasLayer(bert_hub_url,trainable=True)\n",
    "sentencepiece_path = bert_model.resolved_object.sp_model_file.asset_path.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_proto = tf.io.gfile.GFile(sp_model_path, 'rb').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[48, 25, 32]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.encode_as_ids(\"this is it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentencepieceEncodeDense(values=<tf.Tensor: shape=(1, 3), dtype=int32, numpy=array([[48, 25, 32]], dtype=int32)>, sequence_length=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([3], dtype=int32)>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text1 = tfs.encode(['this is it'],model_file=sp_model_path)\n",
    "encoded_text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentencepieceEncodeDense(values=<tf.Tensor: shape=(1, 3), dtype=int32, numpy=array([[48, 25, 32]], dtype=int32)>, sequence_length=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([3], dtype=int32)>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfs.encode(['this is it'],model_proto=model_proto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clear the existing tensorflow graph\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i+1 for i in (range(5))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version  2.2.0\n"
     ]
    }
   ],
   "source": [
    "print('tensorflow version ',tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "##restricting no of gpus\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "device_to_use = gpus[3]\n",
    "tf.config.experimental.set_memory_growth(device_to_use,True)\n",
    "tf.config.experimental.set_visible_devices(device_to_use, 'GPU')\n",
    "print(tf.config.experimental.get_visible_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_path='/var/extra/users/jgeorge/tf2.0/input/dish/data/jan17_2020/vocab_test.txt'\n",
    "\n",
    "model_dir_lookup='/var/extra/users/jgeorge/tf2.0/input/dish/data/jan17_2020/model/lookup'\n",
    "model_dir_lookup2 = model_dir_lookup+'_2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/tensorflow/tensorflow/issues/38305\n",
    "class VocabLookup(tf.keras.layers.Layer):\n",
    "    def __init__(self,vocab_path):\n",
    "        super(VocabLookup, self).__init__(trainable=False,dtype=tf.int32)\n",
    "        self.vocab_path = vocab_path\n",
    "    def build(self,input_shape):\n",
    "        table_init = tf.lookup.TextFileInitializer(self.vocab_path,tf.string,tf.lookup.TextFileIndex.WHOLE_LINE,\n",
    "                              tf.int64,tf.lookup.TextFileIndex.LINE_NUMBER)\n",
    "        self.table = tf.lookup.StaticHashTable(table_init,-1)\n",
    "        self.built=True\n",
    "        \n",
    "    def call(self, input_text):\n",
    "        splitted_text = tf.strings.split(input_text).to_tensor()\n",
    "        word_ids = self.table.lookup(splitted_text)\n",
    "        return word_ids\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(VocabLookup, self).get_config()\n",
    "        config.update({'vocab_path': self.vocab_path})\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VocabLookup2(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(VocabLookup2, self).__init__(trainable=False,dtype=tf.int32)\n",
    "    def build(self,input_shape):\n",
    "        self.keys=['hi','testing','lookup','in','tf']\n",
    "        ##keeping values to start from 1 instead of zero to be consistent with the file based approach\n",
    "        values=range(1,len(self.keys)+1)\n",
    "        table_init = tf.lookup.KeyValueTensorInitializer(keys=self.keys,values=values)\n",
    "        self.table = tf.lookup.StaticHashTable(table_init,-1)\n",
    "        self.built=True\n",
    "        \n",
    "    def call(self, input_text):\n",
    "        splitted_text = tf.strings.split(input_text).to_tensor()\n",
    "        word_ids = self.table.lookup(splitted_text)\n",
    "        return word_ids\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(VocabLookup2, self).get_config()\n",
    "        config.update({'keys': self.keys})\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VocabLookup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.saved_model.SaveOptions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = tf.keras.Input(shape=(),dtype=tf.string,name='input_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lookup_out = VocabLookup(vocab_path=vocab_path)(input_text)\n",
    "lookup_out2 = VocabLookup2()(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_lookup = tf.keras.Model(inputs={'input_text':input_text},outputs=lookup_out)\n",
    "model_lookup2 = tf.keras.Model(inputs={'input_text':input_text},outputs=lookup_out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words in process sentence [b'talk' b'to' b'an' b'aagent']\n",
      "per_word_tokenize word  b'talk' shape [] chars [b't' b'a' b'l' b'k']\n",
      "outer_cond start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([4], dtype=int32)>  chars [b't' b'a' b'l' b'k'] is_bad <tf.Variable 'Variable:0' shape=() dtype=bool, numpy=False> sub_tokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([], dtype=object)>\n",
      "outer_while_loop start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([4], dtype=int32)>  chars [b't' b'a' b'l' b'k'] is_bad <tf.Variable 'Variable:0' shape=() dtype=bool, numpy=False> sub_tokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([], dtype=object)>\n",
      "vocab lookup inside inner_cond  start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([4], dtype=int32)> condition <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=-1> is_vocab_missing True & entire cond True\n",
      "inner_while_body start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([4], dtype=int32)> word b'talk' chars [b't' b'a' b'l' b'k']\n",
      "\n",
      "substring b'talk' vocab lookup <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=31311>\n",
      "present b'talk'\n",
      "vocab lookup inside inner_cond  start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([4], dtype=int32)> condition <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=31311> is_vocab_missing False & entire cond False\n",
      "****end of inner while start [0] end [4] substring b'talk' chars [b't' b'a' b'l' b'k'] vocab_lookup 31311 \n",
      "\n",
      "\n",
      "sub tokens concat  [\"talk\"] shape  [1]\n",
      "subtokens  []  sub tokens shape  []\n",
      "subtokens appended \n",
      "start before assigning end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> & end val <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([4], dtype=int32)>\n",
      "current subtokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([b'talk'], dtype=object)>\n",
      "outer_cond start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([4], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([4], dtype=int32)>  chars [b't' b'a' b'l' b'k'] is_bad <tf.Variable 'Variable:0' shape=() dtype=bool, numpy=False> sub_tokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([b'talk'], dtype=object)>\n",
      "per_word_tokenize word  b'to' shape [] chars [b't' b'o']\n",
      "outer_cond start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)>  chars [b't' b'o'] is_bad <tf.Variable 'Variable:0' shape=() dtype=bool, numpy=False> sub_tokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([], dtype=object)>\n",
      "outer_while_loop start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)>  chars [b't' b'o'] is_bad <tf.Variable 'Variable:0' shape=() dtype=bool, numpy=False> sub_tokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([], dtype=object)>\n",
      "vocab lookup inside inner_cond  start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)> condition <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=-1> is_vocab_missing True & entire cond True\n",
      "inner_while_body start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)> word b'to' chars [b't' b'o']\n",
      "\n",
      "substring b'to' vocab lookup <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=10114>\n",
      "present b'to'\n",
      "vocab lookup inside inner_cond  start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)> condition <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=10114> is_vocab_missing False & entire cond False\n",
      "****end of inner while start [0] end [2] substring b'to' chars [b't' b'o'] vocab_lookup 10114 \n",
      "\n",
      "\n",
      "sub tokens concat  [\"to\"] shape  [1]\n",
      "subtokens  []  sub tokens shape  []\n",
      "subtokens appended \n",
      "start before assigning end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> & end val <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)>\n",
      "current subtokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([b'to'], dtype=object)>\n",
      "outer_cond start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)>  chars [b't' b'o'] is_bad <tf.Variable 'Variable:0' shape=() dtype=bool, numpy=False> sub_tokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([b'to'], dtype=object)>\n",
      "per_word_tokenize word  b'an' shape [] chars [b'a' b'n']\n",
      "outer_cond start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)>  chars [b'a' b'n'] is_bad <tf.Variable 'Variable:0' shape=() dtype=bool, numpy=False> sub_tokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([], dtype=object)>\n",
      "outer_while_loop start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)>  chars [b'a' b'n'] is_bad <tf.Variable 'Variable:0' shape=() dtype=bool, numpy=False> sub_tokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([], dtype=object)>\n",
      "vocab lookup inside inner_cond  start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)> condition <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=-1> is_vocab_missing True & entire cond True\n",
      "inner_while_body start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)> word b'an' chars [b'a' b'n']\n",
      "\n",
      "substring b'an' vocab lookup <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=10151>\n",
      "present b'an'\n",
      "vocab lookup inside inner_cond  start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)> condition <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=10151> is_vocab_missing False & entire cond False\n",
      "****end of inner while start [0] end [2] substring b'an' chars [b'a' b'n'] vocab_lookup 10151 \n",
      "\n",
      "\n",
      "sub tokens concat  [\"an\"] shape  [1]\n",
      "subtokens  []  sub tokens shape  []\n",
      "subtokens appended \n",
      "start before assigning end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> & end val <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)>\n",
      "current subtokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([b'an'], dtype=object)>\n",
      "outer_cond start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)>  chars [b'a' b'n'] is_bad <tf.Variable 'Variable:0' shape=() dtype=bool, numpy=False> sub_tokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([b'an'], dtype=object)>\n",
      "per_word_tokenize word  b'aagent' shape [] chars [b'a' b'a' b'g' b'e' b'n' b't']\n",
      "outer_cond start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([6], dtype=int32)>  chars [b'a' b'a' b'g' b'e' b'n' b't'] is_bad <tf.Variable 'Variable:0' shape=() dtype=bool, numpy=False> sub_tokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([], dtype=object)>\n",
      "outer_while_loop start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([6], dtype=int32)>  chars [b'a' b'a' b'g' b'e' b'n' b't'] is_bad <tf.Variable 'Variable:0' shape=() dtype=bool, numpy=False> sub_tokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([], dtype=object)>\n",
      "vocab lookup inside inner_cond  start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([6], dtype=int32)> condition <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=-1> is_vocab_missing True & entire cond True\n",
      "inner_while_body start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([6], dtype=int32)> word b'aagent' chars [b'a' b'a' b'g' b'e' b'n' b't']\n",
      "\n",
      "substring b'aagent' vocab lookup <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=-1>\n",
      "missing b'aagent'\n",
      "vocab lookup inside inner_cond  start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([5], dtype=int32)> condition <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=-1> is_vocab_missing True & entire cond True\n",
      "inner_while_body start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([5], dtype=int32)> word b'aagent' chars [b'a' b'a' b'g' b'e' b'n' b't']\n",
      "\n",
      "substring b'aagen' vocab lookup <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=-1>\n",
      "missing b'aagen'\n",
      "vocab lookup inside inner_cond  start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([4], dtype=int32)> condition <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=-1> is_vocab_missing True & entire cond True\n",
      "inner_while_body start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([4], dtype=int32)> word b'aagen' chars [b'a' b'a' b'g' b'e' b'n' b't']\n",
      "\n",
      "substring b'aage' vocab lookup <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=-1>\n",
      "missing b'aage'\n",
      "vocab lookup inside inner_cond  start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([3], dtype=int32)> condition <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=-1> is_vocab_missing True & entire cond True\n",
      "inner_while_body start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([3], dtype=int32)> word b'aage' chars [b'a' b'a' b'g' b'e' b'n' b't']\n",
      "\n",
      "substring b'aag' vocab lookup <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=-1>\n",
      "missing b'aag'\n",
      "vocab lookup inside inner_cond  start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)> condition <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=-1> is_vocab_missing True & entire cond True\n",
      "inner_while_body start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)> word b'aag' chars [b'a' b'a' b'g' b'e' b'n' b't']\n",
      "\n",
      "substring b'aa' vocab lookup <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=28335>\n",
      "present b'aa'\n",
      "vocab lookup inside inner_cond  start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)> condition <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=28335> is_vocab_missing False & entire cond False\n",
      "****end of inner while start [0] end [2] substring b'aa' chars [b'a' b'a' b'g' b'e' b'n' b't'] vocab_lookup 28335 \n",
      "\n",
      "\n",
      "sub tokens concat  [\"aa\"] shape  [1]\n",
      "subtokens  []  sub tokens shape  []\n",
      "subtokens appended \n",
      "start before assigning end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([0], dtype=int32)> & end val <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)>\n",
      "current subtokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([b'aa'], dtype=object)>\n",
      "outer_cond start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)>  chars [b'a' b'a' b'g' b'e' b'n' b't'] is_bad <tf.Variable 'Variable:0' shape=() dtype=bool, numpy=False> sub_tokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([b'aa'], dtype=object)>\n",
      "outer_while_loop start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)>  chars [b'a' b'a' b'g' b'e' b'n' b't'] is_bad <tf.Variable 'Variable:0' shape=() dtype=bool, numpy=False> sub_tokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([b'aa'], dtype=object)>\n",
      "vocab lookup inside inner_cond  start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([6], dtype=int32)> condition <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=-1> is_vocab_missing True & entire cond True\n",
      "inner_while_body start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([6], dtype=int32)> word b'gent' chars [b'a' b'a' b'g' b'e' b'n' b't']\n",
      "\n",
      "substring b'##gent' vocab lookup <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=22500>\n",
      "present b'##gent'\n",
      "vocab lookup inside inner_cond  start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([6], dtype=int32)> condition <tf.Variable 'vocab_lookup:0' shape=() dtype=int64, numpy=22500> is_vocab_missing False & entire cond False\n",
      "****end of inner while start [2] end [6] substring b'##gent' chars [b'a' b'a' b'g' b'e' b'n' b't'] vocab_lookup 22500 \n",
      "\n",
      "\n",
      "sub tokens concat  [\"aa\" \"##gent\"] shape  [2]\n",
      "subtokens  [\"aa\"]  sub tokens shape  [\"aa\"]\n",
      "subtokens appended \n",
      "start before assigning end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([2], dtype=int32)> & end val <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([6], dtype=int32)>\n",
      "current subtokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([b'aa', b'##gent'], dtype=object)>\n",
      "outer_cond start <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([6], dtype=int32)> end <tf.Variable 'Variable:0' shape=(1,) dtype=int32, numpy=array([6], dtype=int32)>  chars [b'a' b'a' b'g' b'e' b'n' b't'] is_bad <tf.Variable 'Variable:0' shape=() dtype=bool, numpy=False> sub_tokens <tf.Variable 'Variable:0' shape=(None,) dtype=string, numpy=array([b'aa', b'##gent'], dtype=object)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=int64, numpy=array([31311, 10114, 10151,    -1])>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_sentence('talk to an aagent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict from model1  [[ 0  1  2  3  4 -1]\n",
      " [ 1  2 -1 -1 -1 -1]]\n",
      "predict from model2  [[ 1  2  3  4  5 -1]]\n"
     ]
    }
   ],
   "source": [
    "print('predict from model1 ', model_lookup.predict(['hi testing lookup in tf randomtext','testing lookup']))\n",
    "print('predict from model2 ',model_lookup2.predict(['hi testing lookup in tf randomtext']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: /var/extra/users/jgeorge/tf2.0/input/dish/data/jan17_2020/model/lookup/assets\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "loaded model config 1 - \n",
      " {'name': 'model', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None,), 'dtype': 'string', 'sparse': False, 'ragged': False, 'name': 'input_text'}, 'name': 'input_text', 'inbound_nodes': []}, {'class_name': 'VocabLookup', 'config': {'name': 'vocab_lookup', 'trainable': False, 'dtype': 'int32', 'vocab_path': '/var/extra/users/jgeorge/tf2.0/input/dish/data/jan17_2020/vocab_test.txt'}, 'name': 'vocab_lookup', 'inbound_nodes': [[['input_text', 0, 0, {}]]]}], 'input_layers': {'input_text': ['input_text', 0, 0]}, 'output_layers': [['vocab_lookup', 0, 0]]} \n",
      "\n",
      "predict from loaded model1  [[ 0  1  2  3  4 -1]]\n"
     ]
    }
   ],
   "source": [
    "model_lookup.save(model_dir_lookup)\n",
    "model_lookup_loaded = tf.keras.models.load_model(model_dir_lookup)\n",
    "print('loaded model config 1 - \\n',model_lookup_loaded.get_config(),'\\n')\n",
    "print('predict from loaded model1 ',model_lookup_loaded.predict(['hi testing lookup in tf randomtext']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/extra/users/jgeorge/tf2.0/input/dish/data/jan17_2020/model/lookup_2/assets\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "loaded model config 2 - \n",
      " {'name': 'model_1', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None,), 'dtype': 'string', 'sparse': False, 'ragged': False, 'name': 'input_text'}, 'name': 'input_text', 'inbound_nodes': []}, {'class_name': 'VocabLookup2', 'config': {'name': 'vocab_lookup2', 'trainable': False, 'dtype': 'int32', 'keys': ['hi', 'testing', 'lookup', 'in', 'tf']}, 'name': 'vocab_lookup2', 'inbound_nodes': [[['input_text', 0, 0, {}]]]}], 'input_layers': {'input_text': ['input_text', 0, 0]}, 'output_layers': [['vocab_lookup2', 0, 0]]} \n",
      "\n",
      "predict from loaded model2  [[ 1  2  3  4  5 -1]]\n"
     ]
    }
   ],
   "source": [
    "model_lookup2.save(model_dir_lookup2)\n",
    "model_lookup_loaded2 = tf.keras.models.load_model(model_dir_lookup2)\n",
    "print('loaded model config 2 - \\n',model_lookup_loaded2.get_config(),'\\n')\n",
    "print('predict from loaded model2 ', model_lookup_loaded2.predict(['hi testing lookup in tf randomtext']))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "lookup_out_direct = table.lookup(splitted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lookup = tf.keras.Model(inputs={'input_text':input_text},outputs=lookup_out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = tf.keras.Input(shape=(),dtype=tf.string,name='input_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text2 = tf.keras.Input(shape=(),dtype=tf.string,name='input_text2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_path='/var/extra/users/jgeorge/tf2.0/input/dish/data/jan17_2020/vocab.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_model_dir='/var/extra/users/jgeorge/tf2.0/input/dish/data/jan17_2020/model_dir'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TableLookup(tf.keras.layers.Layer):\n",
    "    def __init__(self,filepath):\n",
    "        super(TableLookup,self).__init__(trainable=False,dtype=tf.string)\n",
    "        self.filepath=filepath\n",
    "        self.default_label='other-other'\n",
    "        \n",
    "    def build(self,input_shape):\n",
    "        self.built=True\n",
    "    \n",
    "    def call(self,label_id):\n",
    "        table = tf.lookup.TextFileInitializer(self.filepath,\n",
    "                                      key_dtype=tf.int64,key_index=tf.lookup.TextFileIndex.LINE_NUMBER,\n",
    "                                     value_dtype=tf.string,value_index=tf.lookup.TextFileIndex.WHOLE_LINE)\n",
    "# table.initialize(intent_file)\n",
    "        label_lookup = tf.lookup.StaticHashTable(table\n",
    "                                        ,default_value=self.default_label)\n",
    "        return label_lookup.lookup(label_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegexReplacement(tf.keras.layers.Layer):\n",
    "    def __init__(self,pattern,replacement):\n",
    "        super(RegexReplacement,self).__init__(trainable=False,dtype=tf.string)\n",
    "        self.pattern=pattern\n",
    "        self.replacement=replacement\n",
    "        \n",
    "    def build(self,input_shape):\n",
    "        self.built=True\n",
    "    \n",
    "    def call(self,input_text):\n",
    "        return tf.strings.regex_replace(input_text,pattern=self.pattern,rewrite=self.replacement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.layers.experimental.preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensorflow.python.keras.engine.base_layer.Layer,\n",
       " tensorflow.python.module.module.Module,\n",
       " tensorflow.python.training.tracking.tracking.AutoTrackable,\n",
       " tensorflow.python.training.tracking.base.Trackable,\n",
       " tensorflow.python.keras.utils.version_utils.LayerVersionSelector,\n",
       " object]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.layers.Layer.mro()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/space/engineering/tf_serve/models/tryout_jithin'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_text = tf.keras.Input(shape=(),dtype=tf.string,name='input_text')\n",
    "@tf.function(input_signature=[tf.TensorSpec(shape=None,dtype=tf.string)])\n",
    "def perform_regex_replace1(input_text):\n",
    "    replaced_text = RegexReplacement(\"abc\",\"xyz\")(input_text)\n",
    "    return replaced_text\n",
    "\n",
    "@tf.function(input_signature=[tf.TensorSpec(shape=None,dtype=tf.string)])\n",
    "def perform_regex_replace(input_text):\n",
    "    replaced_text = tf.strings.regex_replace(input_text,pattern=\"abc\",rewrite=\"xyz\")\n",
    "    return replaced_text\n",
    "# model_replace = tf.keras.Model(inputs={'input_text':input_text},outputs=replaced_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(input_signature=[tf.TensorSpec(shape=None,dtype=tf.float32)])\n",
    "def multiply_x(x):\n",
    "#     return {'intents':[{'className':tf.constant('dummy'),'score': x*2}]}\n",
    "    return {'intents': x*2,'intents3':x*3}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intents': [{'className': <tf.Tensor: shape=(), dtype=string, numpy=b'dummy'>,\n",
       "   'score': <tf.Tensor: shape=(), dtype=float32, numpy=4.0>}]}"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiply_x(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/space/engineering/tf_serve/models/tryout_jithin_2'"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model_path2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /space/engineering/tf_serve/models/tryout_jithin_2/3/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /space/engineering/tf_serve/models/tryout_jithin_2/3/assets\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(to_export,os.path.join(final_model_path2,'3'),\n",
    "                    signatures={'serving_default':multiply_x.get_concrete_function(),\n",
    "                               'web2nl':multiply_x})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.eager.function.ConcreteFunction at 0x7fdbf03230d0>"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perform_regex_replace.get_concrete_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.eager.function.ConcreteFunction at 0x7fdbf007e290>"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiply_x.get_concrete_function(tf.TensorSpec(shape=None,dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiply_x.get_concrete_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=20>"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiply_x.get_concrete_function(10)()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.saved_model.save(to_export,os.path.join(final_model_path2,'2'),\n",
    "                    signatures={'serving_default':perform_regex_replace.get_concrete_function(),\n",
    "                               'regex_replace':perform_regex_replace})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensorflow.python.keras.engine.training.Model,\n",
       " tensorflow.python.keras.engine.network.Network,\n",
       " tensorflow.python.keras.engine.base_layer.Layer,\n",
       " tensorflow.python.module.module.Module,\n",
       " tensorflow.python.training.tracking.tracking.AutoTrackable,\n",
       " tensorflow.python.training.tracking.base.Trackable,\n",
       " tensorflow.python.keras.utils.version_utils.LayerVersionSelector,\n",
       " tensorflow.python.keras.utils.version_utils.ModelVersionSelector,\n",
       " object]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.Model.mro()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensorflow.python.keras.engine.base_layer.Layer,\n",
       " tensorflow.python.module.module.Module,\n",
       " tensorflow.python.training.tracking.tracking.AutoTrackable,\n",
       " tensorflow.python.training.tracking.base.Trackable,\n",
       " tensorflow.python.keras.utils.version_utils.LayerVersionSelector,\n",
       " object]"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.layers.Layer.mro()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.eager.function.ConcreteFunction at 0x7fdbf03230d0>"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perform_regex_replace.get_concrete_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_regex_replace.get_concrete_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model_path2='/space/engineering/tf_serve/models/tryout_jithin_2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /space/engineering/tf_serve/models/tryout_jithin_2/2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /space/engineering/tf_serve/models/tryout_jithin_2/2/assets\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(to_export,os.path.join(final_model_path2,'2'),\n",
    "                    signatures={'serving_default':perform_regex_replace.get_concrete_function(),\n",
    "                               'regex_replace':perform_regex_replace})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adder(tf.Module):\n",
    "\n",
    "    @tf.function(input_signature=[tf.TensorSpec(shape=None, dtype=tf.float32)])\n",
    "#     @tf.function\n",
    "    def add(self,x):\n",
    "        out = x + x + 1\n",
    "#         out = {'value1': x + x + 1, 'value2': x*2 }\n",
    "        return out\n",
    "    \n",
    "    @tf.function(input_signature=[tf.TensorSpec(shape=None, dtype=tf.float32)])\n",
    "    def add_multiply(self,x_mul):\n",
    "        added = self.add(x_mul)\n",
    "        return added*5\n",
    "\n",
    "to_export = Adder()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([ 9., 11.], dtype=float32)>"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_export.add(tf.constant([4.0,5.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([45., 55.], dtype=float32)>"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_export.add_multiply(tf.constant([4.0,5.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.eager.function.ConcreteFunction at 0x7fdbf072c350>"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_export.add.get_concrete_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.eager.function.ConcreteFunction at 0x7fdbf06e3210>"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_export.add_multiply.get_concrete_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.eager.function.ConcreteFunction at 0x7fdbf05d4310>"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = to_export.add.get_concrete_function()\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /space/engineering/tf_serve/models/tryout_jithin/5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /space/engineering/tf_serve/models/tryout_jithin/5/assets\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(to_export,os.path.join(final_model_path,'5'),\n",
    "                    signatures={'serving_default':to_export.add.get_concrete_function(),\n",
    "                               'add_multiply':to_export.add_multiply.get_concrete_function()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /space/engineering/tf_serve/models/tryout_jithin/4/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /space/engineering/tf_serve/models/tryout_jithin/4/assets\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(to_export,os.path.join(final_model_path,'4'),\n",
    "                    signatures={'serving_default':to_export.add.get_concrete_function()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.eager.function.ConcreteFunction at 0x7fdbfccd8e50>,\n",
       " <tensorflow.python.eager.function.ConcreteFunction at 0x7fdbf0cc5610>,\n",
       " <tensorflow.python.eager.function.ConcreteFunction at 0x7fdc94334410>,\n",
       " <tensorflow.python.eager.function.ConcreteFunction at 0x7fdbfce91f90>,\n",
       " <tensorflow.python.eager.function.ConcreteFunction at 0x7fdbfd3bc950>,\n",
       " <tensorflow.python.eager.function.ConcreteFunction at 0x7fdbfc973f10>,\n",
       " <tensorflow.python.eager.function.ConcreteFunction at 0x7fdbfcc31e10>,\n",
       " <tensorflow.python.eager.function.ConcreteFunction at 0x7fdbfc879390>,\n",
       " <tensorflow.python.eager.function.ConcreteFunction at 0x7fdbf1104ad0>,\n",
       " <tensorflow.python.eager.function.ConcreteFunction at 0x7fdc7c7dc990>]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[to_export.add.get_concrete_function(i) for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/space/engineering/tf_serve/models/tryout_jithin'"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.saved_model.save(to_export,os.path.join(final_model_path,'1'),\n",
    "                    signatures={'serving_default':to_export.add.get_concrete_function()},\n",
    "                    options=tf.saved_model.SaveOptions(function_aliases={'add_fn':to_export.add}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /space/engineering/tf_serve/models/tryout_jithin/2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /space/engineering/tf_serve/models/tryout_jithin/2/assets\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(to_export,os.path.join(final_model_path,'2'),\n",
    "                    signatures={'serving_default':to_export.add.get_concrete_function()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(final_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Api call from model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requests version  2.23.0\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-1e6f886130af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequestUrl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0moutput_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0membedding1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'outputs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# embedding for 1st sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0membedding2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'outputs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# embedding for 2nd sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/requests/models.py\u001b[0m in \u001b[0;36mjson\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    896\u001b[0m                     \u001b[0;31m# used.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 898\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcomplexjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    899\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 348\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "##python = 3.6.8 \n",
    "##requests.__version__ = '2.21.0'\n",
    "print('requests version ',requests.__version__)\n",
    "requestUrl='http://stable.api.sv2.247-inc.net/v1/machinelearning/predictions/clients/standard/applications/use/models/model'\n",
    "data_dict = { \"signature_name\": \"serving_default\",\"inputs\": {\"input_text\":[\"talk to an agent\",\"talk to representative\"]}}\n",
    "\n",
    "response = requests.post(requestUrl,json=data_dict)\n",
    "output_dict = response.json()\n",
    "embedding1 = output_dict['outputs'][0] # embedding for 1st sentence\n",
    "embedding2 = output_dict['outputs'][1] # embedding for 2nd sentence\n",
    "print('length of embedding1',len(embedding1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ApiCall(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(ApiCall,self).__init__(trainable=False,dtype=tf.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = tf.keras.Input(shape=(),dtype=tf.string,name='input_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentencepieceEncodeDense(values=<tf.Tensor: shape=(2, 8), dtype=int32, numpy=\n",
       "array([[   48,    25,    32,     0,     0,     0,     0,     0],\n",
       "       [  328,  5123, 13302,    18,    25,    48,  5477,  1854]],\n",
       "      dtype=int32)>, sequence_length=<tf.Tensor: shape=(2,), dtype=int32, numpy=array([3, 8], dtype=int32)>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfs_encoded = tfs.encode(['this is it','next sentence taks is this random text'],model_proto=model_proto)\n",
    "tfs_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 8])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfs_encoded.values.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfs_encoded.values.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[48, 25, 32,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
       " <tf.Tensor: shape=(1, 8), dtype=int32, numpy=\n",
       " array([[  328,  5123, 13302,    18,    25,    48,  5477,  1854]],\n",
       "       dtype=int32)>]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.split(tfs_encoded.values,tfs_encoded.values.shape[0],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(8,), dtype=int32, numpy=array([48, 25, 32,  0,  0,  0,  0,  0], dtype=int32)>,\n",
       " <tf.Tensor: shape=(8,), dtype=int32, numpy=\n",
       " array([  328,  5123, 13302,    18,    25,    48,  5477,  1854],\n",
       "       dtype=int32)>]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.unstack(tfs_encoded.values,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.map_fn(lambda x : x, tf.split(tfs_encoded.values,tfs_encoded.values.shape[0],axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 8), dtype=int32, numpy=\n",
       "array([[   48,    25,    32,     0,     0,     0,     0,     0],\n",
       "       [  328,  5123, 13302,    18,    25,    48,  5477,  1854]],\n",
       "      dtype=int32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfs_encoded.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentencepieceEncodeDense(values=<tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n",
      "array([[  48,   25,   32],\n",
      "       [ 226, 5123,    0]], dtype=int32)>, sequence_length=<tf.Tensor: shape=(2,), dtype=int32, numpy=array([3, 2], dtype=int32)>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n",
       "array([[  48,   25,   32],\n",
       "       [ 226, 5123,    0]], dtype=int32)>"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfs_encoded = tfs.encode(['this is it','another sentence'],model_proto=model_proto)\n",
    "print(tfs_encoded)\n",
    "# seq_len = \n",
    "t = tf.squeeze(tfs_encoded.values)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "[CLS_ID,SEP_ID,PAD_ID]  = tfs.piece_to_id(['[CLS]','[SEP]','<pad>'],model_proto=model_proto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=int32, numpy=array([ 2, 48, 25, 32,  3], dtype=int32)>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.concat([[CLS_ID],t,[SEP_ID]],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5), dtype=int32, numpy=\n",
       "array([[  48,   25,   32,    0,    0],\n",
       "       [ 226, 5123,    0,    0,    0]], dtype=int32)>"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.pad(t,[[0,0],[0,2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3), dtype=int32, numpy=array([[48, 25, 32]], dtype=int32)>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.expand_dims(t,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([3])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([3])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=int32, numpy=array([48, 25], dtype=int32)>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.slice(t,[0],[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "[CLS_ID,SEP_ID,PAD_ID]  = tfs.piece_to_id(['[CLS]','[SEP]','<pad>'],model_proto=model_proto)\n",
    "@tf.function\n",
    "def get_encoded_text1(input_text_batch,model_proto,max_sequence_length):\n",
    "#     https://stackoverflow.com/questions/45404056/tf-unstack-with-dynamic-shape\n",
    "#     batch_size = input_text_batch.shape[0]\n",
    "#     batch_size = input_text_batch.get_shape()[0]\n",
    "#     l = tf.split(input_text_batch,batch_size,axis=0)\n",
    "    \n",
    "#     l = tf.unstack(input_text_batch,axis=0)\n",
    "    l = input_text_batch\n",
    "    tf.print(\"list l is \",l)\n",
    "    tf.print(\"type of list l \",type(l))\n",
    "    \n",
    "    \n",
    "    def process_invidual_line_encoding(x):\n",
    "        list_x = tf.expand_dims(x,axis=0)\n",
    "        sp_encoded = tfs.encode(list_x,model_proto=model_proto)\n",
    "#         sp_encoded = tfs.encode(x)\n",
    "#         removing the batch dim with size=1\n",
    "        values = tf.squeeze(sp_encoded.values,name='squeezed_values')\n",
    "        sequence_length = tf.squeeze(sp_encoded.sequence_length)\n",
    "        tf.print('squeezed_values ' ,values)\n",
    "#         trimming to max_length-2 (-2 to incorporate [CLS], [SEP])\n",
    "        trimmed_max_length = max_sequence_length-2\n",
    "#         values_trimmed = tf.slice(values,begin=[0],size=[trimmed_max_length],name='trimmed_out')\n",
    "        values_trimmed = tf.cond(tf.greater(sequence_length,trimmed_max_length), \n",
    "                                lambda : tf.slice(values,begin=[0],size=[trimmed_max_length],name='trimmed_out'),lambda : values)\n",
    "        tf.print('values_trimmed ',values_trimmed)\n",
    "        concat = tf.concat([[CLS_ID],values_trimmed,[SEP_ID]],axis=0,name='concat_out')\n",
    "        tf.print('concat_out ',concat)\n",
    "        tf.print('size of concat ',tf.size(concat))\n",
    "#         need not prepend anything so 0 for 1st entry in padding, \n",
    "#         & next value for padding is how many dimensions required at the end of tensor\n",
    "        padded = tf.pad(concat,paddings=[[0,max_sequence_length-tf.size(concat)]],name='padded_out')\n",
    "        return padded\n",
    "        \n",
    "    encoded = tf.map_fn(lambda x: process_invidual_line_encoding(x),l,dtype=tf.int32)\n",
    "    stacked = tf.stack(encoded)\n",
    "    tf.print('stacked size ',tf.size(stacked))\n",
    "    tf.print('stacked shape ',tf.shape(stacked))\n",
    "    return stacked\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "[CLS_ID,SEP_ID,PAD_ID]  = tfs.piece_to_id(['[CLS]','[SEP]','<pad>'],model_proto=model_proto)\n",
    "# @tf.function\n",
    "def get_encoded_text(input_text_batch,model_proto,max_sequence_length):\n",
    "#     https://stackoverflow.com/questions/45404056/tf-unstack-with-dynamic-shape\n",
    "#     batch_size = input_text_batch.shape[0]\n",
    "#     batch_size = input_text_batch.get_shape()[0]\n",
    "#     l = tf.split(input_text_batch,batch_size,axis=0)\n",
    "    \n",
    "#     l = tf.unstack(input_text_batch,axis=0)\n",
    "    l = input_text_batch\n",
    "    tf.print(\"list l is \",l)\n",
    "    tf.print(\"type of list l \",type(l))\n",
    "    \n",
    "    \n",
    "    def process_invidual_line_encoding(x):\n",
    "        list_x = tf.expand_dims(x,axis=0)\n",
    "        sp_encoded = tfs.encode(list_x,model_proto=model_proto)\n",
    "#         sp_encoded = tfs.encode(x)\n",
    "#         removing the batch dim with size=1\n",
    "        values = tf.squeeze(sp_encoded.values,name='squeezed_values')\n",
    "        sequence_length = tf.squeeze(sp_encoded.sequence_length)\n",
    "        tf.print('squeezed_values ' ,values)\n",
    "#         trimming to max_length-2 (-2 to incorporate [CLS], [SEP])\n",
    "        trimmed_max_length = max_sequence_length-2\n",
    "#         values_trimmed = tf.slice(values,begin=[0],size=[trimmed_max_length],name='trimmed_out')\n",
    "        values_trimmed = tf.cond(tf.greater(sequence_length,trimmed_max_length), \n",
    "                                lambda : tf.slice(values,begin=[0],size=[trimmed_max_length],name='trimmed_out'),lambda : values)\n",
    "        tf.print('values_trimmed ',values_trimmed)\n",
    "        concat = tf.concat([[CLS_ID],values_trimmed,[SEP_ID]],axis=0,name='concat_out')\n",
    "        tf.print('concat_out ',concat)\n",
    "        tf.print('size of concat ',tf.size(concat))\n",
    "        tf.print('shape of concat ',tf.shape(concat))\n",
    "#         actual_token_length = tf.size(concat) #this would also work since we are processing line by line in this function\n",
    "        actual_token_length = tf.shape(concat)[-1]\n",
    "#         need not prepend anything so 0 for 1st entry in padding, \n",
    "#         & next value for padding is how many dimensions required at the end of tensor\n",
    "        padded = tf.pad(concat,paddings=[[0,max_sequence_length-actual_token_length]],name='input_ids')\n",
    "#         segment_ids = tf.zeros(shape=tf.shape(padded),dtype=tf.int32)\n",
    "#         or\n",
    "        segment_ids = tf.zeros_like(padded,dtype=tf.int32,name='segment_ids')\n",
    "        \n",
    "        input_mask = tf.scatter_nd(indices=tf.expand_dims(tf.range(0,actual_token_length),axis=1),\n",
    "                                   updates=tf.ones(shape=[actual_token_length],dtype=tf.int32),\n",
    "                                   shape=[max_sequence_length],name='input_mask')\n",
    "        return (padded,segment_ids,input_mask)\n",
    "#     Issue running map_fn on gpu https://github.com/tensorflow/tensorflow/issues/28007 \n",
    "#     https://www.tensorflow.org/api_docs/python/tf/device\n",
    "    with tf.device('/device:CPU:0'):\n",
    "        encoded = tf.map_fn(lambda x: process_invidual_line_encoding(x),l,dtype=(tf.int32,tf.int32,tf.int32))\n",
    "    tf.print('encoded map_fn ',encoded)\n",
    "    tf.print('encoded map_fn type ',type(encoded))\n",
    "    tf.print('encoded map_fn shape ',tf.shape(encoded))\n",
    "#     stacked = tf.stack(encoded)\n",
    "    stacked = encoded\n",
    "    tf.print('stacked size ',tf.size(stacked))\n",
    "    tf.print('stacked shape ',tf.shape(stacked))\n",
    "    return stacked\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensorflow.python.keras.engine.base_layer.Layer,\n",
       " tensorflow.python.module.module.Module,\n",
       " tensorflow.python.training.tracking.tracking.AutoTrackable,\n",
       " tensorflow.python.training.tracking.base.Trackable,\n",
       " tensorflow.python.keras.utils.version_utils.LayerVersionSelector,\n",
       " object]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.layers.Layer.mro()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensorflow.python.keras.engine.training.Model,\n",
       " tensorflow.python.keras.engine.network.Network,\n",
       " tensorflow.python.keras.engine.base_layer.Layer,\n",
       " tensorflow.python.module.module.Module,\n",
       " tensorflow.python.training.tracking.tracking.AutoTrackable,\n",
       " tensorflow.python.training.tracking.base.Trackable,\n",
       " tensorflow.python.keras.utils.version_utils.LayerVersionSelector,\n",
       " tensorflow.python.keras.utils.version_utils.ModelVersionSelector,\n",
       " object]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.Model.mro()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_proto = tf.io.gfile.GFile(sentencepiece_path, 'rb').read()\n",
    "tftext_spm = tftext.SentencepieceTokenizer(model=model_proto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.RaggedTensor [[48, 25, 21, 1289, 5123], [328, 5123]]>,\n",
       " <tf.RaggedTensor [[0, 4, 7, 9, 14], [0, 4]]>,\n",
       " <tf.RaggedTensor [[4, 7, 9, 14, 23], [4, 13]]>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tftext_spm.tokenize_with_offsets(['this is a test sentence','next sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5), dtype=int32, numpy=\n",
       "array([[  48,   25,   21, 1289, 5123],\n",
       "       [ 328, 5123,    0,    0,    0]], dtype=int32)>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tftext_spm.tokenize(['this is a test sentence','next sentence']).to_tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensorflow.python.keras.engine.base_layer.Layer,\n",
       " tensorflow.python.module.module.Module,\n",
       " tensorflow.python.training.tracking.tracking.AutoTrackable,\n",
       " tensorflow.python.training.tracking.base.Trackable,\n",
       " tensorflow.python.keras.utils.version_utils.LayerVersionSelector,\n",
       " object]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.layers.Layer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.python.keras.utils.version_utils.LayerVersionSelector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentencepieceTokenization_text(tf.keras.layers.Layer):\n",
    "    def __init__(self,model_path):\n",
    "        super(SentencepieceTokenization_text, self).__init__(trainable=False,dtype=tf.int32)\n",
    "        self.model_path = model_path\n",
    "        \n",
    "    def build(self,input_shape):\n",
    "        self.model_proto = tf.io.gfile.GFile(self.model_path, 'rb').read()\n",
    "        self.sp_model = tftext.SentencepieceTokenizer(model=self.model_proto)\n",
    "#         [self.CLS_ID,self.SEP_ID,self.PAD_ID]  = tfs.piece_to_id(['[CLS]','[SEP]','<pad>'],model_file=self.model_path)\n",
    "        self.built=True\n",
    "        \n",
    "    @tf.function    \n",
    "    def call(self, input_text):\n",
    "        encoded_text = self.sp_model.tokenize(input_text).to_tensor()\n",
    "        ##tensorflow sentence piece works while exporting to graph while, tf_text sentencepiece doesn't\n",
    "#         encoded_text = tfs.encode(input_text,model_proto=self.model_proto)\n",
    "        return encoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_text (InputLayer)      [(None,)]                 0         \n",
      "_________________________________________________________________\n",
      "sentencepiece_tokenization_t (None, None)              0         \n",
      "=================================================================\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[   48,    25,    21,  5717,  5123,     0,     0,     0,     0],\n",
       "       [   48,    25,    21,  5477,  1854,    26,   749,    70, 19608]],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = tf.keras.Input(shape=(),dtype=tf.string,name='input_text')\n",
    "sp_model_path2 = '/space/engineering/tfhub_modules/db96091489ca3b9a526f252d262b880571e1335e/assets/30k-clean.model'\n",
    "sentence_piece_layer = SentencepieceTokenization_text(model_path=sp_model_path2)\n",
    "encoded_text = sentence_piece_layer(input_text)\n",
    "# input_ids,segment_ids,input_mask = sentence_piece_layer(input_text)\n",
    "# tf.autograph.trace(encoded_text)\n",
    "# print('input_ids ',input_ids, 'segment_ids ',segment_ids, 'input_mask ',input_mask )\n",
    "# print('encoded text ',encoded_text)\n",
    "# model = tf.keras.Model(inputs={'input_text':input_text},outputs=encoded_text)\n",
    "\n",
    "# model = tf.keras.Model(inputs={'input_text':input_text},outputs=input_ids)\n",
    "model = tf.keras.Model(inputs={'input_text':input_text},outputs=encoded_text)\n",
    "sample_text = 'this is a sample sentence'\n",
    "sample_text2 = 'this is a random text for trying out encoding'\n",
    "model.summary()\n",
    "model.predict([sample_text,sample_text2])\n",
    "# model.sp_model_file = tf.saved_model.Asset(sentence_piece_layer)\n",
    "# model.save(vocab_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Tried to export a function which references untracked object Tensor(\"25457:0\", shape=(), dtype=resource).TensorFlow objects (e.g. tf.Variable) captured by functions must be tracked by assigning them to an attribute of a tracked object or assigned to an attribute of the main object directly.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-7f90d00765f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msentence_piece_save_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/var/extra/users/jgeorge/tf2.0/input/dish/models/sentence_piece_text'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_piece_save_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filepath, overwrite, include_optimizer, save_format, signatures, options)\u001b[0m\n\u001b[1;32m   1050\u001b[0m     \"\"\"\n\u001b[1;32m   1051\u001b[0m     save.save_model(self, filepath, overwrite, include_optimizer, save_format,\n\u001b[0;32m-> 1052\u001b[0;31m                     signatures, options)\n\u001b[0m\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/keras/saving/save.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer, save_format, signatures, options)\u001b[0m\n\u001b[1;32m    136\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     saved_model_save.save(model, filepath, overwrite, include_optimizer,\n\u001b[0;32m--> 138\u001b[0;31m                           signatures, options)\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/save.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(model, filepath, overwrite, include_optimizer, signatures, options)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;31m# we use the default replica context here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mdistribution_strategy_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_default_replica_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m       \u001b[0msave_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/saved_model/save.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, export_dir, signatures, options)\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m   _, exported_graph, object_saver, asset_info = _build_meta_graph(\n\u001b[0;32m--> 951\u001b[0;31m       obj, export_dir, signatures, options, meta_graph_def)\n\u001b[0m\u001b[1;32m    952\u001b[0m   \u001b[0msaved_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_model_schema_version\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSAVED_MODEL_SCHEMA_VERSION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/saved_model/save.py\u001b[0m in \u001b[0;36m_build_meta_graph\u001b[0;34m(obj, export_dir, signatures, options, meta_graph_def)\u001b[0m\n\u001b[1;32m   1025\u001b[0m   asset_info, exported_graph = _fill_meta_graph_def(meta_graph_def,\n\u001b[1;32m   1026\u001b[0m                                                     \u001b[0msaveable_view\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignatures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1027\u001b[0;31m                                                     options.namespace_whitelist)\n\u001b[0m\u001b[1;32m   1028\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_aliases\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m     \u001b[0mfunction_aliases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta_graph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta_info_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_aliases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/saved_model/save.py\u001b[0m in \u001b[0;36m_fill_meta_graph_def\u001b[0;34m(meta_graph_def, saveable_view, signature_functions, namespace_whitelist)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mexported_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m     \u001b[0msignatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_generate_signatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignature_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresource_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mconcrete_function\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msaveable_view\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcrete_functions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m       \u001b[0mconcrete_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/saved_model/save.py\u001b[0m in \u001b[0;36m_generate_signatures\u001b[0;34m(signature_functions, resource_map)\u001b[0m\n\u001b[1;32m    495\u001b[0m                                                   signature_key, function.name))\n\u001b[1;32m    496\u001b[0m     outputs = _call_function_with_mapped_captures(\n\u001b[0;32m--> 497\u001b[0;31m         function, mapped_inputs, resource_map)\n\u001b[0m\u001b[1;32m    498\u001b[0m     signatures[signature_key] = signature_def_utils.build_signature_def(\n\u001b[1;32m    499\u001b[0m         \u001b[0m_tensor_dict_to_tensorinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexterior_argument_placeholders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/saved_model/save.py\u001b[0m in \u001b[0;36m_call_function_with_mapped_captures\u001b[0;34m(function, args, resource_map)\u001b[0m\n\u001b[1;32m    447\u001b[0m   \u001b[0;34m\"\"\"Calls `function` in the exported graph, using mapped resource captures.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m   export_captures = _map_captures_to_created_tensors(function.graph.captures,\n\u001b[0;32m--> 449\u001b[0;31m                                                      resource_map)\n\u001b[0m\u001b[1;32m    450\u001b[0m   \u001b[0;31m# Calls the function quite directly, since we have new captured resource\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m   \u001b[0;31m# tensors we need to feed in which weren't part of the original function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/saved_model/save.py\u001b[0m in \u001b[0;36m_map_captures_to_created_tensors\u001b[0;34m(original_captures, resource_map)\u001b[0m\n\u001b[1;32m    370\u001b[0m            \u001b[0;34m\"be tracked by assigning them to an attribute of a tracked object \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m            \u001b[0;34m\"or assigned to an attribute of the main object directly.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m           ).format(interior))\n\u001b[0m\u001b[1;32m    373\u001b[0m     \u001b[0mexport_captures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped_resource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexport_captures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Tried to export a function which references untracked object Tensor(\"25457:0\", shape=(), dtype=resource).TensorFlow objects (e.g. tf.Variable) captured by functions must be tracked by assigning them to an attribute of a tracked object or assigned to an attribute of the main object directly."
     ]
    }
   ],
   "source": [
    "sentence_piece_save_path='/var/extra/users/jgeorge/tf2.0/input/dish/models/sentence_piece_text'\n",
    "model.save(sentence_piece_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# @tf.keras.utils.register_keras_serializable(package=\"Text\")\n",
    "# checkout tf.module as well (module seems to be super class of Layer)\n",
    "class SentencepieceTokenization(tf.keras.layers.Layer):\n",
    "#     def __init__(self,model_proto):\n",
    "    def __init__(self,model_path):\n",
    "#         super(SentencepieceTokenization, self).__init__(trainable=False,dynamic=True,dtype=tf.int32)\n",
    "        super(SentencepieceTokenization, self).__init__(trainable=False,dtype=tf.int32)\n",
    "        #     self.sp_model_proto = model_proto\n",
    "#         self.model_path = tf.saved_model.Asset(model_path)\n",
    "        self.model_path = model_path\n",
    "        \n",
    "#         model_proto = tf.io.gfile.GFile(self.model_path, 'rb').read()\n",
    "#         self.sp_model = tftext.SentencepieceTokenizer(model=model_proto)\n",
    "    def build(self,input_shape):\n",
    "#         self.model_proto = tf.io.gfile.GFile(self.model_path.asset_path, 'rb').read()\n",
    "        self.model_proto = tf.io.gfile.GFile(self.model_path, 'rb').read()\n",
    "#         self.sp_model = tftext.SentencepieceTokenizer(model=self.model_proto)\n",
    "        [self.CLS_ID,self.SEP_ID,self.PAD_ID]  = tfs.piece_to_id(['[CLS]','[SEP]','<pad>'],model_file=self.model_path)\n",
    "        self.built=True\n",
    "        \n",
    "#     @tf.function    \n",
    "    def call(self, input_text):\n",
    "#         encoded_text = self.sp_model.tokenize(input_text).to_tensor()\n",
    "        ##tensorflow sentence piece works while exporting to graph while, tf_text sentencepiece doesn't\n",
    "#         encoded_text = tfs.encode(input_text,model_proto=self.model_proto)\n",
    "\n",
    "#         model_proto = tf.io.gfile.GFile(self.model_path, 'rb').read()\n",
    "        encoded_text = self.get_encoded_text(input_text,model_proto=self.model_proto,max_sequence_length=20)\n",
    "        \n",
    "#         encoded_text = tf.RaggedTensor.from_tensor(encoded_text.values)\n",
    "#         return encoded_text.to_tensor()        \n",
    "        return encoded_text\n",
    "\n",
    "    @tf.function\n",
    "    def get_encoded_text(self,input_text_batch,model_proto,max_sequence_length):\n",
    "        tf.print(\"input_text_batch is \",input_text_batch)\n",
    "        tf.print(\"type of input_text_batch \",type(input_text_batch))\n",
    "\n",
    "        def process_invidual_line_encoding(x):\n",
    "#             tf sentencepiece requires a list as input, while the individual value that we get here\n",
    "#             is a single sentence, so adding one more dimension (i.e adding batch dimension = 1)\n",
    "            list_x = tf.expand_dims(x,axis=0)\n",
    "            sp_encoded = tfs.encode(list_x,model_proto=model_proto)\n",
    "    #         removing the batch dim with size=1 (which we added in the previous step)\n",
    "            values = tf.squeeze(sp_encoded.values,name='squeezed_values')\n",
    "            sequence_length = tf.squeeze(sp_encoded.sequence_length)\n",
    "            tf.print('squeezed_values ' ,values)\n",
    "    #         trimming to max_length-2 (-2 to incorporate [CLS], [SEP])\n",
    "            trimmed_max_length = max_sequence_length-2\n",
    "    #         values_trimmed = tf.slice(values,begin=[0],size=[trimmed_max_length],name='trimmed_out')\n",
    "            values_trimmed = tf.cond(tf.greater(sequence_length,trimmed_max_length), \n",
    "                                    lambda : tf.slice(values,begin=[0],size=[trimmed_max_length],name='trimmed_out'),lambda : values)\n",
    "            tf.print('values_trimmed ',values_trimmed)\n",
    "            concat = tf.concat([[self.CLS_ID],values_trimmed,[self.SEP_ID]],axis=0,name='concat_out')\n",
    "            tf.print('concat_out ',concat)\n",
    "            tf.print('size of concat ',tf.size(concat))\n",
    "            tf.print('shape of concat ',tf.shape(concat))\n",
    "    #         actual_token_length = tf.size(concat) #this would also work since we are processing line by line in this function\n",
    "            actual_token_length = tf.shape(concat)[-1]\n",
    "    #         need not prepend anything so 0 for 1st entry in padding, \n",
    "    #         & next value for padding is how many dimensions required at the end of tensor\n",
    "            padded = tf.pad(concat,paddings=[[0,max_sequence_length-actual_token_length]],name='input_ids')\n",
    "    #         segment_ids = tf.zeros(shape=tf.shape(padded),dtype=tf.int32)\n",
    "    #         or\n",
    "            segment_ids = tf.zeros_like(padded,dtype=tf.int32,name='segment_ids')\n",
    "\n",
    "            input_mask = tf.scatter_nd(indices=tf.expand_dims(tf.range(0,actual_token_length),axis=1),\n",
    "                                       updates=tf.ones(shape=[actual_token_length],dtype=tf.int32),\n",
    "                                       shape=[max_sequence_length],name='input_mask')\n",
    "            return (padded,segment_ids,input_mask)\n",
    "    #     Issue running map_fn on gpu https://github.com/tensorflow/tensorflow/issues/28007 \n",
    "    #     https://www.tensorflow.org/api_docs/python/tf/device\n",
    "        with tf.device('/device:CPU:0'):\n",
    "            encoded = tf.map_fn(lambda x: process_invidual_line_encoding(x),input_text_batch,dtype=(tf.int32,tf.int32,tf.int32))\n",
    "        tf.print('encoded map_fn ',encoded)\n",
    "        tf.print('encoded map_fn type ',type(encoded))\n",
    "        tf.print('encoded map_fn shape ',tf.shape(encoded))\n",
    "    #     stacked = tf.stack(encoded)\n",
    "        stacked = encoded\n",
    "        tf.print('stacked size ',tf.size(stacked))\n",
    "        tf.print('stacked shape ',tf.shape(stacked))\n",
    "        return stacked\n",
    "    \n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(SentencepieceTokenization, self).get_config()\n",
    "        config.update({'model_path': self.model_path})\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids  Tensor(\"sentencepiece_tokenization/Identity:0\", shape=(None, None), dtype=int32) segment_ids  Tensor(\"sentencepiece_tokenization/Identity_1:0\", shape=(None, None), dtype=int32) input_mask  Tensor(\"sentencepiece_tokenization/Identity_2:0\", shape=(None, 20), dtype=int32)\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_text (InputLayer)      [(None,)]                 0         \n",
      "_________________________________________________________________\n",
      "sentencepiece_tokenization ( ((None, None), (None, Non 0         \n",
      "=================================================================\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_text = tf.keras.Input(shape=(),dtype=tf.string,name='input_text')\n",
    "sp_model_path2 = '/space/engineering/tfhub_modules/db96091489ca3b9a526f252d262b880571e1335e/assets/30k-clean.model'\n",
    "sentence_piece_layer = SentencepieceTokenization(model_path=sp_model_path2)\n",
    "# encoded_text = sentence_piece_layer(input_text)\n",
    "input_ids,segment_ids,input_mask = sentence_piece_layer(input_text)\n",
    "# tf.autograph.trace(encoded_text)\n",
    "print('input_ids ',input_ids, 'segment_ids ',segment_ids, 'input_mask ',input_mask )\n",
    "# print('encoded text ',encoded_text)\n",
    "# model = tf.keras.Model(inputs={'input_text':input_text},outputs=encoded_text)\n",
    "\n",
    "model = tf.keras.Model(inputs={'input_text':input_text},outputs=input_ids)\n",
    "# model = tf.keras.Model(inputs={'input_text':input_text},outputs=encoded_text)\n",
    "sample_text = 'this is a sample sentence'\n",
    "sample_text2 = 'this is a random text for trying out encoding'\n",
    "model.summary()\n",
    "# model.predict([sample_text,sample_text2])\n",
    "# model.sp_model_file = tf.saved_model.Asset(sentence_piece_layer)\n",
    "# model.save(vocab_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tensorflow.python.keras.engine.input_layer.InputLayer at 0x7fdea8503c50>,\n",
       " <__main__.SentencepieceTokenization at 0x7fdea85038d0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.submodules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.SentencepieceTokenization at 0x7fdea85038d0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_layer(index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/var/extra/users/jgeorge/tf2.0/input/albert_base/assets/30k-clean.model'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'assets'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.saved_model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.lookup.StaticHashTable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_training_output = '/var/extra/users/jgeorge/tf2.0/output/training_out'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_dir = os.path.join(model_training_output, 'summaries')\n",
    "summary_callback = tf.keras.callbacks.TensorBoard(summary_dir)\n",
    "checkpoint_path = os.path.join(model_training_output, 'checkpoint')\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    checkpoint_path, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_callbacks = [summary_callback, checkpoint_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_model_filepath = tf.saved_model.Asset(sp_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'/var/extra/users/jgeorge/tf2.0/input/albert_base/assets/30k-clean.model'>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_model_filepath.asset_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.sp_model_filepath = tf.saved_model.Asset(sp_model_path)\n",
    "model.do_lower_case = tf.saved_model\n",
    "# model.sp_model_proto = tf.saved_model.Asset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.saved_model.SaveOptions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/extra/users/jgeorge/tf2.0/input/dish/data/jan17_2020/model_dir/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/extra/users/jgeorge/tf2.0/input/dish/data/jan17_2020/model_dir/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(vocab_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "loaded_model = tf.keras.models.load_model(vocab_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_text_batch is  [\"this is a sample sentence\" \"this is a random text for trying out encoding\" \"another jithin random text to try out\"]\n",
      "type of input_text_batch  <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "squeezed_values  [48 25 21 5717 5123]\n",
      "values_trimmed  [48 25 21 5717 5123]\n",
      "concat_out  [2 48 25 ... 5717 5123 3]\n",
      "size of concat  7\n",
      "shape of concat  [7]\n",
      "squeezed_values  [48 25 21 ... 749 70 19608]\n",
      "values_trimmed  [48 25 21 ... 749 70 19608]\n",
      "concat_out  [2 48 25 ... 70 19608 3]\n",
      "size of concat  11\n",
      "shape of concat  [11]\n",
      "squeezed_values  [226 7022 96 ... 20 1131 70]\n",
      "values_trimmed  [226 7022 96 ... 20 1131 70]\n",
      "concat_out  [2 226 7022 ... 1131 70 3]\n",
      "size of concat  11\n",
      "shape of concat  [11]\n",
      "encoded map_fn  ([[2 48 25 ... 0 0 0]\n",
      " [2 48 25 ... 0 0 0]\n",
      " [2 226 7022 ... 0 0 0]], [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]], [[1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]])\n",
      "encoded map_fn type  <class 'tuple'>\n",
      "encoded map_fn shape  [3 3 20]\n",
      "stacked size  180\n",
      "stacked shape  [3 3 20]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[    2,    48,    25,    21,  5717,  5123,     3,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0],\n",
       "       [    2,    48,    25,    21,  5477,  1854,    26,   749,    70,\n",
       "        19608,     3,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0],\n",
       "       [    2,   226,  7022,    96,   108,  5477,  1854,    20,  1131,\n",
       "           70,     3,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0]], dtype=int32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.predict([sample_text,sample_text2,'another jithin random text to try out'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_text_batch is  [\"this is a sample sentence\" \"this is a random text for trying out encoding\"]\n",
      "type of input_text_batch  <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "squeezed_values  [48 25 21 5717 5123]\n",
      "values_trimmed  [48 25 21 5717 5123]\n",
      "concat_out  [2 48 25 ... 5717 5123 3]\n",
      "size of concat  7\n",
      "shape of concat  [7]\n",
      "squeezed_values  [48 25 21 ... 749 70 19608]\n",
      "values_trimmed  [48 25 21 ... 749 70 19608]\n",
      "concat_out  [2 48 25 ... 70 19608 3]\n",
      "size of concat  11\n",
      "shape of concat  [11]\n",
      "encoded map_fn  ([[2 48 25 ... 0 0 0]\n",
      " [2 48 25 ... 0 0 0]], [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]], [[1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]])\n",
      "encoded map_fn type  <class 'tuple'>\n",
      "encoded map_fn shape  [3 2 20]\n",
      "stacked size  120\n",
      "stacked shape  [3 2 20]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[    2,    48,    25,    21,  5717,  5123,     3,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0],\n",
       "       [    2,    48,    25,    21,  5477,  1854,    26,   749,    70,\n",
       "        19608,     3,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0]], dtype=int32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([sample_text,sample_text2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.layers.Embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tf.zeros([10])\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=int32, numpy=array([0, 1, 2, 3, 4], dtype=int32)>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.range(0,limit=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 1), dtype=int32, numpy=\n",
       "array([[0],\n",
       "       [1],\n",
       "       [2],\n",
       "       [3],\n",
       "       [4]], dtype=int32)>"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = tf.reshape(tf.range(0,5),shape=(5,1))\n",
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=int32, numpy=array([5, 1], dtype=int32)>"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.shape(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 5], dtype=int32)>"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reverse(tf.shape(t1),axis=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([1., 1., 1., 1., 1., 0., 0., 0., 0., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.scatter_nd(indices=tf.reshape(tf.range(0,5),shape=(5,1)),updates=tf.ones(shape=[5]),shape=[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=int32, numpy=array([0, 1, 2, 3, 4], dtype=int32)>"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.range(0,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 1), dtype=int32, numpy=\n",
       "array([[0],\n",
       "       [1],\n",
       "       [2],\n",
       "       [3],\n",
       "       [4]], dtype=int32)>"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reshape(tf.range(0,5),shape=(5,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 1), dtype=int32, numpy=\n",
       "array([[0],\n",
       "       [1],\n",
       "       [2],\n",
       "       [3],\n",
       "       [4]], dtype=int32)>"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.expand_dims(tf.range(0,5),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_text (InputLayer)      [(None,)]                 0         \n",
      "_________________________________________________________________\n",
      "sentencepiece_tokenization_3 SentencepieceEncodeDense( 0         \n",
      "=================================================================\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'input_text_1:0' shape=(None,) dtype=string>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = tf.keras.Input(shape=(),dtype=tf.string,name='input_text')\n",
    "input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'input_text_1:0' shape=(None, None) dtype=string>"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentencepieceEncodeDense(values=array([[   48,    25,    21,  5717,  5123,     0,     0,     0,     0],\n",
       "       [   48,    25,    21,  5477,  1854,    26,   749,    70, 19608]],\n",
       "      dtype=int32), sequence_length=array([5, 9], dtype=int32))"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text = 'this is a sample sentence'\n",
    "sample_text2 = 'this is a random text for trying out encoding'\n",
    "model.predict([sample_text,sample_text2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentencepieceEncodeDense(values=array([[  48,   25,   21, 5717, 5123]], dtype=int32), sequence_length=array([5], dtype=int32))"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text = 'this is a sample sentence'\n",
    "model.predict([[sample_text]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'VocabLookup' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-83144ba06dc3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mencoded_text4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVocabLookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'VocabLookup' is not defined"
     ]
    }
   ],
   "source": [
    "encoded_text4 = VocabLookup(vocab_path)(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model4 = tf.keras.Model(inputs={'input_text':input_text},outputs=encoded_text4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/extra/users/jgeorge/tf2.0/input/dish/data/jan17_2020/model_dir/assets\n"
     ]
    }
   ],
   "source": [
    "model4.save(vocab_model_dir,include_optimizer=False,save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text2 = tfs.encode([input_text],model_proto=model_proto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_piece_tokenizer = tftext.SentencepieceTokenizer(model=model_proto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_out_ragged =  sentence_piece_tokenizer.tokenize(['this is it','next sentence taks is this random text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 8), dtype=int32, numpy=\n",
       "array([[   48,    25,    32,     0,     0,     0,     0,     0],\n",
       "       [  328,  5123, 13302,    18,    25,    48,  5477,  1854]],\n",
       "      dtype=int32)>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_out_ragged.to_tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[48, 25, 32, 0, 0, 0, 0, 0], [328, 5123, 13302, 18, 25, 48, 5477, 1854]]>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfs_encoded = tfs.encode(['this is it','next sentence taks is this random text'],model_proto=model_proto)\n",
    "tfs_ragged = tf.RaggedTensor.from_tensor(tfs_encoded.values)\n",
    "tfs_ragged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_piece_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.layers.Layer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "replaced_text = RegexReplacement(\"abc\",\"xyz\")(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_regex_replace = tf.keras.Model(inputs={\"input_text\":input_text},outputs=replaced_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'this is it xyz'], dtype=object)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_replaced_text = model_regex_replace.predict([['this is it abc']])\n",
    "\n",
    "predicted_replaced_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_replaced_text2 = RegexReplacement(\"this\",\"that google\")(input_text2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_text = SentencepieceTokenization(model_proto=model_proto)(input_text)\n",
    "sentence_piece_layer = SentencepieceTokenization(model_path=sp_model_path)\n",
    "encoded_text = sentence_piece_layer(input_text)\n",
    "encoded_text2 = sentence_piece_layer(regex_replaced_text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Model(inputs={'input_text':input_text},outputs=encoded_text)\n",
    "model2 = tf.keras.Model(inputs={'input_text':input_text2},outputs=encoded_text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = 'this is a sample sentence'\n",
    "sample_text2 = 'that google is a sample sentence'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  48,   25,   21, 5717, 5123]], dtype=int32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([[sample_text]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Tried to export a function which references untracked object Tensor(\"573:0\", shape=(), dtype=resource).TensorFlow objects (e.g. tf.Variable) captured by functions must be tracked by assigning them to an attribute of a tracked object or assigned to an attribute of the main object directly.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-db601e134421>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_model_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filepath, overwrite, include_optimizer, save_format, signatures, options)\u001b[0m\n\u001b[1;32m   1050\u001b[0m     \"\"\"\n\u001b[1;32m   1051\u001b[0m     save.save_model(self, filepath, overwrite, include_optimizer, save_format,\n\u001b[0;32m-> 1052\u001b[0;31m                     signatures, options)\n\u001b[0m\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/keras/saving/save.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer, save_format, signatures, options)\u001b[0m\n\u001b[1;32m    136\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     saved_model_save.save(model, filepath, overwrite, include_optimizer,\n\u001b[0;32m--> 138\u001b[0;31m                           signatures, options)\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/save.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(model, filepath, overwrite, include_optimizer, signatures, options)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;31m# we use the default replica context here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mdistribution_strategy_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_default_replica_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m       \u001b[0msave_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/saved_model/save.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, export_dir, signatures, options)\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m   _, exported_graph, object_saver, asset_info = _build_meta_graph(\n\u001b[0;32m--> 951\u001b[0;31m       obj, export_dir, signatures, options, meta_graph_def)\n\u001b[0m\u001b[1;32m    952\u001b[0m   \u001b[0msaved_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_model_schema_version\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSAVED_MODEL_SCHEMA_VERSION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/saved_model/save.py\u001b[0m in \u001b[0;36m_build_meta_graph\u001b[0;34m(obj, export_dir, signatures, options, meta_graph_def)\u001b[0m\n\u001b[1;32m   1025\u001b[0m   asset_info, exported_graph = _fill_meta_graph_def(meta_graph_def,\n\u001b[1;32m   1026\u001b[0m                                                     \u001b[0msaveable_view\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignatures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1027\u001b[0;31m                                                     options.namespace_whitelist)\n\u001b[0m\u001b[1;32m   1028\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_aliases\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m     \u001b[0mfunction_aliases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta_graph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta_info_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_aliases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/saved_model/save.py\u001b[0m in \u001b[0;36m_fill_meta_graph_def\u001b[0;34m(meta_graph_def, saveable_view, signature_functions, namespace_whitelist)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mexported_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m     \u001b[0msignatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_generate_signatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignature_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresource_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mconcrete_function\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msaveable_view\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcrete_functions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m       \u001b[0mconcrete_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/saved_model/save.py\u001b[0m in \u001b[0;36m_generate_signatures\u001b[0;34m(signature_functions, resource_map)\u001b[0m\n\u001b[1;32m    495\u001b[0m                                                   signature_key, function.name))\n\u001b[1;32m    496\u001b[0m     outputs = _call_function_with_mapped_captures(\n\u001b[0;32m--> 497\u001b[0;31m         function, mapped_inputs, resource_map)\n\u001b[0m\u001b[1;32m    498\u001b[0m     signatures[signature_key] = signature_def_utils.build_signature_def(\n\u001b[1;32m    499\u001b[0m         \u001b[0m_tensor_dict_to_tensorinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexterior_argument_placeholders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/saved_model/save.py\u001b[0m in \u001b[0;36m_call_function_with_mapped_captures\u001b[0;34m(function, args, resource_map)\u001b[0m\n\u001b[1;32m    447\u001b[0m   \u001b[0;34m\"\"\"Calls `function` in the exported graph, using mapped resource captures.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m   export_captures = _map_captures_to_created_tensors(function.graph.captures,\n\u001b[0;32m--> 449\u001b[0;31m                                                      resource_map)\n\u001b[0m\u001b[1;32m    450\u001b[0m   \u001b[0;31m# Calls the function quite directly, since we have new captured resource\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m   \u001b[0;31m# tensors we need to feed in which weren't part of the original function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/saved_model/save.py\u001b[0m in \u001b[0;36m_map_captures_to_created_tensors\u001b[0;34m(original_captures, resource_map)\u001b[0m\n\u001b[1;32m    370\u001b[0m            \u001b[0;34m\"be tracked by assigning them to an attribute of a tracked object \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m            \u001b[0;34m\"or assigned to an attribute of the main object directly.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m           ).format(interior))\n\u001b[0m\u001b[1;32m    373\u001b[0m     \u001b[0mexport_captures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped_resource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexport_captures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Tried to export a function which references untracked object Tensor(\"573:0\", shape=(), dtype=resource).TensorFlow objects (e.g. tf.Variable) captured by functions must be tracked by assigning them to an attribute of a tracked object or assigned to an attribute of the main object directly."
     ]
    }
   ],
   "source": [
    "model.save(vocab_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentencepieceEncodeDense(values=array([[  30, 8144,   25,   21, 5717, 5123]], dtype=int32), sequence_length=array([6], dtype=int32))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([[sample_text2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentencepieceEncodeDense(values=array([[  30, 8144,   25,   21, 5717, 5123]], dtype=int32), sequence_length=array([6], dtype=int32))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.predict([[sample_text]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentencepieceEncodeDense(values=<tf.Tensor: shape=(1, 6), dtype=int32, numpy=array([[  30, 8144,   25,   21, 5717, 5123]], dtype=int32)>, sequence_length=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([6], dtype=int32)>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfs.encode([sample_text2],model_proto=model_proto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_number = tf.keras.Input(shape=(),dtype=tf.int32,name='input_number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Addition(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(Addition, self).__init__(trainable=False,dtype=tf.int32)\n",
    "        #     self.sp_model_proto = model_proto\n",
    "#         self.sp_model = tftext.SentencepieceTokenizer(model=model_proto)\n",
    "    \n",
    "    def call(self, input_number):\n",
    "        \n",
    "        return input_number + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_number = Addition()(input_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = tf.keras.Model(inputs={'input_number':input_number},outputs=modified_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 8], dtype=int32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.predict([3,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_output_ragged = model3.predict([['this is it']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[48, 25, 32]]>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_output_ragged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1038, shape=(1, 3), dtype=int32, numpy=array([[48, 25, 32]], dtype=int32)>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_output_ragged.to_tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'input_text_2:0' shape=(None,) dtype=string>]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_export_dir = '/var/extra/users/jgeorge/tf2.0/input/albert_base_sentencepiece'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/extra/users/jgeorge/tf2.0/input/albert_base_sentencepiece/assets\n"
     ]
    }
   ],
   "source": [
    "#https://github.com/tensorflow/tensorflow/issues/31300\n",
    "model.save(model_export_dir,include_optimizer=False,save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Tried to export a function which references untracked object Tensor(\"StatefulPartitionedCall/args_1:0\", shape=(), dtype=resource).TensorFlow objects (e.g. tf.Variable) captured by functions must be tracked by assigning them to an attribute of a tracked object or assigned to an attribute of the main object directly.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-10e048f3e3e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_export_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.1/lib/python3.7/site-packages/tensorflow_core/python/saved_model/save.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, export_dir, signatures, options)\u001b[0m\n\u001b[1;32m    907\u001b[0m   \u001b[0mobject_saver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrackableSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_graph_view\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m   asset_info, exported_graph = _fill_meta_graph_def(\n\u001b[0;32m--> 909\u001b[0;31m       meta_graph_def, saveable_view, signatures, options.namespace_whitelist)\n\u001b[0m\u001b[1;32m    910\u001b[0m   saved_model.saved_model_schema_version = (\n\u001b[1;32m    911\u001b[0m       constants.SAVED_MODEL_SCHEMA_VERSION)\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.1/lib/python3.7/site-packages/tensorflow_core/python/saved_model/save.py\u001b[0m in \u001b[0;36m_fill_meta_graph_def\u001b[0;34m(meta_graph_def, saveable_view, signature_functions, namespace_whitelist)\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mexported_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 587\u001b[0;31m     \u001b[0msignatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_generate_signatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignature_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresource_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    588\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mconcrete_function\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msaveable_view\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcrete_functions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0mconcrete_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.1/lib/python3.7/site-packages/tensorflow_core/python/saved_model/save.py\u001b[0m in \u001b[0;36m_generate_signatures\u001b[0;34m(signature_functions, resource_map)\u001b[0m\n\u001b[1;32m    456\u001b[0m             argument_inputs, signature_key, function.name))\n\u001b[1;32m    457\u001b[0m     outputs = _call_function_with_mapped_captures(\n\u001b[0;32m--> 458\u001b[0;31m         function, mapped_inputs, resource_map)\n\u001b[0m\u001b[1;32m    459\u001b[0m     signatures[signature_key] = signature_def_utils.build_signature_def(\n\u001b[1;32m    460\u001b[0m         \u001b[0m_tensor_dict_to_tensorinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexterior_argument_placeholders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.1/lib/python3.7/site-packages/tensorflow_core/python/saved_model/save.py\u001b[0m in \u001b[0;36m_call_function_with_mapped_captures\u001b[0;34m(function, args, resource_map)\u001b[0m\n\u001b[1;32m    408\u001b[0m   \u001b[0;34m\"\"\"Calls `function` in the exported graph, using mapped resource captures.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m   export_captures = _map_captures_to_created_tensors(\n\u001b[0;32m--> 410\u001b[0;31m       function.graph.captures, resource_map)\n\u001b[0m\u001b[1;32m    411\u001b[0m   \u001b[0;31m# Calls the function quite directly, since we have new captured resource\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m   \u001b[0;31m# tensors we need to feed in which weren't part of the original function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.1/lib/python3.7/site-packages/tensorflow_core/python/saved_model/save.py\u001b[0m in \u001b[0;36m_map_captures_to_created_tensors\u001b[0;34m(original_captures, resource_map)\u001b[0m\n\u001b[1;32m    330\u001b[0m            \u001b[0;34m\"be tracked by assigning them to an attribute of a tracked object \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m            \"or assigned to an attribute of the main object directly.\")\n\u001b[0;32m--> 332\u001b[0;31m           .format(interior))\n\u001b[0m\u001b[1;32m    333\u001b[0m     \u001b[0mexport_captures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped_resource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexport_captures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Tried to export a function which references untracked object Tensor(\"StatefulPartitionedCall/args_1:0\", shape=(), dtype=resource).TensorFlow objects (e.g. tf.Variable) captured by functions must be tracked by assigning them to an attribute of a tracked object or assigned to an attribute of the main object directly."
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(model,export_dir=model_export_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/var/extra/users/jgeorge/tf2.0/input/dish/data/jan17_2020/vocab.txt'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = tf.lookup.StaticHashTable(tf.lookup.TextFileInitializer(vocab_path,tf.string,tf.lookup.TextFileIndex.WHOLE_LINE,\n",
    "                              tf.int64,tf.lookup.TextFileIndex.LINE_NUMBER),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-074e73acb983>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hi'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'this'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/custom/python/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/ops/lookup_ops.py\u001b[0m in \u001b[0;36mlookup\u001b[0;34m(self, keys, name)\u001b[0m\n\u001b[1;32m    219\u001b[0m       \u001b[0mkey_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_key_dtype\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m       raise TypeError(\"Signature mismatch. Keys must be dtype %s, got %s.\" %\n\u001b[1;32m    223\u001b[0m                       (self._key_dtype, keys.dtype))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'dtype'"
     ]
    }
   ],
   "source": [
    "table.lookup(['hi','this'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.lookup.TextFileInitializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=5356, shape=(4,), dtype=string, numpy=array([b'hi', b'this', b'is', b'jithin'], dtype=object)>"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.split('hi this is jithin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_out1 = VocabLookup(vocab_path=vocab_path)(splitted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_lookup1 = tf.keras.Model(inputs={'splitted_text':splitted_text},outputs=lookup_out1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = tf.lookup.StaticHashTable(tf.lookup.TextFileInitializer(vocab_path,tf.string,tf.lookup.TextFileIndex.WHOLE_LINE,\n",
    "                              tf.int64,tf.lookup.TextFileIndex.LINE_NUMBER),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lookup_out = VocabLookup(vocab_path=vocab_path)(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [2],\n",
       "       [3],\n",
       "       [0]])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lookup1.predict([['hi','this','is','random']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3, 0]])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lookup.predict([['hi this is asdfasdf']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lookup_export_dir = '/var/extra/users/jgeorge/tf2.0/input/albert_base_lookup'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/custom/python/anaconda3/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: /var/extra/users/jgeorge/tf2.0/input/albert_base_lookup/assets\n"
     ]
    }
   ],
   "source": [
    "model_lookup.save(model_lookup_export_dir,include_optimizer=False,save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 - tensorflow nightly  (tf_nightly)",
   "language": "python",
   "name": "tf_nightly"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
