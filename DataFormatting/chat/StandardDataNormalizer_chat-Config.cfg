## Don't edit the following line
[params]

## The script will create a new directory if it doesn't exit already
#  Eg:- output_dir=D:\\projects\\ebay-DE\\ssi1\\normalized_1st30k
#output_dir=normalized_data/test_complete
output_dir=ssi/data

## DataMaster should be an excel workbook and should contain transcriptions on the first sheet. It should also contain columns filename, transcription, RU Tag and cap_level (if there are second responses in it).
#  Eg:-  DataMaster=D:\\247NETWORKBACKUP\\OneDrive - 247 Customer Pvt. Ltd\\projects\\ebay-DE\\data\\eBay_DE_First30K_DataMaster_20160701.xlsx
DataMaster=data/ups_nl_grammar_specs_v1.2.xlsx

## Name of the sheet/tab in datamaster with all data
#Eg:- DataSheetName=FullData_30K_0615
DataSheetName=data

## Whether words in the dataset should be replaced by word classes
replace_word_classes_in_data=true

##Specify the intent column name, if there is rolled up intent specify that
# Eg:- intent_column_name=ru_tag
intent_column_name=intent

##Specify the granular intent column name (this is for the second column of intents that appear in normalized data set)
# Eg:- granular_intent_column_name=granular_tag
granular_intent_column_name=intent

## File containing all word classes for normalization
# Eg:- word_classes_file=D:\\projects\\IBM\\SupportingFiles\\Word_Classes_v9_ibm.txt
word_classes_file=supporting_files/word_classes.txt

## grxml_dir_URL will be used in class GRXML files. So, this variable should be set depending on where you will place class grammars during model compilation. You can put a dummy string here and replace it later with the actual URL later on.
#Eg:- grxml_dir_URL=http://grammar.svc.tellme.com/nlu/amex/upfrontmenu/v1
grxml_dir_URL=http://grammar.svc.tellme.com/classgrammars_ups

##This parameter is for configuring the lang parameter in the grxml file
# Eg :- grxml_lang=en-US or grxml_lang=de-DE
grxml_lang=en-US

## Whether DataMaster contains second responses. If it does, then ensure that the DataMaster contains a column 'cap_level' specifying whether the utterance is a 'firstresp' or 'secondresp'. Separate output files will be created for first and second responses
second_responses_exist=false

## Whether the DataMaster contains repititions of an utterance as counts. If true, then the datamaster should have a 'Count' column
contains_utt_repititions_as_counts=false

## New transcription convention
new_trans_convention=true

## Encoding to use while reading/writing output to file
#Encoding for German data : ISO-8859-1
#Encoding for English data :  utf-8
encoding=utf-8

#number of parallel executions required. If you need to debug set it to 1 so that the operations for one row will complete before the next ones starts
no_of_parallel_executions=1

#set true if you need to print debug logs
debug_mode=false

#set the minimum frequency below which a word will be tagged as _class_unknown, a separate file is generated with this replaced data
min_word_freq=2

#If stemming have to be performed on the input text
is_perform_stemming=false

#The words mentioned in this file will be ignored whie replacing with stems. This file is useful only if is_perform_stemming=true
#Eg:- stem_exception_file=../supporting_files/stemming_exceptions.txt
#If not required please leave this parameter empty
stem_exception_file=supporting_files/stemming_exceptions.txt

#A replacement list for stem words
#This file is to provide additional stem replacements, lemmatization using wordnet is performed in addition to this
#If not required please leave this parameter empty
stem_replacement_file=supporting_files/stemming_replacement_file.txt

[chat]
#Whether to perform chat normalization
is_perform_chat_normalization=true

##Specify the path to the word expansions file. This would include cases like don't->do not, dont-> do not etc
word_expansions_file=supporting_files/word_expansions_complete_chat_only.txt

#Perform heuristic name check
is_perform_heuristic_name_check=true

#whether to perform spell check
is_perform_spell_check=false

#whether to perform ner extraction
is_perform_ner_extraction=false

#custom regex file
#if there is any customer specific regex provide the file with that config here. separator is tab (\t) between the regex & string to replace
#if custom regex is not required, leave this field empty
#Eg:-custom_regex_file=supporting_files/custom_regex.txt
custom_regex_file=supporting_files/custom_regex.txt

[chat.spell_check]
#Possible values for spell_check_type=enchant, remote
spell_check_type=remote

#This is required if spell_check_type=remote
spell_check_model_url=http://refine02.swamp.sv2.tellme.com/reports/jithinjees/slm/public_html/dish/dish-chatbot-models/spell_corrector.model

#This is required for basic spell check using enchant
personalized_word_dict=PWL.txt

[chat.ner]
#This is the stanford ner model (with case sensitive), currently only 2 entities are supported
# Eg :- stanford_ner_model=/home/jithin.j/applications/stanford_nlp/stanford-ner-2017-06-09/classifiers/english.all.3class.distsim.crf.ser.gz
stanford_ner_model=

#This is the standford ner jar path 
# Eg :- stanford_jar=/home/jithin.j/applications/stanford_nlp/stanford-ner-2017-06-09/stanford-ner.jar
stanford_jar=
